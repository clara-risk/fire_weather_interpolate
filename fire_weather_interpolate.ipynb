{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: latin1 -*-\n",
    "# cython: language_level=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import geopandas as gpd\n",
    "import os, sys\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import json, feather\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import pyproj\n",
    "import cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following programs to obtain data from either lookup csv files (for example, for elevation data) or to obtain the data from feather files. We are using feather files for faster reading. Cython is implemented only for certain functions that will need to be sped up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index of col_name (a str) in the header of the file that file_path points to. \n",
    "def get_col_num_list(file_path,col_name): \n",
    "    '''Get the column in the look up file that corresponds to the correct data\n",
    "    Parameters: \n",
    "        file_path (str): path to the lookup csv file, includes the file name\n",
    "        col_name (str): the column name of the data you want to use in the lookup file, ex. \"ELEV\"\n",
    "    Returns \n",
    "        idx_list (list): list containing all columns in the lookup table that are called col_name\n",
    "    '''\n",
    "    col_nums = [] #Create empty list for the relevent column numbers.\n",
    "    header = []\n",
    "    count = 0\n",
    "    with open(file_path) as lookup_info:\n",
    "        for line in lookup_info:\n",
    "            row = line.rstrip('\\n').split(',')\n",
    "            if count == 0: \n",
    "                header.append(row[0:]) #Access the header \n",
    "            count += 1\n",
    "    \n",
    "    #What is the index of col_name in the header? \n",
    "    idx_list = [i for i, x in enumerate(header[0]) if col_name in x.lower()] \n",
    "\n",
    "    lookup_info.close() \n",
    "\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from a lookup file containing the data + the lat and lon\n",
    "def finding_data_frm_lookup(projected_coordinates_list,file_path,idx_list):\n",
    "    '''Get the dictionary of values for each input point frm lookup file\n",
    "    Parameters\n",
    "        projected_coordinates_list (list of tuples): list of coordinates that you want to find data\n",
    "        in the lookup file for, must be in lon, lat format \n",
    "        file_path (str): file path to the lookup file, includes the file name and ending\n",
    "        idx_list (list): output of get_col_name_list, we will just take the first entry\n",
    "    Returns \n",
    "        min_distance_L (dict): a dictionary of values for each coordinate (taken from the closest \n",
    "        point in the lookup file to the input coordinate)\n",
    "    '''\n",
    "    distance_dictionary = {} \n",
    "    L_dictionary = {}\n",
    "    min_distance_L = {}\n",
    "    temp = []\n",
    "    with open(file_path) as L_information:\n",
    "        next(L_information) # Skip header. \n",
    "        for line in L_information:\n",
    "            line2 = line.rstrip('\\n').split(',')\n",
    "            #Store information in Python dictionary so we can access it efficiently. \n",
    "            L_dictionary[tuple([float(line2[1]),float(line2[2])])] = float(line2[idx_list[0]])\n",
    "            temp.append([float(line2[1]),float(line2[2])]) #Temp lat lon list \n",
    "\n",
    "    L_information.close() \n",
    "\n",
    "    L_locarray = np.array(temp) #Make an array of the lat lons. \n",
    "\n",
    "    loc_dict = {} \n",
    "\n",
    "    for coord in projected_coordinates_list: # projected_coordinates_list --> list of tuples.\n",
    "        ts_lat = coord[1] # Obtain the lat/lon. \n",
    "        ts_lon =  coord[0]\n",
    "        \n",
    "        L = [] # Create a place to store the elevation.\n",
    "\n",
    "        reshaped_coord= np.array([[ts_lon,ts_lat]]) #Input will be in lon, lat format. \n",
    "\n",
    "\n",
    "        hypot = cdist(L_locarray,reshaped_coord) #Get distance between input coords and coords in file.\n",
    "\n",
    "\n",
    "        where_distance_small =  np.argmin(hypot) #Get the smallest distance index. \n",
    "\n",
    "        \n",
    "        distance_dictionary[coord] = L_locarray[where_distance_small] #Access the lat/lon at the index.\n",
    "        #We don't care if there's more than one closest coordinate, same dist in all directions. \n",
    "\n",
    "        convert_to_lst = L_locarray[where_distance_small] \n",
    "        convert_to_tuple = tuple(convert_to_lst) #Convert to tuple to look up value in dictionary. \n",
    "        \n",
    "        min_distance_L[coord] = L_dictionary[convert_to_tuple] #Get value out of dictionary. \n",
    "        \n",
    "    \n",
    "    return min_distance_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to generate an array (in this case, it is basically a map) of 'b', which is a value that we need to calculate the overwinter Drought Code (DC) in the areas that need that. See this for more information: \n",
    "Lawson, B.D. & Armitage, O.B. (2008). Weather Guide for the Canadian Forest Fire Danger Rating System (pp. 1-84). Natural Resources Canada, Canadian Forest Service, Northern Forestry Centre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b(latlon_dict,file_path_slope,idx_slope,file_path_drainage,idx_drainage,shapefile):\n",
    "    '''Create a permanent lookup file for b for study area for future processing\n",
    "    This is used in overwinter dc procedure \n",
    "    Parameters\n",
    "        latlon_dict (dictionary, to be loaded from json file): dictionary of latitude and longitudes \n",
    "        for the hourly stations\n",
    "        file_path_slope (str): path to the slope file, includes file name\n",
    "        idx_slope (list): index of the slope variable in the header of the slope lookup file \n",
    "        file_path_drainage (str): path to the drainage file, includes file name\n",
    "        idx_drainage (list): index of the drainage variable in the header of the drainage lookup file \n",
    "        shapefile (str): path to the shapefile of the study area (.shp format)\n",
    "    Returns \n",
    "        The function will write a .json file to the hard drive, which can be loaded later. \n",
    "    '''\n",
    "\n",
    "    lat = [] #Initialize empty lists to store data \n",
    "    lon = []\n",
    "    for station_name in latlon_dict.keys(): #Loop through the list of stations \n",
    "        loc = latlon_dict[station_name]\n",
    "        latitude = loc[0]\n",
    "        longitude = loc[1]\n",
    "        lat.append(float(latitude))\n",
    "        lon.append(float(longitude))\n",
    "\n",
    "    y = np.array(lat) #Convert to a numpy array for faster processing speed \n",
    "    x = np.array(lon)\n",
    "\n",
    "    na_map = gpd.read_file(shapefile)\n",
    "    bounds = na_map.bounds #Get the bounding box of the shapefile \n",
    "    xmax = bounds['maxx']\n",
    "    xmin= bounds['minx']\n",
    "    ymax = bounds['maxy']\n",
    "    ymin = bounds['miny']\n",
    "    pixelHeight = 10000 #We want a 10 by 10 pixel, or as close as we can get \n",
    "    pixelWidth = 10000\n",
    "            \n",
    "    num_col = int((xmax - xmin) / pixelHeight) #Calculate the number of rows cols to fill the bounding box at that resolution \n",
    "    num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "    #We need to project to a projected system before making distance matrix\n",
    "    source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume NAD83\n",
    "    xProj, yProj = pyproj.Proj('esri:102001')(x,y) #Convert to Canada Albers Equal Area \n",
    "\n",
    "    yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']]) #Add the bounding box coords to the dataset so we can extrapolate the interpolation to cover whole area\n",
    "    xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "    Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row) #Get the value for lat lon in each cell we just made \n",
    "    Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "    Xi,Yi = np.meshgrid(Xi,Yi) #Make a rectangular grid (because eventually we will map the values)\n",
    "    Xi,Yi = Xi.flatten(), Yi.flatten() #Then we flatten the arrays for easier processing \n",
    "    #X and then Y for a reason \n",
    "    concat = np.array((Xi.flatten(), Yi.flatten())).T #Preparing the coordinates to send to the function that will get the elevation grid \n",
    "    send_to_list = concat.tolist()\n",
    "    send_to_tuple = [tuple(x) for x in send_to_list] #The elevation function takes a tuple \n",
    "\n",
    "    #in cython dictionaries maintain insertion order \n",
    "    Xi1_grd=[]\n",
    "    Yi1_grd=[]\n",
    "    slope_grd = []\n",
    "    drainage_grd = [] \n",
    "    slope_grd_dict = finding_data_frm_lookup(send_to_tuple,file_path_slope,idx_slope) #Get the elevations from the lookup file \n",
    "    drainage_grd_dict = finding_data_frm_lookup(send_to_tuple,file_path_drainage,idx_drainage)\n",
    "    for keys in slope_grd_dict.keys(): #The keys are each lat lon pair \n",
    "        x= keys[0]\n",
    "        y = keys[1]\n",
    "        Xi1_grd.append(x)\n",
    "        Yi1_grd.append(y)\n",
    "        slope_grd.append(slope_grd_dict[keys])\n",
    "        drainage_grd.append(drainage_grd_dict[keys])\n",
    "\n",
    "    #combine the arrays\n",
    "    slope_array = np.array(slope_grd)\n",
    "    drainage_array = np.array(drainage_grd)\n",
    "    \n",
    "    #return the b array to be passed to the other function\n",
    "    b_array = np.empty(slope_array.shape)\n",
    "    b_array[drainage_array == 3] = 0.5\n",
    "    b_array[drainage_array == 2] = 0.75\n",
    "    b_array[drainage_array == 0] = 0.75\n",
    "    b_array[drainage_array == 1] = 0.9\n",
    "    b_array[slope_array > 0.5] = 0.5\n",
    "\n",
    "    b_list = list(b_array)\n",
    "\n",
    "    with open('b_list.json', 'w') as fp: #write to hard drive for faster processing later \n",
    "        json.dump(b_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will need to be re-run repeatedly for each day to create the Drought Code, Duff Moisture Code, and Fine Fuel Moisture Code \"stacks\", which will be stored on the hard drive in .json format, so they can be re-accessed later without having to re-generate them. This means they will need to run quickly. I've implemented Cython here to try to optimize the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<!-- Generated by Cython 0.29.19 -->\n",
       "<html>\n",
       "<head>\n",
       "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
       "    <title>Cython: _cython_magic_ccf4ef6244d44c49476183f3ccd3d25c.pyx</title>\n",
       "    <style type=\"text/css\">\n",
       "    \n",
       "body.cython { font-family: courier; font-size: 12; }\n",
       "\n",
       ".cython.tag  {  }\n",
       ".cython.line { margin: 0em }\n",
       ".cython.code { font-size: 9; color: #444444; display: none; margin: 0px 0px 0px 8px; border-left: 8px none; }\n",
       "\n",
       ".cython.line .run { background-color: #B0FFB0; }\n",
       ".cython.line .mis { background-color: #FFB0B0; }\n",
       ".cython.code.run  { border-left: 8px solid #B0FFB0; }\n",
       ".cython.code.mis  { border-left: 8px solid #FFB0B0; }\n",
       "\n",
       ".cython.code .py_c_api  { color: red; }\n",
       ".cython.code .py_macro_api  { color: #FF7000; }\n",
       ".cython.code .pyx_c_api  { color: #FF3000; }\n",
       ".cython.code .pyx_macro_api  { color: #FF7000; }\n",
       ".cython.code .refnanny  { color: #FFA000; }\n",
       ".cython.code .trace  { color: #FFA000; }\n",
       ".cython.code .error_goto  { color: #FFA000; }\n",
       "\n",
       ".cython.code .coerce  { color: #008000; border: 1px dotted #008000 }\n",
       ".cython.code .py_attr { color: #FF0000; font-weight: bold; }\n",
       ".cython.code .c_attr  { color: #0000FF; }\n",
       ".cython.code .py_call { color: #FF0000; font-weight: bold; }\n",
       ".cython.code .c_call  { color: #0000FF; }\n",
       "\n",
       ".cython.score-0 {background-color: #FFFFff;}\n",
       ".cython.score-1 {background-color: #FFFFe7;}\n",
       ".cython.score-2 {background-color: #FFFFd4;}\n",
       ".cython.score-3 {background-color: #FFFFc4;}\n",
       ".cython.score-4 {background-color: #FFFFb6;}\n",
       ".cython.score-5 {background-color: #FFFFaa;}\n",
       ".cython.score-6 {background-color: #FFFF9f;}\n",
       ".cython.score-7 {background-color: #FFFF96;}\n",
       ".cython.score-8 {background-color: #FFFF8d;}\n",
       ".cython.score-9 {background-color: #FFFF86;}\n",
       ".cython.score-10 {background-color: #FFFF7f;}\n",
       ".cython.score-11 {background-color: #FFFF79;}\n",
       ".cython.score-12 {background-color: #FFFF73;}\n",
       ".cython.score-13 {background-color: #FFFF6e;}\n",
       ".cython.score-14 {background-color: #FFFF6a;}\n",
       ".cython.score-15 {background-color: #FFFF66;}\n",
       ".cython.score-16 {background-color: #FFFF62;}\n",
       ".cython.score-17 {background-color: #FFFF5e;}\n",
       ".cython.score-18 {background-color: #FFFF5b;}\n",
       ".cython.score-19 {background-color: #FFFF57;}\n",
       ".cython.score-20 {background-color: #FFFF55;}\n",
       ".cython.score-21 {background-color: #FFFF52;}\n",
       ".cython.score-22 {background-color: #FFFF4f;}\n",
       ".cython.score-23 {background-color: #FFFF4d;}\n",
       ".cython.score-24 {background-color: #FFFF4b;}\n",
       ".cython.score-25 {background-color: #FFFF48;}\n",
       ".cython.score-26 {background-color: #FFFF46;}\n",
       ".cython.score-27 {background-color: #FFFF44;}\n",
       ".cython.score-28 {background-color: #FFFF43;}\n",
       ".cython.score-29 {background-color: #FFFF41;}\n",
       ".cython.score-30 {background-color: #FFFF3f;}\n",
       ".cython.score-31 {background-color: #FFFF3e;}\n",
       ".cython.score-32 {background-color: #FFFF3c;}\n",
       ".cython.score-33 {background-color: #FFFF3b;}\n",
       ".cython.score-34 {background-color: #FFFF39;}\n",
       ".cython.score-35 {background-color: #FFFF38;}\n",
       ".cython.score-36 {background-color: #FFFF37;}\n",
       ".cython.score-37 {background-color: #FFFF36;}\n",
       ".cython.score-38 {background-color: #FFFF35;}\n",
       ".cython.score-39 {background-color: #FFFF34;}\n",
       ".cython.score-40 {background-color: #FFFF33;}\n",
       ".cython.score-41 {background-color: #FFFF32;}\n",
       ".cython.score-42 {background-color: #FFFF31;}\n",
       ".cython.score-43 {background-color: #FFFF30;}\n",
       ".cython.score-44 {background-color: #FFFF2f;}\n",
       ".cython.score-45 {background-color: #FFFF2e;}\n",
       ".cython.score-46 {background-color: #FFFF2d;}\n",
       ".cython.score-47 {background-color: #FFFF2c;}\n",
       ".cython.score-48 {background-color: #FFFF2b;}\n",
       ".cython.score-49 {background-color: #FFFF2b;}\n",
       ".cython.score-50 {background-color: #FFFF2a;}\n",
       ".cython.score-51 {background-color: #FFFF29;}\n",
       ".cython.score-52 {background-color: #FFFF29;}\n",
       ".cython.score-53 {background-color: #FFFF28;}\n",
       ".cython.score-54 {background-color: #FFFF27;}\n",
       ".cython.score-55 {background-color: #FFFF27;}\n",
       ".cython.score-56 {background-color: #FFFF26;}\n",
       ".cython.score-57 {background-color: #FFFF26;}\n",
       ".cython.score-58 {background-color: #FFFF25;}\n",
       ".cython.score-59 {background-color: #FFFF24;}\n",
       ".cython.score-60 {background-color: #FFFF24;}\n",
       ".cython.score-61 {background-color: #FFFF23;}\n",
       ".cython.score-62 {background-color: #FFFF23;}\n",
       ".cython.score-63 {background-color: #FFFF22;}\n",
       ".cython.score-64 {background-color: #FFFF22;}\n",
       ".cython.score-65 {background-color: #FFFF22;}\n",
       ".cython.score-66 {background-color: #FFFF21;}\n",
       ".cython.score-67 {background-color: #FFFF21;}\n",
       ".cython.score-68 {background-color: #FFFF20;}\n",
       ".cython.score-69 {background-color: #FFFF20;}\n",
       ".cython.score-70 {background-color: #FFFF1f;}\n",
       ".cython.score-71 {background-color: #FFFF1f;}\n",
       ".cython.score-72 {background-color: #FFFF1f;}\n",
       ".cython.score-73 {background-color: #FFFF1e;}\n",
       ".cython.score-74 {background-color: #FFFF1e;}\n",
       ".cython.score-75 {background-color: #FFFF1e;}\n",
       ".cython.score-76 {background-color: #FFFF1d;}\n",
       ".cython.score-77 {background-color: #FFFF1d;}\n",
       ".cython.score-78 {background-color: #FFFF1c;}\n",
       ".cython.score-79 {background-color: #FFFF1c;}\n",
       ".cython.score-80 {background-color: #FFFF1c;}\n",
       ".cython.score-81 {background-color: #FFFF1c;}\n",
       ".cython.score-82 {background-color: #FFFF1b;}\n",
       ".cython.score-83 {background-color: #FFFF1b;}\n",
       ".cython.score-84 {background-color: #FFFF1b;}\n",
       ".cython.score-85 {background-color: #FFFF1a;}\n",
       ".cython.score-86 {background-color: #FFFF1a;}\n",
       ".cython.score-87 {background-color: #FFFF1a;}\n",
       ".cython.score-88 {background-color: #FFFF1a;}\n",
       ".cython.score-89 {background-color: #FFFF19;}\n",
       ".cython.score-90 {background-color: #FFFF19;}\n",
       ".cython.score-91 {background-color: #FFFF19;}\n",
       ".cython.score-92 {background-color: #FFFF19;}\n",
       ".cython.score-93 {background-color: #FFFF18;}\n",
       ".cython.score-94 {background-color: #FFFF18;}\n",
       ".cython.score-95 {background-color: #FFFF18;}\n",
       ".cython.score-96 {background-color: #FFFF18;}\n",
       ".cython.score-97 {background-color: #FFFF17;}\n",
       ".cython.score-98 {background-color: #FFFF17;}\n",
       ".cython.score-99 {background-color: #FFFF17;}\n",
       ".cython.score-100 {background-color: #FFFF17;}\n",
       ".cython.score-101 {background-color: #FFFF16;}\n",
       ".cython.score-102 {background-color: #FFFF16;}\n",
       ".cython.score-103 {background-color: #FFFF16;}\n",
       ".cython.score-104 {background-color: #FFFF16;}\n",
       ".cython.score-105 {background-color: #FFFF16;}\n",
       ".cython.score-106 {background-color: #FFFF15;}\n",
       ".cython.score-107 {background-color: #FFFF15;}\n",
       ".cython.score-108 {background-color: #FFFF15;}\n",
       ".cython.score-109 {background-color: #FFFF15;}\n",
       ".cython.score-110 {background-color: #FFFF15;}\n",
       ".cython.score-111 {background-color: #FFFF15;}\n",
       ".cython.score-112 {background-color: #FFFF14;}\n",
       ".cython.score-113 {background-color: #FFFF14;}\n",
       ".cython.score-114 {background-color: #FFFF14;}\n",
       ".cython.score-115 {background-color: #FFFF14;}\n",
       ".cython.score-116 {background-color: #FFFF14;}\n",
       ".cython.score-117 {background-color: #FFFF14;}\n",
       ".cython.score-118 {background-color: #FFFF13;}\n",
       ".cython.score-119 {background-color: #FFFF13;}\n",
       ".cython.score-120 {background-color: #FFFF13;}\n",
       ".cython.score-121 {background-color: #FFFF13;}\n",
       ".cython.score-122 {background-color: #FFFF13;}\n",
       ".cython.score-123 {background-color: #FFFF13;}\n",
       ".cython.score-124 {background-color: #FFFF13;}\n",
       ".cython.score-125 {background-color: #FFFF12;}\n",
       ".cython.score-126 {background-color: #FFFF12;}\n",
       ".cython.score-127 {background-color: #FFFF12;}\n",
       ".cython.score-128 {background-color: #FFFF12;}\n",
       ".cython.score-129 {background-color: #FFFF12;}\n",
       ".cython.score-130 {background-color: #FFFF12;}\n",
       ".cython.score-131 {background-color: #FFFF12;}\n",
       ".cython.score-132 {background-color: #FFFF11;}\n",
       ".cython.score-133 {background-color: #FFFF11;}\n",
       ".cython.score-134 {background-color: #FFFF11;}\n",
       ".cython.score-135 {background-color: #FFFF11;}\n",
       ".cython.score-136 {background-color: #FFFF11;}\n",
       ".cython.score-137 {background-color: #FFFF11;}\n",
       ".cython.score-138 {background-color: #FFFF11;}\n",
       ".cython.score-139 {background-color: #FFFF11;}\n",
       ".cython.score-140 {background-color: #FFFF11;}\n",
       ".cython.score-141 {background-color: #FFFF10;}\n",
       ".cython.score-142 {background-color: #FFFF10;}\n",
       ".cython.score-143 {background-color: #FFFF10;}\n",
       ".cython.score-144 {background-color: #FFFF10;}\n",
       ".cython.score-145 {background-color: #FFFF10;}\n",
       ".cython.score-146 {background-color: #FFFF10;}\n",
       ".cython.score-147 {background-color: #FFFF10;}\n",
       ".cython.score-148 {background-color: #FFFF10;}\n",
       ".cython.score-149 {background-color: #FFFF10;}\n",
       ".cython.score-150 {background-color: #FFFF0f;}\n",
       ".cython.score-151 {background-color: #FFFF0f;}\n",
       ".cython.score-152 {background-color: #FFFF0f;}\n",
       ".cython.score-153 {background-color: #FFFF0f;}\n",
       ".cython.score-154 {background-color: #FFFF0f;}\n",
       ".cython.score-155 {background-color: #FFFF0f;}\n",
       ".cython.score-156 {background-color: #FFFF0f;}\n",
       ".cython.score-157 {background-color: #FFFF0f;}\n",
       ".cython.score-158 {background-color: #FFFF0f;}\n",
       ".cython.score-159 {background-color: #FFFF0f;}\n",
       ".cython.score-160 {background-color: #FFFF0f;}\n",
       ".cython.score-161 {background-color: #FFFF0e;}\n",
       ".cython.score-162 {background-color: #FFFF0e;}\n",
       ".cython.score-163 {background-color: #FFFF0e;}\n",
       ".cython.score-164 {background-color: #FFFF0e;}\n",
       ".cython.score-165 {background-color: #FFFF0e;}\n",
       ".cython.score-166 {background-color: #FFFF0e;}\n",
       ".cython.score-167 {background-color: #FFFF0e;}\n",
       ".cython.score-168 {background-color: #FFFF0e;}\n",
       ".cython.score-169 {background-color: #FFFF0e;}\n",
       ".cython.score-170 {background-color: #FFFF0e;}\n",
       ".cython.score-171 {background-color: #FFFF0e;}\n",
       ".cython.score-172 {background-color: #FFFF0e;}\n",
       ".cython.score-173 {background-color: #FFFF0d;}\n",
       ".cython.score-174 {background-color: #FFFF0d;}\n",
       ".cython.score-175 {background-color: #FFFF0d;}\n",
       ".cython.score-176 {background-color: #FFFF0d;}\n",
       ".cython.score-177 {background-color: #FFFF0d;}\n",
       ".cython.score-178 {background-color: #FFFF0d;}\n",
       ".cython.score-179 {background-color: #FFFF0d;}\n",
       ".cython.score-180 {background-color: #FFFF0d;}\n",
       ".cython.score-181 {background-color: #FFFF0d;}\n",
       ".cython.score-182 {background-color: #FFFF0d;}\n",
       ".cython.score-183 {background-color: #FFFF0d;}\n",
       ".cython.score-184 {background-color: #FFFF0d;}\n",
       ".cython.score-185 {background-color: #FFFF0d;}\n",
       ".cython.score-186 {background-color: #FFFF0d;}\n",
       ".cython.score-187 {background-color: #FFFF0c;}\n",
       ".cython.score-188 {background-color: #FFFF0c;}\n",
       ".cython.score-189 {background-color: #FFFF0c;}\n",
       ".cython.score-190 {background-color: #FFFF0c;}\n",
       ".cython.score-191 {background-color: #FFFF0c;}\n",
       ".cython.score-192 {background-color: #FFFF0c;}\n",
       ".cython.score-193 {background-color: #FFFF0c;}\n",
       ".cython.score-194 {background-color: #FFFF0c;}\n",
       ".cython.score-195 {background-color: #FFFF0c;}\n",
       ".cython.score-196 {background-color: #FFFF0c;}\n",
       ".cython.score-197 {background-color: #FFFF0c;}\n",
       ".cython.score-198 {background-color: #FFFF0c;}\n",
       ".cython.score-199 {background-color: #FFFF0c;}\n",
       ".cython.score-200 {background-color: #FFFF0c;}\n",
       ".cython.score-201 {background-color: #FFFF0c;}\n",
       ".cython.score-202 {background-color: #FFFF0c;}\n",
       ".cython.score-203 {background-color: #FFFF0b;}\n",
       ".cython.score-204 {background-color: #FFFF0b;}\n",
       ".cython.score-205 {background-color: #FFFF0b;}\n",
       ".cython.score-206 {background-color: #FFFF0b;}\n",
       ".cython.score-207 {background-color: #FFFF0b;}\n",
       ".cython.score-208 {background-color: #FFFF0b;}\n",
       ".cython.score-209 {background-color: #FFFF0b;}\n",
       ".cython.score-210 {background-color: #FFFF0b;}\n",
       ".cython.score-211 {background-color: #FFFF0b;}\n",
       ".cython.score-212 {background-color: #FFFF0b;}\n",
       ".cython.score-213 {background-color: #FFFF0b;}\n",
       ".cython.score-214 {background-color: #FFFF0b;}\n",
       ".cython.score-215 {background-color: #FFFF0b;}\n",
       ".cython.score-216 {background-color: #FFFF0b;}\n",
       ".cython.score-217 {background-color: #FFFF0b;}\n",
       ".cython.score-218 {background-color: #FFFF0b;}\n",
       ".cython.score-219 {background-color: #FFFF0b;}\n",
       ".cython.score-220 {background-color: #FFFF0b;}\n",
       ".cython.score-221 {background-color: #FFFF0b;}\n",
       ".cython.score-222 {background-color: #FFFF0a;}\n",
       ".cython.score-223 {background-color: #FFFF0a;}\n",
       ".cython.score-224 {background-color: #FFFF0a;}\n",
       ".cython.score-225 {background-color: #FFFF0a;}\n",
       ".cython.score-226 {background-color: #FFFF0a;}\n",
       ".cython.score-227 {background-color: #FFFF0a;}\n",
       ".cython.score-228 {background-color: #FFFF0a;}\n",
       ".cython.score-229 {background-color: #FFFF0a;}\n",
       ".cython.score-230 {background-color: #FFFF0a;}\n",
       ".cython.score-231 {background-color: #FFFF0a;}\n",
       ".cython.score-232 {background-color: #FFFF0a;}\n",
       ".cython.score-233 {background-color: #FFFF0a;}\n",
       ".cython.score-234 {background-color: #FFFF0a;}\n",
       ".cython.score-235 {background-color: #FFFF0a;}\n",
       ".cython.score-236 {background-color: #FFFF0a;}\n",
       ".cython.score-237 {background-color: #FFFF0a;}\n",
       ".cython.score-238 {background-color: #FFFF0a;}\n",
       ".cython.score-239 {background-color: #FFFF0a;}\n",
       ".cython.score-240 {background-color: #FFFF0a;}\n",
       ".cython.score-241 {background-color: #FFFF0a;}\n",
       ".cython.score-242 {background-color: #FFFF0a;}\n",
       ".cython.score-243 {background-color: #FFFF0a;}\n",
       ".cython.score-244 {background-color: #FFFF0a;}\n",
       ".cython.score-245 {background-color: #FFFF0a;}\n",
       ".cython.score-246 {background-color: #FFFF09;}\n",
       ".cython.score-247 {background-color: #FFFF09;}\n",
       ".cython.score-248 {background-color: #FFFF09;}\n",
       ".cython.score-249 {background-color: #FFFF09;}\n",
       ".cython.score-250 {background-color: #FFFF09;}\n",
       ".cython.score-251 {background-color: #FFFF09;}\n",
       ".cython.score-252 {background-color: #FFFF09;}\n",
       ".cython.score-253 {background-color: #FFFF09;}\n",
       ".cython.score-254 {background-color: #FFFF09;}\n",
       ".cython .hll { background-color: #ffffcc }\n",
       ".cython  { background: #f8f8f8; }\n",
       ".cython .c { color: #408080; font-style: italic } /* Comment */\n",
       ".cython .err { border: 1px solid #FF0000 } /* Error */\n",
       ".cython .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".cython .o { color: #666666 } /* Operator */\n",
       ".cython .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".cython .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".cython .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".cython .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".cython .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".cython .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".cython .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".cython .ge { font-style: italic } /* Generic.Emph */\n",
       ".cython .gr { color: #FF0000 } /* Generic.Error */\n",
       ".cython .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".cython .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".cython .go { color: #888888 } /* Generic.Output */\n",
       ".cython .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".cython .gs { font-weight: bold } /* Generic.Strong */\n",
       ".cython .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".cython .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".cython .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".cython .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".cython .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".cython .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".cython .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".cython .kt { color: #B00040 } /* Keyword.Type */\n",
       ".cython .m { color: #666666 } /* Literal.Number */\n",
       ".cython .s { color: #BA2121 } /* Literal.String */\n",
       ".cython .na { color: #7D9029 } /* Name.Attribute */\n",
       ".cython .nb { color: #008000 } /* Name.Builtin */\n",
       ".cython .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".cython .no { color: #880000 } /* Name.Constant */\n",
       ".cython .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".cython .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".cython .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".cython .nf { color: #0000FF } /* Name.Function */\n",
       ".cython .nl { color: #A0A000 } /* Name.Label */\n",
       ".cython .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".cython .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".cython .nv { color: #19177C } /* Name.Variable */\n",
       ".cython .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".cython .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".cython .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".cython .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".cython .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".cython .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".cython .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".cython .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".cython .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".cython .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".cython .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".cython .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".cython .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".cython .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".cython .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".cython .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".cython .sx { color: #008000 } /* Literal.String.Other */\n",
       ".cython .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".cython .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".cython .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".cython .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".cython .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".cython .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".cython .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".cython .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".cython .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".cython .il { color: #666666 } /* Literal.Number.Integer.Long */\n",
       "    </style>\n",
       "</head>\n",
       "<body class=\"cython\">\n",
       "<p><span style=\"border-bottom: solid 1px grey;\">Generated by Cython 0.29.19</span></p>\n",
       "<p>\n",
       "    <span style=\"background-color: #FFFF00\">Yellow lines</span> hint at Python interaction.<br />\n",
       "    Click on a line that starts with a \"<code>+</code>\" to see the C code that Cython generated for it.\n",
       "</p>\n",
       "<div class=\"cython\"><pre class=\"cython line score-19\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">01</span>: <span class=\"k\">from</span> <span class=\"nn\">datetime</span> <span class=\"k\">import</span> <span class=\"n\">datetime</span> <span class=\"c\">#Needs to be re-imported inside the cell in Jupyter notebook </span></pre>\n",
       "<pre class='cython code score-19 '>  __pyx_t_1 = <span class='py_c_api'>PyList_New</span>(1);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 1, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_n_s_datetime);\n",
       "  <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_n_s_datetime);\n",
       "  <span class='py_macro_api'>PyList_SET_ITEM</span>(__pyx_t_1, 0, __pyx_n_s_datetime);\n",
       "  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_datetime, __pyx_t_1, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "  __pyx_t_1 = <span class='pyx_c_api'>__Pyx_ImportFrom</span>(__pyx_t_2, __pyx_n_s_datetime);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 1, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_datetime, __pyx_t_1) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 1, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-8\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">02</span>: <span class=\"k\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span></pre>\n",
       "<pre class='cython code score-8 '>  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_numpy, 0, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 2, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_np, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 2, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-8\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">03</span>: <span class=\"k\">import</span> <span class=\"nn\">feather</span></pre>\n",
       "<pre class='cython code score-8 '>  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_feather, 0, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 3, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_feather, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 3, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-8\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">04</span>: <span class=\"k\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span></pre>\n",
       "<pre class='cython code score-8 '>  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_pandas, 0, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 4, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_pd, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 4, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-16\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">05</span>: <span class=\"k\">import</span> <span class=\"nn\">os</span><span class=\"o\">,</span> <span class=\"nn\">sys</span></pre>\n",
       "<pre class='cython code score-16 '>  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_os, 0, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 5, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_os, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 5, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "  __pyx_t_2 = <span class='pyx_c_api'>__Pyx_Import</span>(__pyx_n_s_sys, 0, 0);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 5, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_sys, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 5, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">06</span>: </pre>\n",
       "<pre class=\"cython line score-55\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">07</span>: <span class=\"k\">def</span> <span class=\"nf\">get_wind_speed</span><span class=\"p\">(</span><span class=\"nb\">str</span> <span class=\"n\">input_date</span><span class=\"p\">,</span><span class=\"nb\">str</span> <span class=\"n\">file_path</span><span class=\"p\">):</span></pre>\n",
       "<pre class='cython code score-55 '>/* Python wrapper */\n",
       "static PyObject *__pyx_pw_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_1get_wind_speed(PyObject *__pyx_self, PyObject *__pyx_args, PyObject *__pyx_kwds); /*proto*/\n",
       "static char __pyx_doc_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_get_wind_speed[] = \"Create a dictionary for wind speed data on the input date \\n    Parameters\\n        input_date (str): input date for the date of interest, in the format: YYYY-MM-DD HH:MM\\n        file_path (str): path to the feather files containing the hourly data from Environment &amp; \\n        Climate Change Canada \\n    Returns \\n        ws_dictionary (dict): a dictionary of wind speed values for all the active &amp; non-null stations\\n        on the input date \\n    \";\n",
       "static PyMethodDef __pyx_mdef_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_1get_wind_speed = {\"get_wind_speed\", (PyCFunction)(void*)(PyCFunctionWithKeywords)__pyx_pw_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_1get_wind_speed, METH_VARARGS|METH_KEYWORDS, __pyx_doc_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_get_wind_speed};\n",
       "static PyObject *__pyx_pw_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_1get_wind_speed(PyObject *__pyx_self, PyObject *__pyx_args, PyObject *__pyx_kwds) {\n",
       "  PyObject *__pyx_v_input_date = 0;\n",
       "  PyObject *__pyx_v_file_path = 0;\n",
       "  PyObject *__pyx_r = 0;\n",
       "  <span class='refnanny'>__Pyx_RefNannyDeclarations</span>\n",
       "  <span class='refnanny'>__Pyx_RefNannySetupContext</span>(\"get_wind_speed (wrapper)\", 0);\n",
       "  {\n",
       "    static PyObject **__pyx_pyargnames[] = {&amp;__pyx_n_s_input_date,&amp;__pyx_n_s_file_path,0};\n",
       "    PyObject* values[2] = {0,0};\n",
       "    if (unlikely(__pyx_kwds)) {\n",
       "      Py_ssize_t kw_args;\n",
       "      const Py_ssize_t pos_args = <span class='py_macro_api'>PyTuple_GET_SIZE</span>(__pyx_args);\n",
       "      switch (pos_args) {\n",
       "        case  2: values[1] = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_args, 1);\n",
       "        CYTHON_FALLTHROUGH;\n",
       "        case  1: values[0] = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_args, 0);\n",
       "        CYTHON_FALLTHROUGH;\n",
       "        case  0: break;\n",
       "        default: goto __pyx_L5_argtuple_error;\n",
       "      }\n",
       "      kw_args = <span class='py_c_api'>PyDict_Size</span>(__pyx_kwds);\n",
       "      switch (pos_args) {\n",
       "        case  0:\n",
       "        if (likely((values[0] = <span class='pyx_c_api'>__Pyx_PyDict_GetItemStr</span>(__pyx_kwds, __pyx_n_s_input_date)) != 0)) kw_args--;\n",
       "        else goto __pyx_L5_argtuple_error;\n",
       "        CYTHON_FALLTHROUGH;\n",
       "        case  1:\n",
       "        if (likely((values[1] = <span class='pyx_c_api'>__Pyx_PyDict_GetItemStr</span>(__pyx_kwds, __pyx_n_s_file_path)) != 0)) kw_args--;\n",
       "        else {\n",
       "          <span class='pyx_c_api'>__Pyx_RaiseArgtupleInvalid</span>(\"get_wind_speed\", 1, 2, 2, 1); <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L3_error)</span>\n",
       "        }\n",
       "      }\n",
       "      if (unlikely(kw_args &gt; 0)) {\n",
       "        if (unlikely(<span class='pyx_c_api'>__Pyx_ParseOptionalKeywords</span>(__pyx_kwds, __pyx_pyargnames, 0, values, pos_args, \"get_wind_speed\") &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L3_error)</span>\n",
       "      }\n",
       "    } else if (<span class='py_macro_api'>PyTuple_GET_SIZE</span>(__pyx_args) != 2) {\n",
       "      goto __pyx_L5_argtuple_error;\n",
       "    } else {\n",
       "      values[0] = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_args, 0);\n",
       "      values[1] = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_args, 1);\n",
       "    }\n",
       "    __pyx_v_input_date = ((PyObject*)values[0]);\n",
       "    __pyx_v_file_path = ((PyObject*)values[1]);\n",
       "  }\n",
       "  goto __pyx_L4_argument_unpacking_done;\n",
       "  __pyx_L5_argtuple_error:;\n",
       "  <span class='pyx_c_api'>__Pyx_RaiseArgtupleInvalid</span>(\"get_wind_speed\", 1, 2, 2, <span class='py_macro_api'>PyTuple_GET_SIZE</span>(__pyx_args)); <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L3_error)</span>\n",
       "  __pyx_L3_error:;\n",
       "  <span class='pyx_c_api'>__Pyx_AddTraceback</span>(\"_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c.get_wind_speed\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n",
       "  <span class='refnanny'>__Pyx_RefNannyFinishContext</span>();\n",
       "  return NULL;\n",
       "  __pyx_L4_argument_unpacking_done:;\n",
       "  if (unlikely(!<span class='pyx_c_api'>__Pyx_ArgTypeTest</span>(((PyObject *)__pyx_v_input_date), (&amp;PyUnicode_Type), 1, \"input_date\", 1))) <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L1_error)</span>\n",
       "  if (unlikely(!<span class='pyx_c_api'>__Pyx_ArgTypeTest</span>(((PyObject *)__pyx_v_file_path), (&amp;PyUnicode_Type), 1, \"file_path\", 1))) <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L1_error)</span>\n",
       "  __pyx_r = __pyx_pf_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_get_wind_speed(__pyx_self, __pyx_v_input_date, __pyx_v_file_path);\n",
       "  int __pyx_lineno = 0;\n",
       "  const char *__pyx_filename = NULL;\n",
       "  int __pyx_clineno = 0;\n",
       "\n",
       "  /* function exit code */\n",
       "  goto __pyx_L0;\n",
       "  __pyx_L1_error:;\n",
       "  __pyx_r = NULL;\n",
       "  __pyx_L0:;\n",
       "  <span class='refnanny'>__Pyx_RefNannyFinishContext</span>();\n",
       "  return __pyx_r;\n",
       "}\n",
       "\n",
       "static PyObject *__pyx_pf_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_get_wind_speed(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_input_date, PyObject *__pyx_v_file_path) {\n",
       "  PyObject *__pyx_v_station_name = 0;\n",
       "  PyObject *__pyx_v_file_name = 0;\n",
       "  PyObject *__pyx_v_ws_dictionary = 0;\n",
       "  CYTHON_UNUSED PyObject *__pyx_v_search_date = NULL;\n",
       "  PyObject *__pyx_v_file = NULL;\n",
       "  PyObject *__pyx_v_df = NULL;\n",
       "  PyObject *__pyx_r = NULL;\n",
       "  <span class='refnanny'>__Pyx_RefNannyDeclarations</span>\n",
       "  <span class='refnanny'>__Pyx_RefNannySetupContext</span>(\"get_wind_speed\", 0);\n",
       "/* … */\n",
       "  /* function exit code */\n",
       "  __pyx_L1_error:;\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_1);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_2);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_3);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_5);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_8);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_16);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_17);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_18);\n",
       "  <span class='pyx_c_api'>__Pyx_AddTraceback</span>(\"_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c.get_wind_speed\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n",
       "  __pyx_r = NULL;\n",
       "  __pyx_L0:;\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_station_name);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_file_name);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_ws_dictionary);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_search_date);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_file);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_v_df);\n",
       "  <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_r);\n",
       "  <span class='refnanny'>__Pyx_RefNannyFinishContext</span>();\n",
       "  return __pyx_r;\n",
       "}\n",
       "/* … */\n",
       "  __pyx_tuple__2 = <span class='py_c_api'>PyTuple_Pack</span>(8, __pyx_n_s_input_date, __pyx_n_s_file_path, __pyx_n_s_station_name, __pyx_n_s_file_name, __pyx_n_s_ws_dictionary, __pyx_n_s_search_date, __pyx_n_s_file, __pyx_n_s_df);<span class='error_goto'> if (unlikely(!__pyx_tuple__2)) __PYX_ERR(0, 7, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_tuple__2);\n",
       "  <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_tuple__2);\n",
       "/* … */\n",
       "  __pyx_t_2 = PyCFunction_NewEx(&amp;__pyx_mdef_46_cython_magic_ccf4ef6244d44c49476183f3ccd3d25c_1get_wind_speed, NULL, __pyx_n_s_cython_magic_ccf4ef6244d44c4947);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 7, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  if (<span class='py_c_api'>PyDict_SetItem</span>(__pyx_d, __pyx_n_s_get_wind_speed, __pyx_t_2) &lt; 0) <span class='error_goto'>__PYX_ERR(0, 7, __pyx_L1_error)</span>\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">08</span>:     <span class=\"sd\">&#39;&#39;&#39;Create a dictionary for wind speed data on the input date </span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">09</span>: <span class=\"sd\">    Parameters</span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">10</span>: <span class=\"sd\">        input_date (str): input date for the date of interest, in the format: YYYY-MM-DD HH:MM</span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">11</span>: <span class=\"sd\">        file_path (str): path to the feather files containing the hourly data from Environment &amp; </span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">12</span>: <span class=\"sd\">        Climate Change Canada </span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">13</span>: <span class=\"sd\">    Returns </span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">14</span>: <span class=\"sd\">        ws_dictionary (dict): a dictionary of wind speed values for all the active &amp; non-null stations</span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">15</span>: <span class=\"sd\">        on the input date </span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">16</span>: <span class=\"sd\">    &#39;&#39;&#39;</span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">17</span>:     <span class=\"k\">cdef</span> <span class=\"kt\">str</span> <span class=\"nf\">station_name</span><span class=\"p\">,</span> <span class=\"nf\">file_name</span> <span class=\"c\">#Declare the C data types </span></pre>\n",
       "<pre class=\"cython line score-2\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">18</span>:     <span class=\"k\">cdef</span> <span class=\"kt\">dict</span> <span class=\"nf\">ws_dictionary</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></pre>\n",
       "<pre class='cython code score-2 '>  __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyDict_NewPresized</span>(0);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 18, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  __pyx_v_ws_dictionary = ((PyObject*)__pyx_t_1);\n",
       "  __pyx_t_1 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">19</span>: </pre>\n",
       "<pre class=\"cython line score-42\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">20</span>:     <span class=\"n\">search_date</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">strptime</span><span class=\"p\">(</span><span class=\"n\">input_date</span><span class=\"p\">,</span> <span class=\"s\">&#39;%Y-%m-</span><span class=\"si\">%d</span><span class=\"s\"> %H:%M&#39;</span><span class=\"p\">)</span> <span class=\"c\"># Get the datetime object for input date</span></pre>\n",
       "<pre class='cython code score-42 '>  <span class='pyx_c_api'>__Pyx_GetModuleGlobalName</span>(__pyx_t_2, __pyx_n_s_datetime);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "  __pyx_t_3 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_2, __pyx_n_s_strptime);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "  __pyx_t_2 = NULL;\n",
       "  __pyx_t_4 = 0;\n",
       "  if (CYTHON_UNPACK_METHODS &amp;&amp; unlikely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_3))) {\n",
       "    __pyx_t_2 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_3);\n",
       "    if (likely(__pyx_t_2)) {\n",
       "      PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_3);\n",
       "      <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_2);\n",
       "      <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "      <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_3, function);\n",
       "      __pyx_t_4 = 1;\n",
       "    }\n",
       "  }\n",
       "  #if CYTHON_FAST_PYCALL\n",
       "  if (<span class='py_c_api'>PyFunction_Check</span>(__pyx_t_3)) {\n",
       "    PyObject *__pyx_temp[3] = {__pyx_t_2, __pyx_v_input_date, __pyx_kp_u_Y_m_d_H_M};\n",
       "    __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyFunction_FastCall</span>(__pyx_t_3, __pyx_temp+1-__pyx_t_4, 2+__pyx_t_4);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "    <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  } else\n",
       "  #endif\n",
       "  #if CYTHON_FAST_PYCCALL\n",
       "  if (<span class='pyx_c_api'>__Pyx_PyFastCFunction_Check</span>(__pyx_t_3)) {\n",
       "    PyObject *__pyx_temp[3] = {__pyx_t_2, __pyx_v_input_date, __pyx_kp_u_Y_m_d_H_M};\n",
       "    __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyCFunction_FastCall</span>(__pyx_t_3, __pyx_temp+1-__pyx_t_4, 2+__pyx_t_4);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "    <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  } else\n",
       "  #endif\n",
       "  {\n",
       "    __pyx_t_5 = <span class='py_c_api'>PyTuple_New</span>(2+__pyx_t_4);<span class='error_goto'> if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_5);\n",
       "    if (__pyx_t_2) {\n",
       "      <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_t_2); <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_5, 0, __pyx_t_2); __pyx_t_2 = NULL;\n",
       "    }\n",
       "    <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_v_input_date);\n",
       "    <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_v_input_date);\n",
       "    <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_5, 0+__pyx_t_4, __pyx_v_input_date);\n",
       "    <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_kp_u_Y_m_d_H_M);\n",
       "    <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_kp_u_Y_m_d_H_M);\n",
       "    <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_5, 1+__pyx_t_4, __pyx_kp_u_Y_m_d_H_M);\n",
       "    __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyObject_Call</span>(__pyx_t_3, __pyx_t_5, NULL);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 20, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_5); __pyx_t_5 = 0;\n",
       "  }\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "  __pyx_v_search_date = __pyx_t_1;\n",
       "  __pyx_t_1 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">21</span>: </pre>\n",
       "<pre class=\"cython line score-73\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">22</span>:     <span class=\"k\">for</span> <span class=\"n\">station_name</span> <span class=\"ow\">in</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">listdir</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">):</span></pre>\n",
       "<pre class='cython code score-73 '>  <span class='pyx_c_api'>__Pyx_GetModuleGlobalName</span>(__pyx_t_3, __pyx_n_s_os);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "  __pyx_t_5 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_3, __pyx_n_s_listdir);<span class='error_goto'> if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_5);\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "  __pyx_t_3 = NULL;\n",
       "  if (CYTHON_UNPACK_METHODS &amp;&amp; unlikely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_5))) {\n",
       "    __pyx_t_3 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_5);\n",
       "    if (likely(__pyx_t_3)) {\n",
       "      PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_5);\n",
       "      <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_3);\n",
       "      <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "      <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_5, function);\n",
       "    }\n",
       "  }\n",
       "  __pyx_t_1 = (__pyx_t_3) ? __Pyx_PyObject_Call2Args(__pyx_t_5, __pyx_t_3, __pyx_v_file_path) : <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_5, __pyx_v_file_path);\n",
       "  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "  if (unlikely(!__pyx_t_1)) <span class='error_goto'>__PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "  <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_5); __pyx_t_5 = 0;\n",
       "  if (likely(<span class='py_c_api'>PyList_CheckExact</span>(__pyx_t_1)) || <span class='py_c_api'>PyTuple_CheckExact</span>(__pyx_t_1)) {\n",
       "    __pyx_t_5 = __pyx_t_1; <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_5); __pyx_t_6 = 0;\n",
       "    __pyx_t_7 = NULL;\n",
       "  } else {\n",
       "    __pyx_t_6 = -1; __pyx_t_5 = <span class='py_c_api'>PyObject_GetIter</span>(__pyx_t_1);<span class='error_goto'> if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_5);\n",
       "    __pyx_t_7 = Py_TYPE(__pyx_t_5)-&gt;tp_iternext;<span class='error_goto'> if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "  }\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "  for (;;) {\n",
       "    if (likely(!__pyx_t_7)) {\n",
       "      if (likely(<span class='py_c_api'>PyList_CheckExact</span>(__pyx_t_5))) {\n",
       "        if (__pyx_t_6 &gt;= <span class='py_macro_api'>PyList_GET_SIZE</span>(__pyx_t_5)) break;\n",
       "        #if CYTHON_ASSUME_SAFE_MACROS &amp;&amp; !CYTHON_AVOID_BORROWED_REFS\n",
       "        __pyx_t_1 = <span class='py_macro_api'>PyList_GET_ITEM</span>(__pyx_t_5, __pyx_t_6); <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_1); __pyx_t_6++; if (unlikely(0 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "        #else\n",
       "        __pyx_t_1 = <span class='py_macro_api'>PySequence_ITEM</span>(__pyx_t_5, __pyx_t_6); __pyx_t_6++;<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "        <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "        #endif\n",
       "      } else {\n",
       "        if (__pyx_t_6 &gt;= <span class='py_macro_api'>PyTuple_GET_SIZE</span>(__pyx_t_5)) break;\n",
       "        #if CYTHON_ASSUME_SAFE_MACROS &amp;&amp; !CYTHON_AVOID_BORROWED_REFS\n",
       "        __pyx_t_1 = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_t_5, __pyx_t_6); <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_1); __pyx_t_6++; if (unlikely(0 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "        #else\n",
       "        __pyx_t_1 = <span class='py_macro_api'>PySequence_ITEM</span>(__pyx_t_5, __pyx_t_6); __pyx_t_6++;<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "        <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "        #endif\n",
       "      }\n",
       "    } else {\n",
       "      __pyx_t_1 = __pyx_t_7(__pyx_t_5);\n",
       "      if (unlikely(!__pyx_t_1)) {\n",
       "        PyObject* exc_type = <span class='py_c_api'>PyErr_Occurred</span>();\n",
       "        if (exc_type) {\n",
       "          if (likely(<span class='pyx_c_api'>__Pyx_PyErr_GivenExceptionMatches</span>(exc_type, PyExc_StopIteration))) <span class='py_c_api'>PyErr_Clear</span>();\n",
       "          else <span class='error_goto'>__PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "        }\n",
       "        break;\n",
       "      }\n",
       "      <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "    }\n",
       "    if (!(likely(<span class='py_c_api'>PyUnicode_CheckExact</span>(__pyx_t_1))||((__pyx_t_1) == Py_None)||(<span class='py_c_api'>PyErr_Format</span>(PyExc_TypeError, \"Expected %.16s, got %.200s\", \"unicode\", Py_TYPE(__pyx_t_1)-&gt;tp_name), 0))) <span class='error_goto'>__PYX_ERR(0, 22, __pyx_L1_error)</span>\n",
       "    <span class='pyx_macro_api'>__Pyx_XDECREF_SET</span>(__pyx_v_station_name, ((PyObject*)__pyx_t_1));\n",
       "    __pyx_t_1 = 0;\n",
       "/* … */\n",
       "  }\n",
       "  <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_5); __pyx_t_5 = 0;\n",
       "</pre><pre class=\"cython line score-79\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">23</span>:         <span class=\"k\">for</span> <span class=\"n\">file_name</span> <span class=\"ow\">in</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">listdir</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"o\">+</span><span class=\"n\">station_name</span><span class=\"o\">+</span><span class=\"s\">&#39;/&#39;</span><span class=\"p\">):</span></pre>\n",
       "<pre class='cython code score-79 '>    <span class='pyx_c_api'>__Pyx_GetModuleGlobalName</span>(__pyx_t_3, __pyx_n_s_os);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "    __pyx_t_2 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_3, __pyx_n_s_listdir);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "    __pyx_t_3 = <span class='pyx_c_api'>__Pyx_PyUnicode_ConcatSafe</span>(__pyx_v_file_path, __pyx_v_station_name);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "    __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyUnicode_Concat</span>(__pyx_t_3, __pyx_kp_u_);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "    __pyx_t_3 = NULL;\n",
       "    if (CYTHON_UNPACK_METHODS &amp;&amp; unlikely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_2))) {\n",
       "      __pyx_t_3 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_2);\n",
       "      if (likely(__pyx_t_3)) {\n",
       "        PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_2);\n",
       "        <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_3);\n",
       "        <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "        <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_2, function);\n",
       "      }\n",
       "    }\n",
       "    __pyx_t_1 = (__pyx_t_3) ? __Pyx_PyObject_Call2Args(__pyx_t_2, __pyx_t_3, __pyx_t_8) : <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_2, __pyx_t_8);\n",
       "    <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "    if (unlikely(!__pyx_t_1)) <span class='error_goto'>__PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "    if (likely(<span class='py_c_api'>PyList_CheckExact</span>(__pyx_t_1)) || <span class='py_c_api'>PyTuple_CheckExact</span>(__pyx_t_1)) {\n",
       "      __pyx_t_2 = __pyx_t_1; <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_2); __pyx_t_9 = 0;\n",
       "      __pyx_t_10 = NULL;\n",
       "    } else {\n",
       "      __pyx_t_9 = -1; __pyx_t_2 = <span class='py_c_api'>PyObject_GetIter</span>(__pyx_t_1);<span class='error_goto'> if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "      <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_2);\n",
       "      __pyx_t_10 = Py_TYPE(__pyx_t_2)-&gt;tp_iternext;<span class='error_goto'> if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "    }\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "    for (;;) {\n",
       "      if (likely(!__pyx_t_10)) {\n",
       "        if (likely(<span class='py_c_api'>PyList_CheckExact</span>(__pyx_t_2))) {\n",
       "          if (__pyx_t_9 &gt;= <span class='py_macro_api'>PyList_GET_SIZE</span>(__pyx_t_2)) break;\n",
       "          #if CYTHON_ASSUME_SAFE_MACROS &amp;&amp; !CYTHON_AVOID_BORROWED_REFS\n",
       "          __pyx_t_1 = <span class='py_macro_api'>PyList_GET_ITEM</span>(__pyx_t_2, __pyx_t_9); <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_1); __pyx_t_9++; if (unlikely(0 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "          #else\n",
       "          __pyx_t_1 = <span class='py_macro_api'>PySequence_ITEM</span>(__pyx_t_2, __pyx_t_9); __pyx_t_9++;<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "          #endif\n",
       "        } else {\n",
       "          if (__pyx_t_9 &gt;= <span class='py_macro_api'>PyTuple_GET_SIZE</span>(__pyx_t_2)) break;\n",
       "          #if CYTHON_ASSUME_SAFE_MACROS &amp;&amp; !CYTHON_AVOID_BORROWED_REFS\n",
       "          __pyx_t_1 = <span class='py_macro_api'>PyTuple_GET_ITEM</span>(__pyx_t_2, __pyx_t_9); <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_1); __pyx_t_9++; if (unlikely(0 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "          #else\n",
       "          __pyx_t_1 = <span class='py_macro_api'>PySequence_ITEM</span>(__pyx_t_2, __pyx_t_9); __pyx_t_9++;<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "          #endif\n",
       "        }\n",
       "      } else {\n",
       "        __pyx_t_1 = __pyx_t_10(__pyx_t_2);\n",
       "        if (unlikely(!__pyx_t_1)) {\n",
       "          PyObject* exc_type = <span class='py_c_api'>PyErr_Occurred</span>();\n",
       "          if (exc_type) {\n",
       "            if (likely(<span class='pyx_c_api'>__Pyx_PyErr_GivenExceptionMatches</span>(exc_type, PyExc_StopIteration))) <span class='py_c_api'>PyErr_Clear</span>();\n",
       "            else <span class='error_goto'>__PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "          }\n",
       "          break;\n",
       "        }\n",
       "        <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "      }\n",
       "      if (!(likely(<span class='py_c_api'>PyUnicode_CheckExact</span>(__pyx_t_1))||((__pyx_t_1) == Py_None)||(<span class='py_c_api'>PyErr_Format</span>(PyExc_TypeError, \"Expected %.16s, got %.200s\", \"unicode\", Py_TYPE(__pyx_t_1)-&gt;tp_name), 0))) <span class='error_goto'>__PYX_ERR(0, 23, __pyx_L1_error)</span>\n",
       "      <span class='pyx_macro_api'>__Pyx_XDECREF_SET</span>(__pyx_v_file_name, ((PyObject*)__pyx_t_1));\n",
       "      __pyx_t_1 = 0;\n",
       "/* … */\n",
       "    }\n",
       "    <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_2); __pyx_t_2 = 0;\n",
       "</pre><pre class=\"cython line score-18\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">24</span>:             <span class=\"k\">if</span> <span class=\"n\">input_date</span><span class=\"p\">[</span><span class=\"mf\">5</span><span class=\"p\">:</span><span class=\"mf\">7</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">file_name</span><span class=\"p\">[</span><span class=\"mf\">29</span><span class=\"p\">:</span><span class=\"mf\">31</span><span class=\"p\">]:</span> <span class=\"c\">#This is a trick to speed up the code, only look at files which have the month/day in the name</span></pre>\n",
       "<pre class='cython code score-18 '>      if (unlikely(__pyx_v_input_date == Py_None)) {\n",
       "        <span class='py_c_api'>PyErr_SetString</span>(PyExc_TypeError, \"'NoneType' object is not subscriptable\");\n",
       "        <span class='error_goto'>__PYX_ERR(0, 24, __pyx_L1_error)</span>\n",
       "      }\n",
       "      __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyUnicode_Substring</span>(__pyx_v_input_date, 5, 7);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 24, __pyx_L1_error)</span>\n",
       "      <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "      if (unlikely(__pyx_v_file_name == Py_None)) {\n",
       "        <span class='py_c_api'>PyErr_SetString</span>(PyExc_TypeError, \"'NoneType' object is not subscriptable\");\n",
       "        <span class='error_goto'>__PYX_ERR(0, 24, __pyx_L1_error)</span>\n",
       "      }\n",
       "      __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyUnicode_Substring</span>(__pyx_v_file_name, 29, 31);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 24, __pyx_L1_error)</span>\n",
       "      <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "      __pyx_t_11 = (<span class='pyx_c_api'>__Pyx_PyUnicode_Equals</span>(__pyx_t_1, __pyx_t_8, Py_EQ)); if (unlikely(__pyx_t_11 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 24, __pyx_L1_error)</span>\n",
       "      <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "      <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "      __pyx_t_12 = (__pyx_t_11 != 0);\n",
       "      if (__pyx_t_12) {\n",
       "/* … */\n",
       "      }\n",
       "</pre><pre class=\"cython line score-18\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">25</span>:                 <span class=\"k\">if</span> <span class=\"n\">input_date</span><span class=\"p\">[</span><span class=\"mf\">0</span><span class=\"p\">:</span><span class=\"mf\">4</span><span class=\"p\">]</span><span class=\"o\">==</span> <span class=\"n\">file_name</span><span class=\"p\">[</span><span class=\"mf\">32</span><span class=\"p\">:</span><span class=\"mf\">36</span><span class=\"p\">]:</span></pre>\n",
       "<pre class='cython code score-18 '>        if (unlikely(__pyx_v_input_date == Py_None)) {\n",
       "          <span class='py_c_api'>PyErr_SetString</span>(PyExc_TypeError, \"'NoneType' object is not subscriptable\");\n",
       "          <span class='error_goto'>__PYX_ERR(0, 25, __pyx_L1_error)</span>\n",
       "        }\n",
       "        __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyUnicode_Substring</span>(__pyx_v_input_date, 0, 4);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 25, __pyx_L1_error)</span>\n",
       "        <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "        if (unlikely(__pyx_v_file_name == Py_None)) {\n",
       "          <span class='py_c_api'>PyErr_SetString</span>(PyExc_TypeError, \"'NoneType' object is not subscriptable\");\n",
       "          <span class='error_goto'>__PYX_ERR(0, 25, __pyx_L1_error)</span>\n",
       "        }\n",
       "        __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyUnicode_Substring</span>(__pyx_v_file_name, 32, 36);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 25, __pyx_L1_error)</span>\n",
       "        <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "        __pyx_t_12 = (<span class='pyx_c_api'>__Pyx_PyUnicode_Equals</span>(__pyx_t_8, __pyx_t_1, Py_EQ)); if (unlikely(__pyx_t_12 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 25, __pyx_L1_error)</span>\n",
       "        <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "        <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "        __pyx_t_11 = (__pyx_t_12 != 0);\n",
       "        if (__pyx_t_11) {\n",
       "/* … */\n",
       "        }\n",
       "</pre><pre class=\"cython line score-9\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">26</span>:                     <span class=\"nb\">file</span> <span class=\"o\">=</span> <span class=\"n\">file_path</span><span class=\"o\">+</span><span class=\"n\">station_name</span><span class=\"o\">+</span><span class=\"s\">&#39;/&#39;</span><span class=\"o\">+</span><span class=\"n\">file_name</span></pre>\n",
       "<pre class='cython code score-9 '>          __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyUnicode_ConcatSafe</span>(__pyx_v_file_path, __pyx_v_station_name);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 26, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "          __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyUnicode_Concat</span>(__pyx_t_1, __pyx_kp_u_);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 26, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "          <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "          __pyx_t_1 = <span class='pyx_c_api'>__Pyx_PyUnicode_ConcatSafe</span>(__pyx_t_8, __pyx_v_file_name);<span class='error_goto'> if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 26, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "          <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "          <span class='pyx_macro_api'>__Pyx_XDECREF_SET</span>(__pyx_v_file, ((PyObject*)__pyx_t_1));\n",
       "          __pyx_t_1 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">27</span>: </pre>\n",
       "<pre class=\"cython line score-20\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">28</span>:                     <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">feather</span><span class=\"o\">.</span><span class=\"n\">read_dataframe</span><span class=\"p\">(</span><span class=\"nb\">file</span><span class=\"p\">)</span></pre>\n",
       "<pre class='cython code score-20 '>          <span class='pyx_c_api'>__Pyx_GetModuleGlobalName</span>(__pyx_t_8, __pyx_n_s_feather);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 28, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "          __pyx_t_3 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_8, __pyx_n_s_read_dataframe);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 28, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "          <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "          __pyx_t_8 = NULL;\n",
       "          if (CYTHON_UNPACK_METHODS &amp;&amp; unlikely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_3))) {\n",
       "            __pyx_t_8 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_3);\n",
       "            if (likely(__pyx_t_8)) {\n",
       "              PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_3);\n",
       "              <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_8);\n",
       "              <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_3, function);\n",
       "            }\n",
       "          }\n",
       "          __pyx_t_1 = (__pyx_t_8) ? __Pyx_PyObject_Call2Args(__pyx_t_3, __pyx_t_8, __pyx_v_file) : <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_3, __pyx_v_file);\n",
       "          <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "          if (unlikely(!__pyx_t_1)) <span class='error_goto'>__PYX_ERR(0, 28, __pyx_L1_error)</span>\n",
       "          <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "          <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "          <span class='pyx_macro_api'>__Pyx_XDECREF_SET</span>(__pyx_v_df, __pyx_t_1);\n",
       "          __pyx_t_1 = 0;\n",
       "</pre><pre class=\"cython line score-13\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">29</span>:                     <span class=\"k\">try</span><span class=\"p\">:</span></pre>\n",
       "<pre class='cython code score-13 '>          {\n",
       "            /*try:*/ {\n",
       "/* … */\n",
       "            }\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_13); __pyx_t_13 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_14); __pyx_t_14 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_15); __pyx_t_15 = 0;\n",
       "            goto __pyx_L16_try_end;\n",
       "            __pyx_L9_error:;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_16); __pyx_t_16 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_18); __pyx_t_18 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "            <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "/* … */\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_13);\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_14);\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_15);\n",
       "            <span class='pyx_c_api'>__Pyx_ExceptionReset</span>(__pyx_t_13, __pyx_t_14, __pyx_t_15);\n",
       "            goto __pyx_L1_error;\n",
       "            __pyx_L10_exception_handled:;\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_13);\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_14);\n",
       "            <span class='refnanny'>__Pyx_XGIVEREF</span>(__pyx_t_15);\n",
       "            <span class='pyx_c_api'>__Pyx_ExceptionReset</span>(__pyx_t_13, __pyx_t_14, __pyx_t_15);\n",
       "            __pyx_L16_try_end:;\n",
       "          }\n",
       "</pre><pre class=\"cython line score-64\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">30</span>:                         <span class=\"k\">if</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">notnull</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">&#39;Date/Time&#39;</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">input_date</span><span class=\"p\">,</span> <span class=\"s\">&#39;Wind Spd (km/h)&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()):</span></pre>\n",
       "<pre class='cython code score-64 '>              <span class='pyx_c_api'>__Pyx_GetModuleGlobalName</span>(__pyx_t_3, __pyx_n_s_pd);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "              __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_3, __pyx_n_s_notnull);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "              __pyx_t_16 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_v_df, __pyx_n_s_loc);<span class='error_goto'> if (unlikely(!__pyx_t_16)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_16);\n",
       "              __pyx_t_17 = <span class='pyx_c_api'>__Pyx_PyObject_Dict_GetItem</span>(__pyx_v_df, __pyx_kp_u_Date_Time);<span class='error_goto'> if (unlikely(!__pyx_t_17)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_17);\n",
       "              __pyx_t_18 = <span class='py_c_api'>PyObject_RichCompare</span>(__pyx_t_17, __pyx_v_input_date, Py_EQ); <span class='refnanny'>__Pyx_XGOTREF</span>(__pyx_t_18);<span class='error_goto'> if (unlikely(!__pyx_t_18)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "              __pyx_t_17 = <span class='py_c_api'>PyTuple_New</span>(2);<span class='error_goto'> if (unlikely(!__pyx_t_17)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_17);\n",
       "              <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_t_18);\n",
       "              <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_17, 0, __pyx_t_18);\n",
       "              <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_kp_u_Wind_Spd_km_h);\n",
       "              <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_kp_u_Wind_Spd_km_h);\n",
       "              <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_17, 1, __pyx_kp_u_Wind_Spd_km_h);\n",
       "              __pyx_t_18 = 0;\n",
       "              __pyx_t_18 = <span class='pyx_c_api'>__Pyx_PyObject_GetItem</span>(__pyx_t_16, __pyx_t_17);<span class='error_goto'> if (unlikely(!__pyx_t_18)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_18);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_16); __pyx_t_16 = 0;\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "              __pyx_t_17 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_18, __pyx_n_s_item);<span class='error_goto'> if (unlikely(!__pyx_t_17)) __PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_17);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_18); __pyx_t_18 = 0;\n",
       "              __pyx_t_18 = NULL;\n",
       "              if (CYTHON_UNPACK_METHODS &amp;&amp; likely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_17))) {\n",
       "                __pyx_t_18 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_17);\n",
       "                if (likely(__pyx_t_18)) {\n",
       "                  PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_17);\n",
       "                  <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_18);\n",
       "                  <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "                  <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_17, function);\n",
       "                }\n",
       "              }\n",
       "              __pyx_t_3 = (__pyx_t_18) ? <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_17, __pyx_t_18) : <span class='pyx_c_api'>__Pyx_PyObject_CallNoArg</span>(__pyx_t_17);\n",
       "              <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_18); __pyx_t_18 = 0;\n",
       "              if (unlikely(!__pyx_t_3)) <span class='error_goto'>__PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "              __pyx_t_17 = NULL;\n",
       "              if (CYTHON_UNPACK_METHODS &amp;&amp; unlikely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_8))) {\n",
       "                __pyx_t_17 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_8);\n",
       "                if (likely(__pyx_t_17)) {\n",
       "                  PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_8);\n",
       "                  <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_17);\n",
       "                  <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "                  <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_8, function);\n",
       "                }\n",
       "              }\n",
       "              __pyx_t_1 = (__pyx_t_17) ? __Pyx_PyObject_Call2Args(__pyx_t_8, __pyx_t_17, __pyx_t_3) : <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_8, __pyx_t_3);\n",
       "              <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "              if (unlikely(!__pyx_t_1)) <span class='error_goto'>__PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "              __pyx_t_11 = <span class='pyx_c_api'>__Pyx_PyObject_IsTrue</span>(__pyx_t_1); if (unlikely(__pyx_t_11 &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 30, __pyx_L9_error)</span>\n",
       "              <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "              if (__pyx_t_11) {\n",
       "/* … */\n",
       "                goto __pyx_L17;\n",
       "              }\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">31</span>: </pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">32</span>:                             <span class=\"c\">#Put the value into the dictionary. </span></pre>\n",
       "<pre class=\"cython line score-47\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">33</span>:                             <span class=\"n\">ws_dictionary</span><span class=\"p\">[</span><span class=\"n\">station_name</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">&#39;Date/Time&#39;</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">input_date</span><span class=\"p\">,</span> <span class=\"s\">&#39;Wind Spd (km/h)&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span></pre>\n",
       "<pre class='cython code score-47 '>                __pyx_t_8 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_v_df, __pyx_n_s_loc);<span class='error_goto'> if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_8);\n",
       "                __pyx_t_3 = <span class='pyx_c_api'>__Pyx_PyObject_Dict_GetItem</span>(__pyx_v_df, __pyx_kp_u_Date_Time);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "                __pyx_t_17 = <span class='py_c_api'>PyObject_RichCompare</span>(__pyx_t_3, __pyx_v_input_date, Py_EQ); <span class='refnanny'>__Pyx_XGOTREF</span>(__pyx_t_17);<span class='error_goto'> if (unlikely(!__pyx_t_17)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "                __pyx_t_3 = <span class='py_c_api'>PyTuple_New</span>(2);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "                <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_t_17);\n",
       "                <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_3, 0, __pyx_t_17);\n",
       "                <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_kp_u_Wind_Spd_km_h);\n",
       "                <span class='refnanny'>__Pyx_GIVEREF</span>(__pyx_kp_u_Wind_Spd_km_h);\n",
       "                <span class='py_macro_api'>PyTuple_SET_ITEM</span>(__pyx_t_3, 1, __pyx_kp_u_Wind_Spd_km_h);\n",
       "                __pyx_t_17 = 0;\n",
       "                __pyx_t_17 = <span class='pyx_c_api'>__Pyx_PyObject_GetItem</span>(__pyx_t_8, __pyx_t_3);<span class='error_goto'> if (unlikely(!__pyx_t_17)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_17);\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_8); __pyx_t_8 = 0;\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "                __pyx_t_3 = <span class='pyx_c_api'>__Pyx_PyObject_GetAttrStr</span>(__pyx_t_17, __pyx_n_s_item);<span class='error_goto'> if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_3);\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "                __pyx_t_17 = NULL;\n",
       "                if (CYTHON_UNPACK_METHODS &amp;&amp; likely(<span class='py_c_api'>PyMethod_Check</span>(__pyx_t_3))) {\n",
       "                  __pyx_t_17 = <span class='py_macro_api'>PyMethod_GET_SELF</span>(__pyx_t_3);\n",
       "                  if (likely(__pyx_t_17)) {\n",
       "                    PyObject* function = <span class='py_macro_api'>PyMethod_GET_FUNCTION</span>(__pyx_t_3);\n",
       "                    <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_t_17);\n",
       "                    <span class='pyx_macro_api'>__Pyx_INCREF</span>(function);\n",
       "                    <span class='pyx_macro_api'>__Pyx_DECREF_SET</span>(__pyx_t_3, function);\n",
       "                  }\n",
       "                }\n",
       "                __pyx_t_1 = (__pyx_t_17) ? <span class='pyx_c_api'>__Pyx_PyObject_CallOneArg</span>(__pyx_t_3, __pyx_t_17) : <span class='pyx_c_api'>__Pyx_PyObject_CallNoArg</span>(__pyx_t_3);\n",
       "                <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_t_17); __pyx_t_17 = 0;\n",
       "                if (unlikely(!__pyx_t_1)) <span class='error_goto'>__PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='refnanny'>__Pyx_GOTREF</span>(__pyx_t_1);\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_3); __pyx_t_3 = 0;\n",
       "                if (unlikely(<span class='py_c_api'>PyDict_SetItem</span>(__pyx_v_ws_dictionary, __pyx_v_station_name, __pyx_t_1) &lt; 0)) <span class='error_goto'>__PYX_ERR(0, 33, __pyx_L9_error)</span>\n",
       "                <span class='pyx_macro_api'>__Pyx_DECREF</span>(__pyx_t_1); __pyx_t_1 = 0;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">34</span>: </pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">35</span>:                         <span class=\"k\">else</span><span class=\"p\">:</span></pre>\n",
       "<pre class=\"cython line score-0\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">36</span>:                             <span class=\"k\">pass</span></pre>\n",
       "<pre class='cython code score-0 '>              /*else*/ {\n",
       "              }\n",
       "              __pyx_L17:;\n",
       "</pre><pre class=\"cython line score-4\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">37</span>:                     <span class=\"k\">except</span> <span class=\"ne\">ValueError</span><span class=\"p\">:</span></pre>\n",
       "<pre class='cython code score-4 '>            __pyx_t_4 = <span class='pyx_c_api'>__Pyx_PyErr_ExceptionMatches</span>(__pyx_builtin_ValueError);\n",
       "            if (__pyx_t_4) {\n",
       "              <span class='pyx_c_api'>__Pyx_ErrRestore</span>(0,0,0);\n",
       "              goto __pyx_L10_exception_handled;\n",
       "            }\n",
       "            goto __pyx_L11_except_error;\n",
       "            __pyx_L11_except_error:;\n",
       "</pre><pre class=\"cython line score-0\">&#xA0;<span class=\"\">38</span>:                         <span class=\"k\">pass</span></pre>\n",
       "<pre class=\"cython line score-0\">&#xA0;<span class=\"\">39</span>: </pre>\n",
       "<pre class=\"cython line score-2\" onclick=\"(function(s){s.display=s.display==='block'?'none':'block'})(this.nextElementSibling.style)\">+<span class=\"\">40</span>:     <span class=\"k\">return</span> <span class=\"n\">ws_dictionary</span></pre>\n",
       "<pre class='cython code score-2 '>  <span class='pyx_macro_api'>__Pyx_XDECREF</span>(__pyx_r);\n",
       "  <span class='pyx_macro_api'>__Pyx_INCREF</span>(__pyx_v_ws_dictionary);\n",
       "  __pyx_r = __pyx_v_ws_dictionary;\n",
       "  goto __pyx_L0;\n",
       "</pre></div></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cython --annotate\n",
    "from datetime import datetime #Needs to be re-imported inside the cell in Jupyter notebook \n",
    "import numpy as np\n",
    "import feather\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def get_wind_speed(str input_date,str file_path): \n",
    "    '''Create a dictionary for wind speed data on the input date \n",
    "    Parameters\n",
    "        input_date (str): input date for the date of interest, in the format: YYYY-MM-DD HH:MM\n",
    "        file_path (str): path to the feather files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "    Returns \n",
    "        ws_dictionary (dict): a dictionary of wind speed values for all the active & non-null stations\n",
    "        on the input date \n",
    "    '''\n",
    "    cdef str station_name, file_name #Declare the C data types \n",
    "    cdef dict ws_dictionary = {}\n",
    "\n",
    "    search_date = datetime.strptime(input_date, '%Y-%m-%d %H:%M') # Get the datetime object for input date\n",
    "\n",
    "    for station_name in os.listdir(file_path):\n",
    "        for file_name in os.listdir(file_path+station_name+'/'):\n",
    "            if input_date[5:7] == file_name[29:31]: #This is a trick to speed up the code, only look at files which have the month/day in the name\n",
    "                if input_date[0:4]== file_name[32:36]:\n",
    "                    file = file_path+station_name+'/'+file_name\n",
    "\n",
    "                    df = feather.read_dataframe(file)\n",
    "                    try: \n",
    "                        if pd.notnull(df.loc[df['Date/Time'] == input_date, 'Wind Spd (km/h)'].item()):\n",
    "\n",
    "                            #Put the value into the dictionary. \n",
    "                            ws_dictionary[station_name] = df.loc[df['Date/Time'] == input_date, 'Wind Spd (km/h)'].item()\n",
    "\n",
    "                        else: \n",
    "                            pass\n",
    "                    except ValueError:\n",
    "                        pass \n",
    "\n",
    "    return ws_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to see an example of the output and how long the function takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from datetime import datetime #Needs to be re-imported inside the cell in Jupyter notebook \n",
    "import numpy as np\n",
    "import feather\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def get_noon_temp(str input_date,str file_path):\n",
    "    '''Create a dictionary for noon temp data on the input date \n",
    "    Parameters\n",
    "        input_date (str): input date for the date of interest, in the format: YYYY-MM-DD HH:MM\n",
    "        file_path (str): path to the feather files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "    Returns \n",
    "        temp_dictionary (dict): a dictionary of temperature values for all the active & non-null stations\n",
    "        on the input date \n",
    "    '''\n",
    "    cdef str station_name, file_name\n",
    "    cdef dict temp_dictionary = {}\n",
    "    \n",
    "    search_date = datetime.strptime(input_date, '%Y-%m-%d %H:%M')\n",
    "\n",
    "\n",
    "    for station_name in os.listdir(file_path):\n",
    "        for file_name in os.listdir(file_path+station_name+'/'):\n",
    "            if input_date[5:7] == file_name[29:31]:\n",
    "                if input_date[0:4]== file_name[32:36]:\n",
    "                    file = file_path+station_name+'/'+file_name\n",
    "\n",
    "                    df = feather.read_dataframe(file)\n",
    "\n",
    "                    try: \n",
    "                        if pd.notnull(df.loc[df['Date/Time'] == input_date, 'Temp (Â°C)'].item()):\n",
    "\n",
    "\n",
    "                            temp_dictionary[station_name] = df.loc[df['Date/Time'] == input_date, 'Temp (Â°C)'].item()\n",
    "\n",
    "                        else: \n",
    "                            pass\n",
    "                    except ValueError:\n",
    "                        pass \n",
    "\n",
    "    return temp_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from datetime import datetime #Needs to be re-imported inside the cell in Jupyter notebook \n",
    "import numpy as np\n",
    "import feather\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def get_relative_humidity(str input_date,str file_path):\n",
    "    '''Create a dictionary for rh% data on the input date \n",
    "    Parameters\n",
    "        input_date (str): input date for the date of interest, in the format: YYYY-MM-DD HH:MM\n",
    "        file_path (str): path to the feather files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "    Returns \n",
    "        temp_dictionary (dict): a dictionary of relative humidity values for all the active & \n",
    "        non-null stations on the input date \n",
    "    '''\n",
    "    cdef str station_name, file_name\n",
    "    cdef dict RH_dictionary = {}\n",
    "\n",
    "    search_date = datetime.strptime(input_date, '%Y-%m-%d %H:%M')\n",
    "\n",
    "\n",
    "    for station_name in os.listdir(file_path):\n",
    "        for file_name in os.listdir(file_path+station_name+'/'):\n",
    "            if input_date[5:7] == file_name[29:31]:\n",
    "                if input_date[0:4]== file_name[32:36]:\n",
    "                    file = file_path+station_name+'/'+file_name\n",
    "\n",
    "                    df = feather.read_dataframe(file)\n",
    "                    try: \n",
    "                        if pd.notnull(df.loc[df['Date/Time'] == input_date, 'Rel Hum (%)'].item()):\n",
    "\n",
    "\n",
    "                            RH_dictionary[station_name] = df.loc[df['Date/Time'] == input_date, 'Rel Hum (%)'].item()\n",
    "\n",
    "                        else: \n",
    "                            pass\n",
    "                    except ValueError:\n",
    "                        pass \n",
    "\n",
    "    return RH_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcp_dictionary_by_year(file_path):\n",
    "    '''Create a lookup file for the year_month that each daily station has data for faster \n",
    "    processing later --> this is an input to get_pcp \n",
    "    Parameters\n",
    "        file_path (str): file path to the daily csv files provided by Environment & Climate Change\n",
    "        Canada, including the name of the file \n",
    "    Returns \n",
    "        The function writes a .json file to the hard drive that can be read back into the code before\n",
    "        running get_pcp \n",
    "    '''\n",
    "    \n",
    "    date_dictionary = {}\n",
    "    for station_name in os.listdir(file_path):\n",
    "        \n",
    "        yearList = []\n",
    "        count = 0\n",
    "        with open(file_path+station_name, encoding='latin1') as year_information:\n",
    "            for row in year_information:\n",
    "                information = row.rstrip('\\n').split(',')\n",
    "                information_stripped = [i.replace('\"','') for i in information]\n",
    "                if count==0:\n",
    "                    \n",
    "                    header= information_stripped\n",
    "\n",
    "                    keyword = 'month' #There is also the flag which is why we include the (\n",
    "                    idx_list = [i for i, x in enumerate(header) if keyword in x.lower()]\n",
    "                    if len(idx_list) >1:\n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the month. Please check on this.') #there could be no index if the file is empty, which sometimes happens \n",
    "                        sys.exit()\n",
    "\n",
    "                    keyword2 = 'year'\n",
    "                    idx_list2 = [i for i, x in enumerate(header) if keyword2 in x.lower()]\n",
    "                    if len(idx_list2) > 1: # There should only be one field \n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the year. Please check on this.')\n",
    "                        sys.exit()\n",
    "                if count > 0:\n",
    "                    if int(information_stripped[idx_list[0]]) >= 10: \n",
    "                        year_month = str(int(information_stripped[idx_list2[0]]))+'-'+str(int(information_stripped[idx_list[0]]))\n",
    "                    else:\n",
    "                        year_month = str(int(information_stripped[idx_list2[0]]))+'-0'+str(int(information_stripped[idx_list[0]]))\n",
    "                    if year_month not in yearList: \n",
    "                        yearList.append(year_month)\n",
    "                count+=1\n",
    "        date_dictionary[station_name[:-4]] =yearList \n",
    "\n",
    "    with open('daily_lookup_file_TEMP.json', 'w') as fp:\n",
    "        json.dump(date_dictionary, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_lat_lon(file_path):\n",
    "    '''Get the latitude and longitude of the daily stations and store in a json file in the directory\n",
    "    Parameters\n",
    "        file_path (str): file path to the daily csv files provided by Environment & Climate Change\n",
    "        Canada, including the name of the file \n",
    "    Returns \n",
    "        The function writes a .json file to the hard drive that can be read back into the code before\n",
    "        running get_pcp \n",
    "    '''\n",
    "    latlon_dictionary = {} \n",
    "    #for station_name in lat_lon_list: #This is the list of available stations on that day\n",
    "    for station_name in os.listdir(file_path):\n",
    "        latlon_list = []\n",
    "        with open(file_path+station_name, encoding='latin1') as latlon_information:\n",
    "            \n",
    "            count=0\n",
    "            for row in latlon_information:\n",
    "                \n",
    "                information = row.rstrip('\\n').split(',')\n",
    "                information_stripped = [i.replace('\"','') for i in information]\n",
    "                \n",
    "                if count==0:\n",
    "                    header= information_stripped #We will look for latitude and longitude keywords in the header and find the index\n",
    "                    keyword = 'lon'\n",
    "                    idx_list = [i for i, x in enumerate(header) if keyword in x.lower()]\n",
    "                    keyword2 = 'lat'\n",
    "                    idx_list2 = [i for i, x in enumerate(header) if keyword2 in x.lower()]\n",
    "                    if len(idx_list) > 1: # There should only be one field \n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the longitude. Please check on this.')\n",
    "                        sys.exit()\n",
    "                    if len(idx_list2) > 1: # There should only be one field \n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the latitude. Please check on this.')\n",
    "                        sys.exit()\n",
    "                        \n",
    "                if count == 1:\n",
    "                    if float(information_stripped[idx_list2[0]]) != 0 or float(information_stripped[idx_list[0]]) != 0: #sometimes lat and lon is 0, need to exclude\n",
    "\n",
    "                        latlon_list.append((information_stripped[idx_list2[0]],information_stripped[idx_list[0]]))\n",
    "                        break\n",
    "                    else:\n",
    "                        pass \n",
    "                    \n",
    "                count+=1\n",
    "                \n",
    "        if len(set(latlon_list)) > 1:\n",
    "            print('For %s there is more than one location in the list! You can only have one record per station so please check the data.'%(station_name))\n",
    "        elif len(set(latlon_list)) == 0:\n",
    "            print('A valid lat lon for that station was not found in the file.') \n",
    "        else:\n",
    "            try: \n",
    "                latlon_dictionary[station_name[:-4]]=latlon_list[0]\n",
    "\n",
    "            except:\n",
    "                print('There is a problem with the files for %s and the location has not been recorded. Please check.'%(station_name))\n",
    "    with open('daily_lat_lon_TEMP.json', 'w') as fp:\n",
    "        json.dump(latlon_dictionary, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from datetime import datetime #Needs to be re-imported inside the cell in Jupyter notebook \n",
    "import numpy as np\n",
    "import feather\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "def get_pcp(str input_date,str file_path,dict date_dictionary):\n",
    "    '''Get a dictionary of the precipitation data from the feather files of the daily stations \n",
    "    Parameters\n",
    "        input_date (str): input date for the day of interest, in the format YYYY:MM:DD \n",
    "        file_path (str): file path to the daily feather files \n",
    "        date_dictionary (dict, loaded from .json): lookup file that has what day/month pairs each \n",
    "        station is active on \n",
    "    Returns \n",
    "        rain_dictionary (dict): dictionary containing rain amount for each station \n",
    "    '''\n",
    "\n",
    "    cdef str station_name, yearMonth\n",
    "    cdef dict rain_dictionary = {}\n",
    "\n",
    "    yearMonth = input_date[0:7]\n",
    "\n",
    "    for station_name in os.listdir(file_path):\n",
    "        yearsMonths = date_dictionary[station_name[:-8]] #-8 bc we are now working with feather files \n",
    "\n",
    "        if yearMonth in yearsMonths:\n",
    "\n",
    "            file = file_path+station_name\n",
    "\n",
    "            df = feather.read_dataframe(file)\n",
    "            try: \n",
    "                if pd.notnull(df.loc[df['date'] == input_date, 'total_precip'].item()):\n",
    "\n",
    "\n",
    "                    rain_dictionary[station_name[:-8]] = df.loc[df['date'] == input_date, 'total_precip'].item()\n",
    "\n",
    "                else: \n",
    "                    pass\n",
    "            except ValueError:\n",
    "                pass #is this for trace precip?\n",
    "\n",
    "    return rain_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon(file_path):\n",
    "    '''Get the latitude and longitude of the hourly stations and write to hard drive as a json file \n",
    "    Parameters\n",
    "        file_path (str): file path to the hourly csv files provided by Environment & Climate Change\n",
    "        Canada, including the name of the file \n",
    "    Returns \n",
    "        The function writes a .json file to the hard drive that can be read back into the code before\n",
    "        running get_pcp \n",
    "    '''\n",
    "    latlon_dictionary = {} \n",
    "\n",
    "    for station_name in os.listdir(file_path):\n",
    "        latlon_list = []\n",
    "        files = os.listdir(file_path+station_name+'/')[0]\n",
    "        with open(file_path+station_name+'/'+files, encoding='latin1') as latlon_information:\n",
    "            print(station_name)\n",
    "            count=0\n",
    "            for row in latlon_information:\n",
    "                \n",
    "                information = row.rstrip('\\n').split(',')\n",
    "                information_stripped = [i.replace('\"','') for i in information]\n",
    "                \n",
    "                if count==0:\n",
    "                    header= information_stripped #We will look for latitude and longitude keywords in the header and find the index\n",
    "                    keyword = 'longitude'\n",
    "                    idx_list = [i for i, x in enumerate(header) if keyword in x.lower()]\n",
    "                    keyword2 = 'latitude'\n",
    "                    idx_list2 = [i for i, x in enumerate(header) if keyword2 in x.lower()]\n",
    "                    if len(idx_list) > 1: # There should only be one field \n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the longitude. Please check on this.')\n",
    "                        sys.exit()\n",
    "                    if len(idx_list2) > 1: # There should only be one field \n",
    "                        print('The program is confused because there is more than one field name that could \\\n",
    "                        contain the latitude. Please check on this.')\n",
    "                        sys.exit()\n",
    "                        \n",
    "                if count == 1:\n",
    "                    latlon_list.append((information_stripped[idx_list2[0]],information_stripped[idx_list[0]]))\n",
    "\n",
    "                    break \n",
    "                    \n",
    "                count+=1\n",
    "                \n",
    "        if len(set(latlon_list)) > 1:\n",
    "            print('For %s there is more than one location in the list! You can only have one record per station so please check the data.'%(station_name))\n",
    "        else:\n",
    "            try: \n",
    "                latlon_dictionary[station_name]=latlon_list[0]\n",
    "            except:\n",
    "                print('There is a problem with the files for %s and the location has not been recorded. Please check.'%(station_name))\n",
    "\n",
    "    with open('hourly_lat_lon_TEMP.json', 'w') as fp:\n",
    "        json.dump(latlon_dictionary, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next group of functions (see below) are used for calculating the fire season start up and end dates for each individual station with an unbroken temperature record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions needed for the next group of functions \n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import math\n",
    "import os, sys\n",
    "import gc\n",
    "import feather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the feather fires, which are copied faster to another computer, but the code is slower than\n",
    "#if we use the csv files \n",
    "def start_date_calendar(file_path_hourly,year):\n",
    "    '''Returns a dictionary of where each station meets the start up criteria, plus a reference dictionary for the lat lon of the stations\n",
    "    Parameters\n",
    "        file_path (str): path to the feather files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "        year (str): year we want to find the fire season start up date for \n",
    "    Returns \n",
    "        date_dict (dict): dictionary containing the start up date for each station (days since Mar 1)\n",
    "        latlon_dictionary (dict): the latitude and longitude of those stations \n",
    "    '''\n",
    "\n",
    "    #Input: path to hourly data, string of the year, i.e. '1998' \n",
    "    maxTempList_dict = {} #Locations where we will store the data\n",
    "    maxTemp_dictionary = {}\n",
    "    date_dict = {}\n",
    "    latlon_dictionary = {}\n",
    "\n",
    "    for station_name in os.listdir(file_path_hourly): #The dictionary will be keyed by the hourly (temperature) station names, which means all the names must be unique\n",
    "        Temp_subdict = {} #We will need an empty dictionary to store the data due to data ordering issues \n",
    "        temp_list = [] #Initialize an empty list to temporarily store data we will later send to a permanent dictionary \n",
    "        for csv in os.listdir(file_path_hourly+station_name+'/'): #Loop through the csv in the station folder\n",
    "            if year in csv: #Only open if it is the csv for the year of interest (this is contained in the csv name)\n",
    "                file = file_path_hourly+station_name+'/'+csv  #Open the file - for CAN data we use latin 1 due to à, é etc.\n",
    "                df = feather.read_dataframe(file)\n",
    "\n",
    "                count = 0 \n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    if count == 0:\n",
    "                        \n",
    "                        latlon_dictionary[station_name] = (row['Latitude (y)'], row['ï»¿\"Longitude (x)\"']) #unicode characters at beginning, not sure why \n",
    "                    if str(row['Year']) == year:\n",
    "\n",
    "                        if str(row['Month']) == '3' or str(row['Month']) == '4' or str(row['Month']) == '5' or \\\n",
    "                           str(row['Month']) == '6' or str(row['Month']) == '7':\n",
    "                            \n",
    "                            if str(row['Time']) == '13:00':\n",
    "\n",
    "                                if pd.notnull(row['Temp (Â°C)']):\n",
    "\n",
    "                                    Temp_subdict[str(row['Date/Time'])] = float(row['Temp (Â°C)'])\n",
    "                                    temp_list.append(float(row['Temp (Â°C)'])) #Get the 13h00 temperature, send to temp list\n",
    "                                else:\n",
    "                                    Temp_subdict[str(row['Date/Time'])] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "                    count +=1 \n",
    "\n",
    "        maxTemp_dictionary[station_name] = Temp_subdict\n",
    "        maxTempList_dict[station_name] = temp_list #Store the information for each station in the permanent dictionary \n",
    "\n",
    "        vals = maxTempList_dict[station_name]\n",
    "\n",
    "        if 'NA' not in vals and len(vals) == 153: #only consider the stations with unbroken records, num_days between March-July is 153\n",
    "\n",
    "            varray = np.array(vals)\n",
    "            where_g12 = np.array(varray >= 12) #Where is the temperature >=12? \n",
    "\n",
    "\n",
    "            groups = [list(j) for i, j in groupby(where_g12)] #Put the booleans in groups, ex. [True, True], [False, False, False] \n",
    "\n",
    "            length = [x for x in groups if len(x) >= 3 and x[0] == True] #Obtain a list of where the groups are three or longer which corresponds to at least 3 days >= 12\n",
    "\n",
    "\n",
    "            index = groups.index(length[0]) #Get the index of the group\n",
    "            group_len = [len(x) for x in groups] #Get length of each group\n",
    "            length_sofar = 0 #We need to get the number of days up to where the criteria is met \n",
    "            for i in range(0,index): #loop through each group until you get to the index and add the length of that group \n",
    "                length_sofar += group_len[i]\n",
    "\n",
    "            Sdate = list(sorted(maxTemp_dictionary[station_name].keys()))[length_sofar+2] #Go two days ahead for the third day \n",
    "\n",
    "            d0 = date(int(year), 3, 1) #March 1, Year \n",
    "            d1 = date(int(Sdate[0:4]), int(Sdate[5:7]), int(Sdate[8:10])) #Convert to days since march 1 so we can interpolate\n",
    "            delta = d1 - d0\n",
    "            day = int(delta.days) #Convert to integer \n",
    "            date_dict[station_name] = day #Store the integer in the dictionary \n",
    "\n",
    "\n",
    "            #print('The start date for %s for %s is %s'%(station_name,year,Sdate))\n",
    "\n",
    "    #Return the dates for each station\n",
    "\n",
    "    return date_dict, latlon_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import os, sys\n",
    "import numpy as np \n",
    "from itertools import groupby\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "#Same function as above but will read directly from the csv files, which is faster \n",
    "def start_date_calendar_csv(file_path_hourly,year):\n",
    "    '''Returns a dictionary of where each station meets the start up criteria, plus a reference dictionary for the lat lon of the stations\n",
    "    Parameters\n",
    "        file_path (str): path to the csv files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "        year (str): year we want to find the fire season start up date for \n",
    "    Returns \n",
    "        date_dict (dict): dictionary containing the start up date for each station (days since Mar 1)\n",
    "        latlon_dictionary (dict): the latitude and longitude of those stations \n",
    "    '''\n",
    "\n",
    "    #Input: path to hourly data, string of the year, i.e. '1998' \n",
    "    maxTempList_dict = {} #Locations where we will store the data\n",
    "    maxTemp_dictionary = {}\n",
    "    date_dict = {}\n",
    "    latlon_dictionary = {}\n",
    "\n",
    "    for station_name in os.listdir(file_path_hourly): #The dictionary will be keyed by the hourly (temperature) station names, which means all the names must be unique\n",
    "        Temp_subdict = {} #We will need an empty dictionary to store the data due to data ordering issues \n",
    "        temp_list = [] #Initialize an empty list to temporarily store data we will later send to a permanent dictionary \n",
    "        count=0\n",
    "        for csv in os.listdir(file_path_hourly+station_name+'/'): #Loop through the csv in the station folder \n",
    "            if year in csv: #Only open if it is the csv for the year of interest (this is contained in the csv name)\n",
    "\n",
    "                with open(file_path_hourly+station_name+'/'+csv, encoding='latin1') as year_information: #Open the file - for CAN data we use latin 1 due to à, é etc. \n",
    "                    for row in year_information: #Look at each row \n",
    "                        information = row.rstrip('\\n').split(',') #Split each row into a list so we can loop through \n",
    "                        information_stripped = [i.replace('\"','') for i in information] #Get rid of extra quotes in the header\n",
    "                        if count==0: #This is getting the first row \n",
    "                            \n",
    "                            header= information_stripped\n",
    "\n",
    "\n",
    "                            keyword = 'temp (' #Look for this keyword in the header \n",
    "                            filter_out_keyword = 'dew' #We don't want dewpoint temperature, we want to skip over it \n",
    "                            idx_list1 = [i for i, x in enumerate(header) if keyword in x.lower() and filter_out_keyword not in x.lower()] #Get the index of the temperature column\n",
    "\n",
    "                            if len(idx_list1) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the temp data. Please check on this.')\n",
    "                                sys.exit()\n",
    "                            keyword2 = 'date/time' #Getting the index of the datetime object so we can later make sure we are using 13h00 value \n",
    "                            idx_list2 = [i for i, x in enumerate(header) if keyword2 in x.lower()]\n",
    "\n",
    "                            if len(idx_list2) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the date. Please check on this.')\n",
    "                                sys.exit()\n",
    "\n",
    "                            keyword3 = 'latitude' #Here we use the same methods to get the latitude and longitude \n",
    "                            idx_list3 = [i for i, x in enumerate(header) if keyword3 in x.lower()]\n",
    "                            if len(idx_list3) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the latitude. Please check on this.')\n",
    "                                sys.exit()\n",
    "                            keyword4 = 'longitude'\n",
    "                            idx_list4 = [i for i, x in enumerate(header) if keyword4 in x.lower()]\n",
    "                            if len(idx_list4) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the latitude. Please check on this.')\n",
    "                                sys.exit()\n",
    "                                \n",
    "                        if count > 0: #Now we are looking at the rest of the file, after the header \n",
    "\n",
    "                            if count == 1: #Lat/lon will be all the same so only record it once\n",
    "                                try: #If the file is corrupted (it usually looks like a bunch of random characters) we will get an error, so we need a try/except loop\n",
    "                                    lat =float(information_stripped[idx_list3[0]])\n",
    "                                    lon =float(information_stripped[idx_list4[0]])\n",
    "                                    latlon_dictionary[station_name] = tuple((lat,lon)) #Get the lat lon and send the tuple to the dictionary \n",
    "                                except:\n",
    "                                    print('Something is wrong with the lat/lon header names for %s!'%(station_name))\n",
    "                                    break \n",
    " \n",
    "          \n",
    "                                try:\n",
    "                                    if information_stripped[idx_list2[0]][0:4] == year: #Make sure we have the right year \n",
    "                                        if information_stripped[idx_list2[0]][5:7] == '03' or information_stripped[idx_list2[0]][5:7] == '04' or \\\n",
    "                                           information_stripped[idx_list2[0]][5:7] == '05' or information_stripped[idx_list2[0]][5:7] == '06' or \\\n",
    "                                           information_stripped[idx_list2[0]][5:7] == '07': #Make sure we are only considering months since March in case of heat wave in another month\n",
    "                                            if information_stripped[idx_list2[0]][11:13] == '13': #We are only interested in checking the 13h00 temperature\n",
    "                                                Temp_subdict[information_stripped[idx_list2[0]]] = float(information_stripped[idx_list1[0]])\n",
    "                                                temp_list.append(float(information_stripped[idx_list1[0]])) #Get the 13h00 temperature, send to temp list\n",
    "                                            \n",
    "\n",
    "\n",
    "                                except: #In the case of a nodata value\n",
    "                                    Temp_subdict[information_stripped[idx_list2[0]]] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "                                    \n",
    "\n",
    "                            else: #Proceed down the rows \n",
    "                                try:\n",
    "\n",
    "                                    if information_stripped[idx_list2[0]][0:4] == year: \n",
    "                                        if information_stripped[idx_list2[0]][5:7] == '03' or information_stripped[idx_list2[0]][5:7] == '04' or information_stripped[idx_list2[0]][5:7] == '05'\\\n",
    "                                           or information_stripped[idx_list2[0]][5:7] == '06' or information_stripped[idx_list2[0]][5:7] == '07':\n",
    "                                            if information_stripped[idx_list2[0]][11:13] == '13':\n",
    "                                                Temp_subdict[information_stripped[idx_list2[0]]] = float(information_stripped[idx_list1[0]])\n",
    "                                                temp_list.append(float(information_stripped[idx_list1[0]]))\n",
    "\n",
    "\n",
    "\n",
    "                                except:\n",
    "                                    Temp_subdict[information_stripped[idx_list2[0]]] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "\n",
    "                        count+=1   \n",
    "\n",
    "        maxTemp_dictionary[station_name] = Temp_subdict\n",
    "        maxTempList_dict[station_name] = temp_list #Store the information for each station in the permanent dictionary \n",
    "\n",
    "        vals = maxTempList_dict[station_name]\n",
    "\n",
    "        if 'NA' not in vals and len(vals) == 153: #only consider the stations with unbroken records, num_days between March-July is 153\n",
    "\n",
    "            varray = np.array(vals)\n",
    "            where_g12 = np.array(varray >= 12) #Where is the temperature >=12? \n",
    "\n",
    "\n",
    "            groups = [list(j) for i, j in groupby(where_g12)] #Put the booleans in groups, ex. [True, True], [False, False, False] \n",
    "\n",
    "            length = [x for x in groups if len(x) >= 3 and x[0] == True] #Obtain a list of where the groups are three or longer which corresponds to at least 3 days >= 12\n",
    "\n",
    "\n",
    "            index = groups.index(length[0]) #Get the index of the group\n",
    "            group_len = [len(x) for x in groups] #Get length of each group\n",
    "            length_sofar = 0 #We need to get the number of days up to where the criteria is met \n",
    "            for i in range(0,index): #loop through each group until you get to the index and add the length of that group \n",
    "                length_sofar += group_len[i]\n",
    "\n",
    "            Sdate = list(sorted(maxTemp_dictionary[station_name].keys()))[length_sofar+2] #Go two days ahead for the third day \n",
    "\n",
    "            d0 = date(int(year), 3, 1) #March 1, Year \n",
    "            d1 = date(int(Sdate[0:4]), int(Sdate[5:7]), int(Sdate[8:10])) #Convert to days since march 1 so we can interpolate\n",
    "            delta = d1 - d0\n",
    "            day = int(delta.days) #Convert to integer \n",
    "            date_dict[station_name] = day #Store the integer in the dictionary \n",
    "\n",
    "\n",
    "            #print('The start date for %s for %s is %s'%(station_name,year,Sdate))\n",
    "\n",
    "    #Return the dates for each station \n",
    "    return date_dict, latlon_dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the end date calculation, please see: \n",
    "\n",
    "Wotton, B. M., & Flannigan, M. D. (1993). Length of the fire season in a changing climate. Forestry Chronicle, 69(2), 187–192. https://doi.org/10.5558/tfc69187-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_date_calendar(file_path_hourly,year):\n",
    "    '''Returns a dictionary of where each station meets the start up criteria, \n",
    "    plus a reference dictionary for the lat lon of the stations\n",
    "    Parameters\n",
    "        file_path (str): path to the feather files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "        year (str): year we want to find the fire season end date for \n",
    "    Returns \n",
    "        date_dict (dict): dictionary containing the end date for each station (days since Oct 1)\n",
    "        latlon_dictionary (dict): the latitude and longitude of those stations \n",
    "    '''\n",
    "    #Input: path to hourly data, string of the year, i.e. '1998'\n",
    "    maxTempList_dict = {} #Locations where we will store the data\n",
    "    maxTemp_dictionary = {}\n",
    "    date_dict = {}\n",
    "    latlon_dictionary = {}\n",
    "    for station_name in os.listdir(file_path_hourly): #The dictionary will be keyed by the hourly (temperature) station names, which means all the names must be unique\n",
    "        Temp_subdict = {} #We will need an empty dictionary to store the data due to data ordering issues \n",
    "        temp_list = [] #Initialize an empty list to temporarily store data we will later send to a permanent dictionary \n",
    "        for csv in os.listdir(file_path_hourly+station_name+'/'): #Loop through the csv in the station folder\n",
    "            if year in csv: #Only open if it is the csv for the year of interest (this is contained in the csv name)\n",
    "                file = file_path_hourly+station_name+'/'+csv  #Open the file - for CAN data we use latin 1 due to à, é etc.\n",
    "                df = feather.read_dataframe(file)\n",
    "\n",
    "                count = 0 \n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    if count == 0:\n",
    "                        \n",
    "                        latlon_dictionary[station_name] = (row['Latitude (y)'], row['ï»¿\"Longitude (x)\"']) #unicode characters at beginning, not sure why \n",
    "                    if str(row['Year']) == year:\n",
    "\n",
    "                        if str(row['Month']) == '10' or str(row['Month']) == '11' or str(row['Month']) == '12':\n",
    "\n",
    "                            if str(row['Time']) == '13:00':\n",
    "\n",
    "                                if pd.notnull(row['Temp (Â°C)']):\n",
    "                                    Temp_subdict[row['Date/Time']] = float(row['Temp (Â°C)'])\n",
    "                                    temp_list.append(float(row['Temp (Â°C)'])) #Get the 13h00 temperature, send to temp list\n",
    "                                else:\n",
    "                                    Temp_subdict[row['Date/Time']] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "                    count +=1 \n",
    "\n",
    "                    \n",
    "        maxTemp_dictionary[station_name] = Temp_subdict\n",
    "        maxTempList_dict[station_name] = temp_list #Store the information for each station in the permanent dictionary\n",
    "        vals = maxTempList_dict[station_name]\n",
    "\n",
    "        \n",
    "        if 'NA' not in vals and len(vals) == 92: #only consider the stations with unbroken records, num_days between Oct1-Dec31 = 92\n",
    "\n",
    "            varray = np.array(vals)\n",
    "            where_g12 = np.array(varray < 5) #Where is the temperature < 5? \n",
    "\n",
    "\n",
    "            groups = [list(j) for i, j in groupby(where_g12)] #Put the booleans in groups, ex. [True, True], [False, False, False] \n",
    "\n",
    "            length = [x for x in groups if len(x) >= 3 and x[0] == True] #Obtain a list of where the groups are three or longer which corresponds to at least 3 days < 5\n",
    "\n",
    "            \n",
    "            index = groups.index(length[0]) #Get the index of the group\n",
    "            group_len = [len(x) for x in groups] #Get length of each group\n",
    "            length_sofar = 0 #We need to get the number of days up to where the criteria is met \n",
    "            for i in range(0,index): #loop through each group until you get to the index and add the length of that group \n",
    "                length_sofar += group_len[i]\n",
    "\n",
    "            Sdate = list(sorted(maxTemp_dictionary[station_name].keys()))[length_sofar+2] #Go two days ahead for the third day \n",
    "\n",
    "            d0 = date(int(year), 10, 1) #Oct 1, Year \n",
    "            d1 = date(int(Sdate[0:4]), int(Sdate[5:7]), int(Sdate[8:10])) #Convert to days since Oct 1 so we can interpolate\n",
    "            delta = d1 - d0\n",
    "            day = int(delta.days) #Convert to integer \n",
    "            date_dict[station_name] = day #Store the integer in the dictionary \n",
    "\n",
    "\n",
    "            #print('The end date for %s for %s is %s'%(station_name,year,Sdate))\n",
    "\n",
    "    #Return the dates for each station\n",
    "    return date_dict, latlon_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import os, sys\n",
    "import numpy as np \n",
    "from itertools import groupby\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "def end_date_calendar_csv(str file_path_hourly,str year):\n",
    "    '''Returns a dictionary of where each station meets the end criteria see \n",
    "    Wotton & Flannigan 1993, plus a reference dictionary for the lat lon of the stations\n",
    "    Parameters\n",
    "        file_path (str): path to the csv files containing the hourly data from Environment & \n",
    "        Climate Change Canada \n",
    "        year (str): year we want to find the fire season end date for \n",
    "    Returns \n",
    "        date_dict (dict): dictionary containing the end date for each station (days since Oct 1)\n",
    "        latlon_dictionary (dict): the latitude and longitude of those stations \n",
    "    '''\n",
    "    #Input: path to hourly data, string of the year, i.e. '1998' \n",
    "    maxTempList_dict = {} #Locations where we will store the data\n",
    "    maxTemp_dictionary = {}\n",
    "    date_dict = {}\n",
    "    latlon_dictionary = {}\n",
    "\n",
    "    for station_name in os.listdir(file_path_hourly): #The dictionary will be keyed by the hourly (temperature) station names, which means all the names must be unique\n",
    "        Temp_subdict = {} #We will need an empty dictionary to store the data due to data ordering issues \n",
    "        temp_list = [] #Initialize an empty list to temporarily store data we will later send to a permanent dictionary \n",
    "        count=0\n",
    "        for csv in os.listdir(file_path_hourly+station_name+'/'): #Loop through the csv in the station folder \n",
    "            if year in csv: #Only open if it is the csv for the year of interest (this is contained in the csv name)\n",
    "\n",
    "                with open(file_path_hourly+station_name+'/'+csv, encoding='latin1') as year_information: #Open the file - for CAN data we use latin 1 due to à, é etc. \n",
    "                    for row in year_information: #Look at each row \n",
    "                        information = row.rstrip('\\n').split(',') #Split each row into a list so we can loop through \n",
    "                        information_stripped = [i.replace('\"','') for i in information] #Get rid of extra quotes in the header\n",
    "                        if count==0: #This is getting the first row \n",
    "                            \n",
    "                            header= information_stripped\n",
    "\n",
    "\n",
    "                            keyword = 'temp (' #Look for this keyword in the header \n",
    "                            filter_out_keyword = 'dew' #We don't want dewpoint temperature, we want to skip over it \n",
    "                            idx_list1 = [i for i, x in enumerate(header) if keyword in x.lower() and filter_out_keyword not in x.lower()] #Get the index of the temperature column\n",
    "\n",
    "                            if len(idx_list1) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the temp data. Please check on this.')\n",
    "                                sys.exit()\n",
    "                            keyword2 = 'date/time' #Getting the index of the datetime object so we can later make sure we are using 13h00 value \n",
    "                            idx_list2 = [i for i, x in enumerate(header) if keyword2 in x.lower()]\n",
    "\n",
    "                            if len(idx_list2) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the date. Please check on this.')\n",
    "                                sys.exit()\n",
    "\n",
    "                            keyword3 = 'latitude' #Here we use the same methods to get the latitude and longitude \n",
    "                            idx_list3 = [i for i, x in enumerate(header) if keyword3 in x.lower()]\n",
    "                            if len(idx_list3) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the latitude. Please check on this.')\n",
    "                                sys.exit()\n",
    "                            keyword4 = 'longitude'\n",
    "                            idx_list4 = [i for i, x in enumerate(header) if keyword4 in x.lower()]\n",
    "                            if len(idx_list4) > 1: # There should only be one field \n",
    "                                print('The program is confused because there is more than one field name that could \\\n",
    "                                contain the latitude. Please check on this.')\n",
    "                                sys.exit()\n",
    "                                \n",
    "                        if count > 0: #Now we are looking at the rest of the file, after the header \n",
    "\n",
    "                            if count == 1: #Lat/lon will be all the same so only record it once\n",
    "                                try: #If the file is corrupted (it usually looks like a bunch of random characters) we will get an error, so we need a try/except loop\n",
    "                                    lat =float(information_stripped[idx_list3[0]])\n",
    "                                    lon =float(information_stripped[idx_list4[0]])\n",
    "                                    latlon_dictionary[station_name] = tuple((lat,lon)) #Get the lat lon and send the tuple to the dictionary \n",
    "                                except:\n",
    "                                    print('Something is wrong with the lat/lon header names for %s!'%(station_name))\n",
    "                                    break \n",
    " \n",
    "          \n",
    "                                try:\n",
    "                                    if information_stripped[idx_list2[0]][0:4] == year: #Make sure we have the right year \n",
    "                                        if information_stripped[idx_list2[0]][5:7] == '10' or information_stripped[idx_list2[0]][5:7] == '11' or information_stripped[idx_list2[0]][5:7] == '12': #Make sure we are only considering months after October \n",
    "                                            if information_stripped[idx_list2[0]][11:13] == '13': #We are only interested in checking the 13h00 temperature\n",
    "                                                Temp_subdict[information_stripped[idx_list2[0]]] = float(information_stripped[idx_list1[0]])\n",
    "                                                temp_list.append(float(information_stripped[idx_list1[0]])) #Get the 13h00 temperature, send to temp list\n",
    "                                            \n",
    "\n",
    "\n",
    "                                except: #In the case of a nodata value\n",
    "                                    Temp_subdict[information_stripped[idx_list2[0]]] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "                                    \n",
    "\n",
    "                            else: #Proceed down the rows \n",
    "                                try:\n",
    "\n",
    "                                    if information_stripped[idx_list2[0]][0:4] == year: \n",
    "                                        if information_stripped[idx_list2[0]][5:7] == '10' or information_stripped[idx_list2[0]][5:7] == '11' or information_stripped[idx_list2[0]][5:7] == '12':\n",
    "                                            if information_stripped[idx_list2[0]][11:13] == '13':\n",
    "                                                Temp_subdict[information_stripped[idx_list2[0]]] = float(information_stripped[idx_list1[0]])\n",
    "                                                temp_list.append(float(information_stripped[idx_list1[0]]))\n",
    "\n",
    "\n",
    "\n",
    "                                except:\n",
    "                                    Temp_subdict[information_stripped[idx_list2[0]]] = 'NA'\n",
    "                                    temp_list.append('NA')\n",
    "\n",
    "                        count+=1   \n",
    "\n",
    "        maxTemp_dictionary[station_name] = Temp_subdict\n",
    "        maxTempList_dict[station_name] = temp_list #Store the information for each station in the permanent dictionary \n",
    "\n",
    "        vals = maxTempList_dict[station_name]\n",
    "\n",
    "        if 'NA' not in vals and len(vals) == 92: #only consider the stations with unbroken records, num_days between Oct1-Dec31 = 92\n",
    "\n",
    "            varray = np.array(vals)\n",
    "            where_g12 = np.array(varray < 5) #Where is the temperature < 5? \n",
    "\n",
    "\n",
    "            groups = [list(j) for i, j in groupby(where_g12)] #Put the booleans in groups, ex. [True, True], [False, False, False] \n",
    "\n",
    "            length = [x for x in groups if len(x) >= 3 and x[0] == True] #Obtain a list of where the groups are three or longer which corresponds to at least 3 days < 5\n",
    "\n",
    "            \n",
    "            index = groups.index(length[0]) #Get the index of the group\n",
    "            group_len = [len(x) for x in groups] #Get length of each group\n",
    "            length_sofar = 0 #We need to get the number of days up to where the criteria is met \n",
    "            for i in range(0,index): #loop through each group until you get to the index and add the length of that group \n",
    "                length_sofar += group_len[i]\n",
    "\n",
    "            Sdate = list(sorted(maxTemp_dictionary[station_name].keys()))[length_sofar+2] #Go two days ahead for the third day \n",
    "\n",
    "            d0 = date(int(year), 10, 1) #Oct 1, Year \n",
    "            d1 = date(int(Sdate[0:4]), int(Sdate[5:7]), int(Sdate[8:10])) #Convert to days since Oct 1 so we can interpolate\n",
    "            delta = d1 - d0\n",
    "            day = int(delta.days) #Convert to integer \n",
    "            date_dict[station_name] = day #Store the integer in the dictionary \n",
    "\n",
    "\n",
    "            #print('The end date for %s for %s is %s'%(station_name,year,Sdate))\n",
    "\n",
    "    #Return the dates for each station \n",
    "    return date_dict, latlon_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for spatial interpolation and leave-one-out cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #Runtime warning suppress, this suppresses the /0 warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def IDW(latlon_dict,Cvar_dict,str input_date,str var_name,str shapefile,bint show,int d): #See IDEW for comments\n",
    "     '''Inverse distance weighting interpolation\n",
    "     Parameters\n",
    "         latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "         .json file\n",
    "         Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "         input_date (str): the date you want to interpolate for \n",
    "         shapefile (str): path to the study area shapefile \n",
    "         show (bool): whether you want to plot a map \n",
    "         d (int): the weighting function for IDW interpolation \n",
    "     Returns\n",
    "         idw_grid (np_array): the array of values for the interpolated surface\n",
    "         maxmin: the bounds of the array surface, for use in other functions \n",
    "     '''\n",
    "     lat = []\n",
    "     lon = []\n",
    "     Cvar = []\n",
    "     for station_name in Cvar_dict.keys():\n",
    "\n",
    "        if station_name in latlon_dict.keys():\n",
    "\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            cvar_val = Cvar_dict[station_name]\n",
    "            lat.append(float(latitude))\n",
    "            lon.append(float(longitude))\n",
    "            Cvar.append(cvar_val)\n",
    "     y = np.array(lat)\n",
    "     x = np.array(lon)\n",
    "     z = np.array(Cvar) \n",
    "\n",
    "     na_map = gpd.read_file(shapefile)\n",
    "     bounds = na_map.bounds\n",
    "     xmax = bounds['maxx']\n",
    "     xmin= bounds['minx']\n",
    "     ymax = bounds['maxy']\n",
    "     ymin = bounds['miny']\n",
    "     cdef int pixelHeight = 10000 \n",
    "     cdef int pixelWidth = 10000\n",
    "            \n",
    "     cdef int num_col = int((xmax - xmin) / pixelHeight)\n",
    "     cdef int num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "     #We need to project to a projected system before making distance matrix\n",
    "     source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') \n",
    "     xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "     yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "     xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "     Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "     Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "     Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "     Xi,Yi = Xi.flatten(), Yi.flatten()\n",
    "     maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)]\n",
    "\n",
    "     vals = np.vstack((xProj,yProj)).T\n",
    "\n",
    "     interpol = np.vstack((Xi,Yi)).T\n",
    "     dist_not = np.subtract.outer(vals[:,0], interpol[:,0]) \n",
    "     dist_one = np.subtract.outer(vals[:,1], interpol[:,1]) \n",
    "     distance_matrix = np.hypot(dist_not,dist_one) \n",
    "\n",
    "     weights = 1/(distance_matrix **d)\n",
    "     weights[np.where(np.isinf(weights))] = 1/(1.0E-50) \n",
    "     weights /= weights.sum(axis = 0) \n",
    "\n",
    "     Zi = np.dot(weights.T, z)\n",
    "     idw_grid = Zi.reshape(num_row,num_col)\n",
    "\n",
    "     if show:\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(idw_grid,extent=(xProj_extent.min()-1,xProj_extent.max()+1,yProj_extent.max()-1,yProj_extent.min()+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.scatter(xProj,yProj,c=z,edgecolors='k')\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label(var_name) \n",
    "        \n",
    "        title = 'IDW Interpolation for %s on %s'%(var_name,input_date)\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude') \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "     return idw_grid, maxmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_IDW(latlon_dict,Cvar_dict,shapefile,d):\n",
    "     '''Leave-one-out cross-validation procedure for IDW \n",
    "     Parameters\n",
    "         latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "         .json file\n",
    "         Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "         shapefile (str): path to the study area shapefile \n",
    "         d (int): the weighting function for IDW interpolation \n",
    "     Returns \n",
    "         absolute_error_dictionary (dict): a dictionary of the absolute error at each station when it\n",
    "         was left out \n",
    "     '''\n",
    "     x_origin_list = []\n",
    "     y_origin_list = [] \n",
    "\n",
    "     absolute_error_dictionary = {} #for plotting\n",
    "     station_name_list = []\n",
    "     projected_lat_lon = {}\n",
    "\n",
    "     for station_name in Cvar_dict.keys():\n",
    "          \n",
    "          if station_name in latlon_dict.keys():\n",
    "               \n",
    "             station_name_list.append(station_name)\n",
    "\n",
    "             loc = latlon_dict[station_name]\n",
    "             latitude = loc[0]\n",
    "             longitude = loc[1]\n",
    "             Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "             Plat = float(Plat)\n",
    "             Plon = float(Plon)\n",
    "             projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "\n",
    "\n",
    "     for station_name_hold_back in station_name_list:\n",
    "\n",
    "        lat = []\n",
    "        lon = []\n",
    "        Cvar = []\n",
    "        for station_name in sorted(Cvar_dict.keys()):\n",
    "             if station_name in latlon_dict.keys():\n",
    "                 if station_name != station_name_hold_back:\n",
    "                     loc = latlon_dict[station_name]\n",
    "                     latitude = loc[0]\n",
    "                     longitude = loc[1]\n",
    "                     cvar_val = Cvar_dict[station_name]\n",
    "                     lat.append(float(latitude))\n",
    "                     lon.append(float(longitude))\n",
    "                     Cvar.append(cvar_val)\n",
    "                 else:\n",
    "\n",
    "                     pass\n",
    "                \n",
    "        y = np.array(lat)\n",
    "        x = np.array(lon)\n",
    "        z = np.array(Cvar) \n",
    "        \n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "        xmax = bounds['maxx']\n",
    "        xmin= bounds['minx']\n",
    "        ymax = bounds['maxy']\n",
    "        ymin = bounds['miny']\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "                \n",
    "        num_col = int((xmax - xmin) / pixelHeight)\n",
    "        num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "        #We need to project to a projected system before making distance matrix\n",
    "        source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "        xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "        yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "        xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "        Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "        Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "        Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "        Xi,Yi = Xi.flatten(), Yi.flatten()\n",
    "        maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)]\n",
    "\n",
    "        vals = np.vstack((xProj,yProj)).T\n",
    "        \n",
    "        interpol = np.vstack((Xi,Yi)).T\n",
    "        dist_not = np.subtract.outer(vals[:,0], interpol[:,0]) #Length of the triangle side from the cell to the point with data \n",
    "        dist_one = np.subtract.outer(vals[:,1], interpol[:,1]) #Length of the triangle side from the cell to the point with data \n",
    "        distance_matrix = np.hypot(dist_not,dist_one) #euclidean distance, getting the hypotenuse\n",
    "        \n",
    "        weights = 1/(distance_matrix**d) #what if distance is 0 --> np.inf? have to account for the pixel underneath\n",
    "        weights[np.where(np.isinf(weights))] = 1/(1.0E-50) #Making sure to assign the value of the weather station above the pixel directly to the pixel underneath\n",
    "        weights /= weights.sum(axis = 0) \n",
    "\n",
    "        Zi = np.dot(weights.T, z)\n",
    "        idw_grid = Zi.reshape(num_row,num_col)\n",
    "\n",
    "        #Delete at a certain point\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "\n",
    "        interpolated_val = idw_grid[y_orig][x_orig] \n",
    "\n",
    "        original_val = Cvar_dict[station_name]\n",
    "        absolute_error = abs(interpolated_val-original_val)\n",
    "        absolute_error_dictionary[station_name_hold_back] = absolute_error\n",
    "\n",
    "\n",
    "     return absolute_error_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDEW(latlon_dict,Cvar_dict,input_date,var_name,shapefile,show,file_path_elev,idx_list,d):\n",
    "    '''Inverse distance elevation weighting\n",
    "    Parameters\n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        input_date (str): the date you want to interpolate for \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        show (bool): whether you want to plot a map \n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        d (int): the weighting function for IDW interpolation \n",
    "    Returns \n",
    "        idew_grid (np_array): the array of values for the interpolated surface\n",
    "        maxmin: the bounds of the array surface, for use in other functions \n",
    "        elev_array (np_array): the array of elevation values for the study area, so we can return it\n",
    "        to the cross-validation function for faster processing \n",
    "        \n",
    "    '''\n",
    "    #Input: lat lon of station, variable (start day, rainfall, etc), date of interest,variable name (for plotting), show (bool true/false), file path to elevation lookup file\n",
    "    #idx_list (for the column containing the elevation data), d is the power applied to get the weight \n",
    "    lat = [] #Initialize empty lists to store data \n",
    "    lon = []\n",
    "    Cvar = []\n",
    "    for station_name in Cvar_dict.keys(): #Loop through the list of stations \n",
    "        if station_name in latlon_dict.keys(): #Make sure the station is present in the latlon dict \n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            cvar_val = Cvar_dict[station_name]\n",
    "            lat.append(float(latitude))\n",
    "            lon.append(float(longitude))\n",
    "            Cvar.append(cvar_val)\n",
    "    y = np.array(lat) #Convert to a numpy array for faster processing speed \n",
    "    x = np.array(lon)\n",
    "    z = np.array(Cvar) \n",
    "\n",
    "    na_map = gpd.read_file(shapefile)\n",
    "    bounds = na_map.bounds #Get the bounding box of the shapefile \n",
    "    xmax = bounds['maxx']\n",
    "    xmin= bounds['minx']\n",
    "    ymax = bounds['maxy']\n",
    "    ymin = bounds['miny']\n",
    "    pixelHeight = 10000 #We want a 10 by 10 pixel, or as close as we can get \n",
    "    pixelWidth = 10000\n",
    "            \n",
    "    num_col = int((xmax - xmin) / pixelHeight) #Calculate the number of rows cols to fill the bounding box at that resolution \n",
    "    num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "    #We need to project to a projected system before making distance matrix\n",
    "    source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume NAD83\n",
    "    xProj, yProj = pyproj.Proj('esri:102001')(x,y) #Convert to Canada Albers Equal Area \n",
    "\n",
    "    yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']]) #Add the bounding box coords to the dataset so we can extrapolate the interpolation to cover whole area\n",
    "    xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "    Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row) #Get the value for lat lon in each cell we just made \n",
    "    Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "    Xi,Yi = np.meshgrid(Xi,Yi) #Make a rectangular grid (because eventually we will map the values)\n",
    "    Xi,Yi = Xi.flatten(), Yi.flatten() #Then we flatten the arrays for easier processing \n",
    "    maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)] #We will later return this for use in other functions \n",
    "\n",
    "    vals = np.vstack((xProj,yProj)).T #vertically stack station x and y vals and then transpose them so they are in pairs \n",
    "\n",
    "    interpol = np.vstack((Xi,Yi)).T #Do the same thing for the grid x and y vals \n",
    "    dist_not = np.subtract.outer(vals[:,0], interpol[:,0]) #Length of the triangle side from the cell to the point with data \n",
    "    dist_one = np.subtract.outer(vals[:,1], interpol[:,1]) #Length of the triangle side from the cell to the point with data \n",
    "    distance_matrix = np.hypot(dist_not,dist_one) #Euclidean distance, getting the hypotenuse\n",
    "\n",
    "\n",
    "    weights = 1/(distance_matrix**d) #what if distance is 0 --> np.inf? have to account for the pixel underneath\n",
    "    weights[np.where(np.isinf(weights))] = 1/(1.0E-50) #Making sure to assign the value of the weather station above the pixel directly to the pixel underneath\n",
    "    weights /= weights.sum(axis = 0) #The weights must add up to 0 \n",
    "\n",
    "\n",
    "    Zi = np.dot(weights.T, z) #Take the dot product of the weights and the values, in this case the dot product is the sum product over the last axis of Weights.T and z\n",
    "\n",
    "    idw_grid = Zi.reshape(num_row,num_col) #reshape the array into the proper format for the map \n",
    "\n",
    "    #Elevation weights\n",
    "    #Lon (X) goes in first for a REASON. It has to do with order in the lookup file. \n",
    "    concat = np.array((Xi.flatten(), Yi.flatten())).T #Preparing the coordinates to send to the function that will get the elevation grid \n",
    "    send_to_list = concat.tolist()\n",
    "    send_to_tuple = [tuple(x) for x in send_to_list] #The elevation function takes a tuple \n",
    "\n",
    "\n",
    "    Xi1_grd=[]\n",
    "    Yi1_grd=[]\n",
    "    elev_grd = []\n",
    "    elev_grd_dict = finding_data_frm_lookup(send_to_tuple,file_path_elev,idx_list) #Get the elevations from the lookup file \n",
    "\n",
    "    for keys in elev_grd_dict.keys(): #The keys are each lat lon pair \n",
    "        x= keys[0]\n",
    "        y = keys[1]\n",
    "        Xi1_grd.append(x)\n",
    "        Yi1_grd.append(y)\n",
    "        elev_grd.append(elev_grd_dict[keys]) #Append the elevation data to the empty list \n",
    "\n",
    "    elev_array = np.array(elev_grd) #make an elevation array\n",
    "\n",
    "    \n",
    "\n",
    "    elev_dict= finding_data_frm_lookup(zip(xProj, yProj),file_path_elev,idx_list) #Get the elevations for the stations \n",
    "\n",
    "    xProj_input=[]\n",
    "    yProj_input=[]\n",
    "    e_input = []\n",
    "\n",
    "\n",
    "    for keys in zip(xProj,yProj): #Repeat process for just the stations not the whole grid \n",
    "        x= keys[0]\n",
    "        y = keys[1]\n",
    "        xProj_input.append(x)\n",
    "        yProj_input.append(y)\n",
    "        e_input.append(elev_dict[keys])\n",
    "\n",
    "    source_elev = np.array(e_input)\n",
    "\n",
    "\n",
    "    vals2 = np.vstack(source_elev).T\n",
    "\n",
    "    interpol2 = np.vstack(elev_array).T\n",
    "\n",
    "    dist_not2 = np.subtract.outer(vals2[0], interpol2[0]) #Get distance in terms of the elevation (vertical distance) from the station to the point to be interpolated \n",
    "    dist_not2 = np.absolute(dist_not2) #Take the absolute value, we just care about what is the difference \n",
    "    weights2 = 1/(dist_not2**d) #Get the inverse distance weight \n",
    "    weights2[np.where(np.isinf(weights2))] = 1 #In the case of no elevation change \n",
    "    weights2 /= weights2.sum(axis = 0) #Make weights add up to 1 \n",
    "\n",
    "\n",
    "    fin = 0.8*np.dot(weights.T,z) + 0.2*np.dot(weights2.T,z) #Weight distance as 0.8 and elevation as 0.2 \n",
    "\n",
    "    idew_grid = fin.reshape(num_row,num_col) #Reshape the final array \n",
    "\n",
    "\n",
    "    if show: #Plot if show == True \n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(idew_grid,extent=(xProj_extent.min()-1,xProj_extent.max()+1,yProj_extent.max()-1,yProj_extent.min()+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.scatter(xProj,yProj,c=z,edgecolors='k')\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label(var_name) \n",
    "        \n",
    "        title = 'IDEW Interpolation for %s on %s'%(var_name,input_date)\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude') \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return idew_grid, maxmin, elev_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_IDEW(latlon_dict,Cvar_dict,shapefile,file_path_elev,elev_array,idx_list,d):\n",
    "    '''Leave-one-out cross-validation procedure for IDEW\n",
    "    Parameters\n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        elev_array (np_array): the elevation array for the study area \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        d (int): the weighting function for IDW interpolation \n",
    "    Returns \n",
    "        absolute_error_dictionary (dict): a dictionary of the absolute error at each station when it\n",
    "        was left out \n",
    "    '''\n",
    "    x_origin_list = []\n",
    "    y_origin_list = [] \n",
    "\n",
    "    absolute_error_dictionary = {} #for plotting\n",
    "    station_name_list = []\n",
    "    projected_lat_lon = {}\n",
    "\n",
    "    for station_name in Cvar_dict.keys():\n",
    "        if station_name in latlon_dict.keys():\n",
    "            station_name_list.append(station_name)\n",
    "\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "            Plat = float(Plat)\n",
    "            Plon = float(Plon)\n",
    "            projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "\n",
    "\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        lat = []\n",
    "        lon = []\n",
    "        Cvar = []\n",
    "        for station_name in sorted(Cvar_dict.keys()):\n",
    "            if station_name in latlon_dict.keys():\n",
    "                if station_name != station_name_hold_back:\n",
    "                    loc = latlon_dict[station_name]\n",
    "                    latitude = loc[0]\n",
    "                    longitude = loc[1]\n",
    "                    cvar_val = Cvar_dict[station_name]\n",
    "                    lat.append(float(latitude))\n",
    "                    lon.append(float(longitude))\n",
    "                    Cvar.append(cvar_val)\n",
    "                else:\n",
    "\n",
    "                    pass\n",
    "                \n",
    "        y = np.array(lat)\n",
    "        x = np.array(lon)\n",
    "        z = np.array(Cvar) #what if we add the bounding locations to the array??? ==> that would be extrapolation not interpolation? \n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "        xmax = bounds['maxx']\n",
    "        xmin= bounds['minx']\n",
    "        ymax = bounds['maxy']\n",
    "        ymin = bounds['miny']\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "                \n",
    "        num_col = int((xmax - xmin) / pixelHeight)\n",
    "        num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "        #We need to project to a projected system before making distance matrix\n",
    "        source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "        xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "        yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "        xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "        Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "        Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "        Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "        Xi,Yi = Xi.flatten(), Yi.flatten()\n",
    "        maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)]\n",
    "\n",
    "        vals = np.vstack((xProj,yProj)).T\n",
    "        \n",
    "        interpol = np.vstack((Xi,Yi)).T\n",
    "        dist_not = np.subtract.outer(vals[:,0], interpol[:,0]) #Length of the triangle side from the cell to the point with data \n",
    "        dist_one = np.subtract.outer(vals[:,1], interpol[:,1]) #Length of the triangle side from the cell to the point with data \n",
    "        distance_matrix = np.hypot(dist_not,dist_one) #euclidean distance, getting the hypotenuse\n",
    "        \n",
    "        weights = 1/(distance_matrix**d) #what if distance is 0 --> np.inf? have to account for the pixel underneath\n",
    "        weights[np.where(np.isinf(weights))] = 1/(1.0E-50) #Making sure to assign the value of the weather station above the pixel directly to the pixel underneath\n",
    "        weights /= weights.sum(axis = 0) \n",
    "\n",
    "        Zi = np.dot(weights.T, z)\n",
    "        idw_grid = Zi.reshape(num_row,num_col)\n",
    "\n",
    "\n",
    "        \n",
    "        elev_dict= GD.finding_data_frm_lookup(zip(yProj, xProj),file_path_elev,idx_list)\n",
    "\n",
    "        xProj_input=[]\n",
    "        yProj_input=[]\n",
    "        e_input = []\n",
    "        \n",
    "\n",
    "        for keys in zip(yProj,xProj): # in case there are two stations at the same lat\\lon \n",
    "            x= keys[0]\n",
    "            y = keys[1]\n",
    "            xProj_input.append(x)\n",
    "            yProj_input.append(y)\n",
    "            e_input.append(elev_dict[keys])\n",
    "\n",
    "        source_elev = np.array(e_input)\n",
    "\n",
    "\n",
    "        vals2 = np.vstack(source_elev).T\n",
    "\n",
    "        interpol2 = np.vstack(elev_array).T\n",
    "\n",
    "        dist_not2 = np.subtract.outer(vals2[0], interpol2[0])\n",
    "        dist_not2 = np.absolute(dist_not2)\n",
    "        weights2 = 1/(dist_not2**d)\n",
    "\n",
    "        weights2[np.where(np.isinf(weights2))] = 1 \n",
    "        weights2 /= weights2.sum(axis = 0)\n",
    "\n",
    "        fin = 0.8*np.dot(weights.T,z) + 0.2*np.dot(weights2.T,z)\n",
    "        \n",
    "        fin = fin.reshape(num_row,num_col)\n",
    "\n",
    "        #Calc the RMSE, MAE at the pixel loc\n",
    "        #Delete at a certain point\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "\n",
    "        interpolated_val = fin[y_orig][x_orig] \n",
    "\n",
    "        original_val = Cvar_dict[station_name]\n",
    "        absolute_error = abs(interpolated_val-original_val)\n",
    "        absolute_error_dictionary[station_name_hold_back] = absolute_error\n",
    "\n",
    "\n",
    "    return absolute_error_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as interpolate\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPS(latlon_dict,Cvar_dict,input_date,var_name,shapefile,show,phi):\n",
    "    '''Thin plate splines interpolation implemented using the interpolate radial basis function from \n",
    "    SciPy \n",
    "    Parameters\n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        input_date (str): the date you want to interpolate for \n",
    "        var_name (str): name of the variable you are interpolating\n",
    "        shapefile (str): path to the study area shapefile \n",
    "        show (bool): whether you want to plot a map \n",
    "        phi (float): smoothing parameter for the thin plate spline, if 0 no smoothing \n",
    "    Returns \n",
    "        spline (np_array): the array of values for the interpolated surface\n",
    "        maxmin: the bounds of the array surface, for use in other functions \n",
    "    \n",
    "    '''\n",
    "    x_origin_list = []\n",
    "    y_origin_list = []\n",
    "    z_origin_list = [] \n",
    "\n",
    "    absolute_error_dictionary = {} #for plotting\n",
    "    station_name_list = []\n",
    "    projected_lat_lon = {}\n",
    "\n",
    "    for station_name in Cvar_dict.keys():\n",
    "        if station_name in latlon_dict.keys():\n",
    "            station_name_list.append(station_name)\n",
    "\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "            Plat = float(Plat)\n",
    "            Plon = float(Plon)\n",
    "            projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    Cvar = []\n",
    "    for station_name in Cvar_dict.keys(): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step\n",
    "        if station_name in latlon_dict.keys():\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            cvar_val = Cvar_dict[station_name]\n",
    "            lat.append(float(latitude))\n",
    "            lon.append(float(longitude))\n",
    "            Cvar.append(cvar_val)\n",
    "    y = np.array(lat)\n",
    "    x = np.array(lon)\n",
    "    z = np.array(Cvar)\n",
    "\n",
    "    Cvar = []\n",
    "    for station_name in Cvar_dict.keys(): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step \n",
    "\n",
    "        cvar_val = Cvar_dict[station_name]\n",
    "\n",
    "        Cvar.append(cvar_val)\n",
    "\n",
    "    z2 = np.array(Cvar)\n",
    "\n",
    "\n",
    "    #for station_name in sorted(Cvar_dict.keys()): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "\n",
    "\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "        z_origin_list.append(Cvar_dict[station_name_hold_back])\n",
    "\n",
    "\n",
    "    na_map = gpd.read_file(shapefile)\n",
    "    bounds = na_map.bounds\n",
    "    xmax = bounds['maxx']\n",
    "    xmin= bounds['minx']\n",
    "    ymax = bounds['maxy']\n",
    "    ymin = bounds['miny']\n",
    "    pixelHeight = 10000 \n",
    "    pixelWidth = 10000\n",
    "            \n",
    "    num_col = int((xmax - xmin) / pixelHeight)\n",
    "    num_row = int((ymax - ymin) / pixelWidth)\n",
    "    \n",
    "\n",
    "    source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "    xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "    yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "    xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "    maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)]\n",
    "\n",
    "    Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "    Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "    Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "\n",
    "    empty_grid = np.empty((num_row,num_col,))*np.nan\n",
    "\n",
    "    for x,y,z in zip(x_origin_list,y_origin_list,z_origin_list):\n",
    "        empty_grid[y][x] = z\n",
    "\n",
    "\n",
    "\n",
    "    vals = ~np.isnan(empty_grid)\n",
    "\n",
    "    func = interpolate.Rbf(Xi[vals],Yi[vals],empty_grid[vals], function='thin_plate',smooth=phi)\n",
    "    thin_plate = func(Xi,Yi)\n",
    "    spline = thin_plate.reshape(num_row,num_col)\n",
    "\n",
    "    if show: \n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(spline,extent=(xProj_extent.min()-1,xProj_extent.max()+1,yProj_extent.max()-1,yProj_extent.min()+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.scatter(xProj,yProj,c=z_origin_list,edgecolors='k',linewidth=1)\n",
    "\n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label(var_name) \n",
    "        \n",
    "        title = 'Thin Plate Spline Interpolation for %s on %s'%(var_name,input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude') \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return spline, maxmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tps(latlon_dict,Cvar_dict,shapefile,phi):\n",
    "    '''Leave-one-out cross-validation for thin plate splines \n",
    "    Parameters\n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        phi (float): smoothing parameter for the thin plate spline, if 0 no smoothing \n",
    "    Returns \n",
    "        absolute_error_dictionary (dict): a dictionary of the absolute error at each station when it\n",
    "        was left out \n",
    "     '''\n",
    "    x_origin_list = []\n",
    "    y_origin_list = [] \n",
    "    z_origin_list = []\n",
    "    absolute_error_dictionary = {} #for plotting\n",
    "    station_name_list = []\n",
    "    projected_lat_lon = {}\n",
    "\n",
    "    for station_name in Cvar_dict.keys():\n",
    "        if station_name in latlon_dict.keys():\n",
    "            station_name_list.append(station_name)\n",
    "\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "            Plat = float(Plat)\n",
    "            Plon = float(Plon)\n",
    "            projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "\n",
    "\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "        z_origin_list.append(Cvar_dict[station_name_hold_back])\n",
    "\n",
    "\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        lat = []\n",
    "        lon = []\n",
    "        Cvar = []\n",
    "        for station_name in sorted(Cvar_dict.keys()):\n",
    "            if station_name in latlon_dict.keys():\n",
    "                if station_name != station_name_hold_back:\n",
    "                    loc = latlon_dict[station_name]\n",
    "                    latitude = loc[0]\n",
    "                    longitude = loc[1]\n",
    "                    cvar_val = Cvar_dict[station_name]\n",
    "                    lat.append(float(latitude))\n",
    "                    lon.append(float(longitude))\n",
    "                    Cvar.append(cvar_val)\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "        y = np.array(lat)\n",
    "        x = np.array(lon)\n",
    "        z = np.array(Cvar) #what if we add the bounding locations to the array??? ==> that would be extrapolation not interpolation? \n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "        xmax = bounds['maxx']\n",
    "        xmin= bounds['minx']\n",
    "        ymax = bounds['maxy']\n",
    "        ymin = bounds['miny']\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "                \n",
    "        num_col = int((xmax - xmin) / pixelHeight)\n",
    "        num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "        #We need to project to a projected system before making distance matrix\n",
    "        source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "        xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "        yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "        xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "        Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "        Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "        Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "\n",
    "        empty_grid = np.empty((num_row,num_col,))*np.nan\n",
    "\n",
    "        for x,y,z in zip(x_origin_list,y_origin_list,z_origin_list):\n",
    "            empty_grid[y][x] = z\n",
    "\n",
    "\n",
    "\n",
    "        vals = ~np.isnan(empty_grid)\n",
    "\n",
    "        func = interpolate.Rbf(Xi[vals],Yi[vals],empty_grid[vals], function='thin_plate',smooth=phi)\n",
    "        thin_plate = func(Xi,Yi)\n",
    "        spline = thin_plate.reshape(num_row,num_col)\n",
    "\n",
    "        #Calc the RMSE, MAE, NSE, and MRAE at the pixel loc\n",
    "        #Delete at a certain point\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "\n",
    "        interpolated_val = spline[y_orig][x_orig] \n",
    "\n",
    "        original_val = Cvar_dict[station_name]\n",
    "        absolute_error = abs(interpolated_val-original_val)\n",
    "        absolute_error_dictionary[station_name_hold_back] = absolute_error\n",
    "\n",
    "    return absolute_error_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykrige.ok import OrdinaryKriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OKriging(latlon_dict,Cvar_dict,input_date,var_name,shapefile,model,show):\n",
    "    '''Implement ordinary kriging \n",
    "    Parameters\n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        input_date (str): the date you want to interpolate for \n",
    "        var_name (str): name of the variable you are interpolating\n",
    "        shapefile (str): path to the study area shapefile \n",
    "        model (str): semivariogram model name, ex. 'gaussian'\n",
    "        show (bool): whether you want to plot a map \n",
    "    Returns \n",
    "        kriging_surface (np_array): the array of values for the interpolated surface\n",
    "        maxmin: the bounds of the array surface, for use in other functions \n",
    "        \n",
    "        if there is an error when interpolating the surface, it does not return anything \n",
    "        \n",
    "    '''\n",
    "    x_origin_list = []\n",
    "    y_origin_list = []\n",
    "    z_origin_list = [] \n",
    "\n",
    "    absolute_error_dictionary = {} #for plotting\n",
    "    station_name_list = []\n",
    "    projected_lat_lon = {}\n",
    "\n",
    "    for station_name in Cvar_dict.keys():\n",
    "        station_name_list.append(station_name)\n",
    "\n",
    "        loc = latlon_dict[station_name]\n",
    "        latitude = loc[0]\n",
    "        longitude = loc[1]\n",
    "        Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "        Plat = float(Plat)\n",
    "        Plon = float(Plon)\n",
    "        projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    Cvar = []\n",
    "    for station_name in Cvar_dict.keys(): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step \n",
    "        loc = latlon_dict[station_name]\n",
    "        latitude = loc[0]\n",
    "        longitude = loc[1]\n",
    "        cvar_val = Cvar_dict[station_name]\n",
    "        lat.append(float(latitude))\n",
    "        lon.append(float(longitude))\n",
    "        Cvar.append(cvar_val)\n",
    "    y = np.array(lat)\n",
    "    x = np.array(lon)\n",
    "    z = np.array(Cvar)\n",
    "\n",
    "    Cvar = []\n",
    "    for station_name in Cvar_dict.keys(): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step \n",
    "\n",
    "        cvar_val = Cvar_dict[station_name]\n",
    "\n",
    "        Cvar.append(cvar_val)\n",
    "\n",
    "    z2 = np.array(Cvar)\n",
    "\n",
    "\n",
    "    #for station_name in sorted(Cvar_dict.keys()): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "\n",
    "\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "        z_origin_list.append(Cvar_dict[station_name_hold_back])\n",
    "\n",
    "\n",
    "    #Uncomment to check if the correct stations are being accessed\n",
    "    na_map = gpd.read_file(shapefile)\n",
    "    bounds = na_map.bounds\n",
    "    xmax = bounds['maxx']\n",
    "    xmin= bounds['minx']\n",
    "    ymax = bounds['maxy']\n",
    "    ymin = bounds['miny']\n",
    "    \n",
    "    pixelHeight = 10000 \n",
    "    pixelWidth = 10000\n",
    "                \n",
    "    num_col = int((xmax - xmin) / pixelHeight)\n",
    "    num_row = int((ymax - ymin) / pixelWidth)\n",
    "    \n",
    "\n",
    "    source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "    xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "    yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "    xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "    maxmin = [np.min(yProj_extent),np.max(yProj_extent),np.max(xProj_extent),np.min(xProj_extent)]\n",
    "\n",
    "    Yi1 = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "    Xi1 = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "    Xi,Yi = np.meshgrid(Xi1,Yi1)\n",
    "\n",
    "    empty_grid = np.empty((num_row,num_col,))*np.nan\n",
    "\n",
    "    for x3,y3,z3 in zip(x_origin_list,y_origin_list,z_origin_list):\n",
    "        empty_grid[y3][x3] = z3\n",
    "\n",
    "\n",
    "\n",
    "    vals = ~np.isnan(empty_grid)\n",
    "\n",
    "\n",
    "\n",
    "    OK = OrdinaryKriging(xProj,yProj,z,variogram_model=model,verbose=False,enable_plotting=False)\n",
    "    try: \n",
    "        z1,ss1 = OK.execute('grid',Xi1,Yi1,n_closest_points=10,backend='C') #n_closest_points=10\n",
    "        \n",
    "        kriging_surface = z1.reshape(num_row,num_col)\n",
    "        if show:\n",
    "            fig, ax = plt.subplots(figsize= (15,15))\n",
    "            crs = {'init': 'esri:102001'}\n",
    "\n",
    "            na_map = gpd.read_file(shapefile)\n",
    "            \n",
    "          \n",
    "            plt.imshow(kriging_surface,extent=(xProj_extent.min()-1,xProj_extent.max()+1,yProj_extent.max()-1,yProj_extent.min()+1)) \n",
    "            na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "                \n",
    "            plt.scatter(xProj,yProj,c=z_origin_list,edgecolors='k',linewidth=1)\n",
    "\n",
    "            plt.gca().invert_yaxis()\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.set_label(var_name) \n",
    "            \n",
    "            title = 'Ordinary Kriging Interpolation for %s on %s'%(var_name,input_date) \n",
    "            fig.suptitle(title, fontsize=14)\n",
    "            plt.xlabel('Longitude')\n",
    "            plt.ylabel('Latitude') \n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        return kriging_surface, maxmin\n",
    "\n",
    "    except:\n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_OK(latlon_dict,Cvar_dict,shapefile,model):\n",
    "    '''Cross_validate the ordinary kriging \n",
    "    Parameters \n",
    "        latlon_dict (dict): the latitude and longitudes of the hourly stations, loaded from the \n",
    "        .json file\n",
    "        Cvar_dict (dict): dictionary of weather variable values for each station \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        model (str): semivariogram model name, ex. 'gaussian'\n",
    "    Returns \n",
    "        absolute_error_dictionary (dict): a dictionary of the absolute error at each station when it\n",
    "        was left out \n",
    "    '''\n",
    "    x_origin_list = []\n",
    "    y_origin_list = [] \n",
    "    z_origin_list = []\n",
    "    absolute_error_dictionary = {} #for plotting\n",
    "    station_name_list = []\n",
    "    projected_lat_lon = {}\n",
    "\n",
    "    for station_name in Cvar_dict.keys():\n",
    "        if station_name in latlon_dict.keys():\n",
    "            station_name_list.append(station_name)\n",
    "\n",
    "            loc = latlon_dict[station_name]\n",
    "            latitude = loc[0]\n",
    "            longitude = loc[1]\n",
    "            Plat, Plon = pyproj.Proj('esri:102001')(longitude,latitude)\n",
    "            Plat = float(Plat)\n",
    "            Plon = float(Plon)\n",
    "            projected_lat_lon[station_name] = [Plat,Plon]\n",
    "\n",
    "\n",
    "\n",
    "    for station_name_hold_back in station_name_list:\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "\n",
    "\n",
    "        coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "        x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "        y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "        x_origin_list.append(x_orig)\n",
    "        y_origin_list.append(y_orig)\n",
    "        z_origin_list.append(Cvar_dict[station_name_hold_back])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #for station_name in sorted(Cvar_dict.keys()): #DONT use list of stations, because if there's a no data we delete that in the climate dictionary step\n",
    "    for station_name_hold_back in station_name_list:\n",
    "         \n",
    "        lat = []\n",
    "        lon = []\n",
    "        Cvar = []\n",
    "        for station_name in sorted(Cvar_dict.keys()):\n",
    "            if station_name in latlon_dict.keys():\n",
    "                if station_name != station_name_hold_back:\n",
    "                    loc = latlon_dict[station_name]\n",
    "                    latitude = loc[0]\n",
    "                    longitude = loc[1]\n",
    "                    cvar_val = Cvar_dict[station_name]\n",
    "                    lat.append(float(latitude))\n",
    "                    lon.append(float(longitude))\n",
    "                    Cvar.append(cvar_val)\n",
    "                else:\n",
    "                    #print('Skipping station!')\n",
    "                    pass\n",
    "                \n",
    "        y = np.array(lat)\n",
    "        x = np.array(lon)\n",
    "        z = np.array(Cvar) #what if we add the bounding locations to the array??? ==> that would be extrapolation not interpolation? \n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        bounds = na_map.bounds\n",
    "        xmax = bounds['maxx']\n",
    "        xmin= bounds['minx']\n",
    "        ymax = bounds['maxy']\n",
    "        ymin = bounds['miny']\n",
    "        pixelHeight = 10000 \n",
    "        pixelWidth = 10000\n",
    "                \n",
    "        num_col = int((xmax - xmin) / pixelHeight)\n",
    "        num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "\n",
    "        #We need to project to a projected system before making distance matrix\n",
    "        source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume \n",
    "        xProj, yProj = pyproj.Proj('esri:102001')(x,y)\n",
    "\n",
    "        yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']])\n",
    "        xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "\n",
    "        Yi1 = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row)\n",
    "        Xi1 = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "        Xi,Yi = np.meshgrid(Xi1,Yi1)\n",
    "        \n",
    "        empty_grid = np.empty((num_row,num_col,))*np.nan\n",
    "\n",
    "        for x3,y3,z3 in zip(x_origin_list,y_origin_list,z_origin_list):\n",
    "            empty_grid[y3][x3] = z3\n",
    "\n",
    "\n",
    "\n",
    "        vals = ~np.isnan(empty_grid)\n",
    "\n",
    "        OK = OrdinaryKriging(xProj,yProj,z,variogram_model=model,verbose=False,enable_plotting=False)\n",
    "        try: \n",
    "            z1,ss1 = OK.execute('grid',Xi1,Yi1,n_closest_points=10,backend='C') #n_closest_points=10\n",
    "    \n",
    "            kriging_surface = z1.reshape(num_row,num_col)\n",
    "\n",
    "        #Calc the RMSE, MAE at the pixel loc\n",
    "        #Delete at a certain point\n",
    "            coord_pair = projected_lat_lon[station_name_hold_back]\n",
    "\n",
    "            x_orig = int((coord_pair[0] - float(bounds['minx']))/pixelHeight) #lon \n",
    "            y_orig = int((coord_pair[1] - float(bounds['miny']))/pixelWidth) #lat\n",
    "            x_origin_list.append(x_orig)\n",
    "            y_origin_list.append(y_orig)\n",
    "\n",
    "            interpolated_val = kriging_surface[y_orig][x_orig] #which comes first?\n",
    "\n",
    "            original_val = Cvar_dict[station_name]\n",
    "            absolute_error = abs(interpolated_val-original_val)\n",
    "            absolute_error_dictionary[station_name_hold_back] = absolute_error\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "\n",
    "    return absolute_error_dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next group of functions is for calculating the FWI metrics. Much of this code is translated directly from the R cffdrs package, see: \n",
    "\n",
    "Wang, X., Wotton, B. M., Cantin, A. S., Parisien, M. A., Anderson, K., Moore, B., & Flannigan, M. D. (2017). cffdrs: an R package for the Canadian Forest Fire Danger Rating System. Ecological Processes, 6(1). https://doi.org/10.1186/s13717-017-0070-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "def get_date_index(str year,str input_date,int month):\n",
    "    '''Get the number of days for the date of interest from the first of the month of interest\n",
    "    Example, convert to days since March 1 \n",
    "    Parameters\n",
    "        year (str): year of interest \n",
    "        input_date (str): input date of interest\n",
    "        month (str): the month from when you want to calculate the date (ex, Mar 1)\n",
    "    Returns \n",
    "        day (int): days since 1st of the month of interest \n",
    "    '''\n",
    "    d0 = date(int(year), month, 1)\n",
    "    input_date = str(input_date)\n",
    "    d1 = date(int(input_date[0:4]), int(input_date[5:7]), int(input_date[8:10])) #convert to days since march 1/oct 1 so we can interpolate\n",
    "    delta = d1 - d0\n",
    "    day = int(delta.days)\n",
    "    return day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_start_date_mask(int day_index,day_interpolated_surface):\n",
    "    '''Turn the interpolated surface of start dates into a numpy array\n",
    "    Parameters\n",
    "        day_index (int): index of the day of interest since Mar 1\n",
    "        day_interpolated_surface (np_array): the interpolated surface of the start dates across the \n",
    "        study area \n",
    "    Returns \n",
    "        new (np_array): a mask array of the start date, either activated (1) or inactivated (np.nan)\n",
    "    '''\n",
    "    shape = day_interpolated_surface.shape\n",
    "    new = np.ones(shape)\n",
    "    new[day_interpolated_surface <= day_index] = 1 #If the day in the interpolated surface is before the index, it is activated \n",
    "    new[day_interpolated_surface > day_index] = np.nan #If it is the opposite it will be masked out, so assign it to np.nan (no data) \n",
    "    return new\n",
    "\n",
    "def make_end_date_mask(int day_index,day_interpolated_surface):\n",
    "    '''Turn the interpolated surface of end dates into a numpy array\n",
    "    Parameters\n",
    "        day_index (int): index of the day of interest since Oct 1\n",
    "        day_interpolated_surface (np_array): the interpolated surface of the end dates across the \n",
    "        study area \n",
    "    Returns \n",
    "        new (np_array): a mask array of the end date, either activated (1) or inactivated (np.nan)\n",
    "    '''\n",
    "    shape = day_interpolated_surface.shape\n",
    "    new = np.ones(shape)\n",
    "    new[day_interpolated_surface <= day_index] = np.nan #If the day in the interpolated surface is before the index, its closed\n",
    "    new[day_interpolated_surface > day_index] = 1 #If it is the opposite it will be left open, so assign it to 1\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate whether we need the overwinter procedure for each cell in the study area. These are areas where winter precipitation is less than 200 mm (Lawson & Armitage, 2008). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_overwinter_pcp(overwinter_dates, file_path_daily,start_surface,end_surface,maxmin, shapefile,\n",
    "                       show,date_dictionary,latlon_dictionary, file_path_elev,idx_list, json):\n",
    "    '''Get the total amount of overwinter pcp for the purpose of knowing where to use the \n",
    "    overwinter DC procedure \n",
    "    Parameters\n",
    "        overwinter_dates (list): list of dates that are in the winter (i.e. station shut down to \n",
    "        start up), you can just input generally Oct 1-June 1 and stations that are still active \n",
    "        will be masked out \n",
    "        file_path_daily (str): file path to the daily feather files containing the precipitation data\n",
    "        start_surface (np_array): array containing the interpolated start-up date for each cell \n",
    "        end_surface (np_array): array containing the interpolated end date for each cell, from the \n",
    "        year before \n",
    "        maxmin (list): bounds of the study area \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        date_dictionary (dict, loaded from .json): lookup file that has what day/month pairs each \n",
    "        station contains data for \n",
    "        latlon_dictionary (dict, loaded from .json): lat lons of the daily stations\n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        json (bool): if True, convert the array to a flat list so it can be written as a .json file\n",
    "        to the hard drive\n",
    "    Returns \n",
    "        pcp_overwinter (np_array): array of interpolated overwinter precipitation for the study area \n",
    "        overwinter_reqd (np_array): array indicating where to do the overwinter DC procedure \n",
    "    '''\n",
    "    pcp_list = [] \n",
    "\n",
    "    for o_dat in overwinter_dates: #dates is from Oct 1 year before to start day current year (up to Apr 1)\n",
    "        year = str(o_dat)[0:4]\n",
    "        index = overwinter_dates.index(o_dat) #we need to take index while its still a timestamp before convert to str\n",
    "        o_dat = str(o_dat)\n",
    "        day_index= get_date_index(year,o_dat,3)\n",
    "        eDay_index = get_date_index(year,o_dat,10)\n",
    "        if int(str(o_dat)[5:7]) >= 10:\n",
    "\n",
    "\n",
    "            invertEnd = np.ones(end_surface.shape)\n",
    "            endMask = make_end_date_mask(eDay_index,end_surface)\n",
    "            invertEnd[endMask == 1] = 0 #0 in place of np.nan, because we use sum function later \n",
    "            invertEnd[np.where(np.isnan(endMask))] = 1\n",
    "\n",
    "            endMask = invertEnd \n",
    "            mask = np.ones(start_surface.shape) #No stations will start up until the next spring\n",
    "        elif int(str(o_dat)[5:7]) < 10 and int(str(o_dat)[5:7]) >= 7: #All stations are in summer\n",
    "\n",
    "            endMask = np.zeros(end_surface.shape)\n",
    "            mask = np.zeros(start_surface.shape)\n",
    "        else:\n",
    "\n",
    "            endMask = np.ones(end_surface.shape) #by this time (Jan 1) all stations are in winter\n",
    "            invertMask = np.ones(start_surface.shape)\n",
    "            mask = make_start_date_mask(day_index,start_surface)\n",
    "            invertMask[mask == 1] = 0 #If the station has started, stop counting it \n",
    "            invertMask[np.where(np.isnan(mask))] = 1 #If the station is still closed, keep counting\n",
    "            mask = invertMask\n",
    "\n",
    "        rainfall = get_pcp(str(o_dat)[0:10],file_path_daily,date_dictionary)\n",
    "        rain_grid,maxmin = IDW(latlon_dictionary,rainfall,o_dat,'Precipitation',shapefile,False,1)\n",
    "\n",
    "        masked = rain_grid * mask * endMask\n",
    "\n",
    "        masked[np.where(np.isnan(masked))] = 0 #sum can't handle np.nan\n",
    "\n",
    "        pcp_list.append(masked)\n",
    "\n",
    "        \n",
    "    #when we sum, we need to treat nan as 0, otherwise any nan in the list will cause the whole val to be nan \n",
    "    pcp_overwinter = sum(pcp_list)\n",
    "\n",
    "\n",
    "    overwinter_reqd = np.ones(pcp_overwinter.shape)\n",
    "    overwinter_reqd[pcp_overwinter> 200] = np.nan #not Required\n",
    "    overwinter_reqd[pcp_overwinter <= 200] = 1 #required \n",
    "\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(overwinter_reqd,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        title = 'Areas Requiring Overwinter DC Procedure for %s'%(year)\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    if json:\n",
    "        pcp_overwinter = list(pcp_overwinter.flatten()) #json cannot handle numpy arrays \n",
    "        overwinter_reqd = list(overwinter_reqd.flatten()) \n",
    "\n",
    "    return pcp_overwinter,overwinter_reqd #This is the array, This is a mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descartes import PolygonPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DC(input_date,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,dc_yesterday,index,show,shapefile,mask,endMask,\n",
    "       last_DC_val_before_shutdown,overwinter):\n",
    "    '''Calculate the DC. See cffdrs R code\n",
    "    Parameters\n",
    "        input_date (str): input date of interest\n",
    "        rain_grid: interpolated surface for rainfall on the date of interest\n",
    "        temp_grid: interpolated surface for temperature on the date of interest\n",
    "        wind_grid: interpolated surface for wind on the date of interest\n",
    "        maxmin: bounds of the study area \n",
    "        dc_yesterday: array of DC values for yesterday (from the dc stack list/if this function\n",
    "        is being used inside dc_stack it is calculated then) \n",
    "        index (int): index of the date since Mar 1\n",
    "        show (bool): whether you want to show the map \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        mask (np_array): mask for the start dates \n",
    "        endMask (np_array): mask for the end days \n",
    "        last_DC_val_before_shutdown (np_array): array for last dc values before cell shut down, if no areas \n",
    "        required the procedure, you can input an empty array of the correct size (if not using overwinter, input\n",
    "        the empty array)\n",
    "        overwinter (bool): whether or not to implement the overwinter procedure \n",
    "    Returns \n",
    "        dc1 (np_array): array of dc values on the date on interest for the study area\n",
    "    '''\n",
    "    \n",
    "    yesterday_index = index-1\n",
    "    if yesterday_index == -1:\n",
    "        if overwinter:\n",
    "            rain_shape = rain_grid.shape\n",
    "            dc_initialize = np.zeros(rain_shape)\n",
    "            dc_initialize[np.isnan(last_DC_val_before_shutdown)] = 15\n",
    "            dc_initialize[~np.isnan(last_DC_val_before_shutdown)] = last_DC_val_before_shutdown[~np.isnan(last_DC_val_before_shutdown)]\n",
    "            dc_yesterday1 = dc_initialize*mask \n",
    "        else: \n",
    "            rain_shape = rain_grid.shape\n",
    "            dc_initialize = np.zeros(rain_shape)+15\n",
    "            dc_yesterday1 = dc_initialize\n",
    "            dc_yesterday1 = dc_yesterday1*mask #mask out areas that haven't started\n",
    "    else:\n",
    "        if overwinter:\n",
    "            dc_yesterday1 = dc_yesterday\n",
    "            dc_yesterday1[np.where(np.isnan(dc_yesterday1) & ~np.isnan(mask) & ~np.isnan(last_DC_val_before_shutdown))] = last_DC_val_before_shutdown[np.where(np.isnan(dc_yesterday1) & ~np.isnan(mask) & ~np.isnan(last_DC_val_before_shutdown))]\n",
    "            dc_yesterday1[np.where(np.isnan(dc_yesterday1) & ~np.isnan(mask) & np.isnan(last_DC_val_before_shutdown))] = 15\n",
    "        else: \n",
    "            dc_yesterday1 = dc_yesterday\n",
    "            dc_yesterday1[np.where(np.isnan(dc_yesterday1) & ~np.isnan(mask))] = 15 #set started pixels since yesterday to 15\n",
    "\n",
    "    input_date = str(input_date)\n",
    "    month = int(input_date[6])\n",
    "    #Get day length factor\n",
    "\n",
    "    f101 = [-1.6,-1.6,-1.6,0.9,3.8,5.8,6.4,5,2.4,0.4,-1.6,-1.6]\n",
    "\n",
    "    #Put constraint on low end of temp\n",
    "    temp_grid[temp_grid < -2.8] = -2.8\n",
    "\n",
    "    #E22 potential evapT\n",
    "\n",
    "    pe = (0.36*(temp_grid+2.8)+f101[month])/2\n",
    "\n",
    "    #Make empty dmc array\n",
    "    new_shape = dc_yesterday1.shape\n",
    "    dc = np.zeros(new_shape)\n",
    "\n",
    "    #starting rain\n",
    "\n",
    "    netRain = 0.83*rain_grid-1.27\n",
    "\n",
    "    #eq 19\n",
    "    smi = 800*np.exp(-1*dc_yesterday1/400)\n",
    "\n",
    "    #eq 21\n",
    "    dr0 = dc_yesterday1 -400*np.log(1+3.937*netRain/smi) #log is the natural logarithm \n",
    "    dr0[dr0<0] = 0\n",
    "    dr0[rain_grid <= 2.8] = dc_yesterday1[rain_grid <= 2.8]\n",
    "\n",
    "    dc1 = dr0 + pe\n",
    "    dc1[dc1 < 0] = 0\n",
    "        \n",
    "    dc1 = dc1 * mask * endMask\n",
    "    if show == True: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        circ = PolygonPatch(na_map['geometry'][0],visible=False)\n",
    "        ax.add_patch(circ) \n",
    "        plt.imshow(dc1,extent=(xProj_extent.min(),xProj_extent.max(),yProj_extent.max(),yProj_extent.min()),clip_path=circ, \n",
    "                   clip_on=True) \n",
    "      \n",
    "        #plt.imshow(dc1,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('DC') \n",
    "        \n",
    "        title = 'DC for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return dc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_stack(dates,file_path_daily,file_path_hourly,var_name,shapefile,day_interpolated_surface,end_interpolated_surface,\n",
    "             last_DC_val_before_shutdown,overwinter,file_path_elev,idx_list,date_dictionary,latlon_dict,latlon_dictionary,\n",
    "             json,interpolation_method):\n",
    "    '''Calc dc for each day in season. This is the only metric with overwinter procedure applied. \n",
    "    For notes see cffdrs R code.\n",
    "    Parameters\n",
    "        dates (list): list of all dates within the fire season, inactive stations will be masked out \n",
    "        so you can define it as Mar 1 - Dec 31\n",
    "        file_path_daily (str): file path to the daily feather files\n",
    "        file_path_hourly (str): file path to the hourly feather files\n",
    "        var_name (str): name of the variable you are interpolating\n",
    "        shapefile (str): path to the study area shapefile \n",
    "        day_interpolated_surface (np_array): array of start-up days (since Mar 1) for the study area\n",
    "        end_interpolated_surface (np_array): array of end days (since Oct 1) for the study area\n",
    "        last_DC_val_before_shutdown (np_array): the last DC value in each cell before that cell shut down\n",
    "        for the winter; only need to input this if overwinter = True otherwise you can just input None \n",
    "        overwinter (bool): if True, the program overwinters the DC where it needs to be\n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        date_dictionary (dict, loaded from .json): lookup file that has what day/month pairs each \n",
    "        station contains data for \n",
    "        latlon_dict (dict, loaded from .json): lat lons of the hourly stations\n",
    "        latlon_dictionary (dict, loaded from .json): lat lons of the daily stations\n",
    "        json (bool): if True, convert the array to a flat list so it can be written as a .json file\n",
    "        interpolation_method (str): the interpolation method to use to get the continuous DC surface, \n",
    "        there are seven options - IDW-1, IDW-2, IDEW-1, IDEW-2, TPS, TPSS, OK \n",
    "    Returns \n",
    "        dc_list (list of np_array): a list of the interpolated surfaces for the drought code for \n",
    "        each day in the fire season \n",
    "    '''\n",
    "\n",
    "    print(interpolation_method)\n",
    "    \n",
    "    dc_list = [] \n",
    "    count = 0 \n",
    "    for dat in dates:\n",
    "        gc.collect() \n",
    "        year = str(dat)[0:4]\n",
    "        index = dates.index(dat)\n",
    "        dat = str(dat)\n",
    "        day_index= get_date_index(year,dat,3)\n",
    "        eDay_index = get_date_index(year,dat,10)\n",
    "\n",
    "        mask1 = make_start_date_mask(day_index,day_interpolated_surface)\n",
    "        if eDay_index < 0:\n",
    "            endMask = np.ones(end_interpolated_surface.shape) #in the case that the index is before Oct 1\n",
    "        else: \n",
    "            endMask = make_end_date_mask(eDay_index,end_interpolated_surface)\n",
    "\n",
    "        hourly = str(dat)[0:10]+' 13:00'\n",
    "        \n",
    "        rainfall = get_pcp(str(dat)[0:10],file_path_daily,date_dictionary)\n",
    "        wind = get_wind_speed(hourly,file_path_hourly) #Using the list, get the data for wind speed for those stations on the input date\n",
    "        temp = get_noon_temp(hourly,file_path_hourly) #Using the list, get the data for temperature for those stations on the input date\n",
    "        rh =get_relative_humidity(hourly,file_path_hourly) #Using the list, get the data for rh% for those stations on the input date\n",
    "\n",
    "        #what type of interpolation are we using here?\n",
    "\n",
    "        if interpolation_method == 'IDW-1': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = IDW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,1)\n",
    "            temp_grid, maxmin = IDW(latlon_dict,temp,hourly,var_name,shapefile,False,1)\n",
    "            rh_grid, maxmin = IDW(latlon_dict,rh,hourly,var_name,shapefile,False,1)\n",
    "            wind_grid, maxmin = IDW(latlon_dict,wind,hourly,var_name,shapefile,False,1)\n",
    "\n",
    "        if interpolation_method == 'IDW-2': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = idw.IDW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,2)\n",
    "            temp_grid, maxmin = idw.IDW(latlon_dict,temp,hourly,var_name,shapefile,False,2)\n",
    "            rh_grid, maxmin = idw.IDW(latlon_dict,rh,hourly,var_name,shapefile,False,2)\n",
    "            wind_grid, maxmin = idw.IDW(latlon_dict,wind,hourly,var_name,shapefile,False,2)\n",
    "\n",
    "        if interpolation_method == 'IDEW-1':\n",
    "\n",
    "            rain_grid, maxmin, elev_array= IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            temp_grid, maxmin, elev_array= IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            rh_grid, maxmin, elev_array = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            wind_grid, maxmin, elev_array= IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            \n",
    "        if interpolation_method == 'IDEW-2':\n",
    "\n",
    "            rain_grid, maxmin, elev_array = IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            temp_grid, maxmin, elev_array = IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            rh_grid, maxmin, elev_array = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            wind_grid, maxmin, elev_array = IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "\n",
    "        if interpolation_method == 'TPS':\n",
    "\n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,0)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,0)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,0)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,0)\n",
    "\n",
    "        if interpolation_method == 'TPSS':\n",
    "\n",
    "            num_stations_R = len(rainfall.keys())\n",
    "            num_stations_t = len(temp.keys())\n",
    "            num_stations_rh = len(rh.keys())\n",
    "            num_stations_w = len(wind.keys())\n",
    "            \n",
    "            smoothing_parameterR = int(num_stations_R)-(math.sqrt(2*num_stations_R))\n",
    "            smoothing_parameterT = int(num_stations_t)-(math.sqrt(2*num_stations_t))\n",
    "            smoothing_parameterRH = int(num_stations_rh)-(math.sqrt(2*num_stations_rh))\n",
    "            smoothing_parameterW = int(num_stations_w)-(math.sqrt(2*num_stations_w))\n",
    "            \n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,smoothing_parameterR)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,smoothing_parameterT)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,smoothing_parameterRH)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,smoothing_parameterW)\n",
    "\n",
    "\n",
    "        if interpolation_method == 'OK':\n",
    "\n",
    "            rain_grid, maxmin = OKriging(latlon_dictionary,rainfall,dat,var_name,shapefile,False)\n",
    "            temp_grid, maxmin = OKriging(latlon_dict,temp,hourly,var_name,shapefile,False)\n",
    "            rh_grid, maxmin = OKriging(latlon_dict,rh,hourly,var_name,shapefile,False)\n",
    "            wind_grid, maxmin = OKriging(latlon_dict,wind,hourly,var_name,shapefile,False)\n",
    "\n",
    "        if (interpolation_method == 'OK' or interpolation_method == 'TPSS' or interpolation_method == 'TPS' or interpolation_method == 'IDEW-2'\\\n",
    "           or interpolation_method == 'IDEW-1' or interpolation_method == 'IDW-2' or interpolation_method == 'IDW-1') != True:\n",
    "\n",
    "            print('The entered interpolation method is not recognized')\n",
    "            sys.exit() \n",
    "        \n",
    "        if count > 0:  \n",
    "            dc_array = dc_list[count-1] #the last one added will be yesterday's val, but there's a lag bc none was added when count was0, so just use count-1\n",
    "            index = count-1\n",
    "            dc = DC(dat,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,dc_array,index,False,shapefile,mask1,endMask,last_DC_val_before_shutdown,overwinter)\n",
    "            dc_list.append(dc)\n",
    "        else: #Initialize procedure\n",
    "            if overwinter:\n",
    "                rain_shape = rain_grid.shape\n",
    "                dc_initialize = np.zeros(rain_shape)\n",
    "                dc_initialize[np.isnan(last_DC_val_before_shutdown)] = 15\n",
    "                dc_initialize[~np.isnan(last_DC_val_before_shutdown)] = last_DC_val_before_shutdown[~np.isnan(last_DC_val_before_shutdown)]\n",
    "                dc_list.append(dc_initialize*mask1)\n",
    "            else: \n",
    "                rain_shape = rain_grid.shape\n",
    "                dc_initialize = np.zeros(rain_shape)+15 #merge with the other overwinter array once it's calculated \n",
    "                dc_yesterday1 = dc_initialize*mask1\n",
    "                dc_list.append(dc_yesterday1) #placeholder \n",
    "        count += 1\n",
    "\n",
    "    if json:\n",
    "        dc_list = [i.tolist() for i in dc_list]\n",
    "\n",
    "    return dc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_dc_before_shutdown(dc_list,endsurface,overwinter_reqd,show,maxmin):\n",
    "    '''Get an array of the last dc vals before station shutdown for winter for the study area \n",
    "    Parameters\n",
    "        dc_list (list of np_array): a list of the interpolated surfaces for the drought code for \n",
    "        each day in the fire season \n",
    "        endsurface (np_array): array for end dates for the year before the year of interest \n",
    "        overwinter_reqd (np_array): where the overwinter procedure is required \n",
    "        show (bool): whether you want to plot the map \n",
    "        maxmin (list): bounds of the study area \n",
    "    Returns \n",
    "        last_DC_val_before_shutdown_masked_reshape (np_array): array of dc values before shutdown for \n",
    "        cells requiring overwinter procedure \n",
    "    '''\n",
    "\n",
    "    flatten_dc = list(map(lambda x:x.flatten(),dc_list)) #flatten the arrays for easier processing - avoid 3d array\n",
    "    stackDC = np.stack(flatten_dc,axis=-1) #Create an array from the list that we can index\n",
    "\n",
    "    days_since_mar1 = endsurface.flatten().astype(int)+214-1 #add 214 days (mar-aug31 to convert to days since March 1) to get index in the stack... based on make_end_date_mask() -1 for day before\n",
    "\n",
    "    last_DC_val_before_shutdown = stackDC[np.arange(len(stackDC)), days_since_mar1] #Index each cell in the array by the end date to get the last val\n",
    "    last_DC_val_before_shutdown_masked = last_DC_val_before_shutdown * overwinter_reqd.flatten() #Mask the areas that don't require the overwinter procedure\n",
    "    last_DC_val_before_shutdown_masked_reshape = last_DC_val_before_shutdown_masked.reshape(endsurface.shape)\n",
    "\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(last_DC_val_before_shutdown_masked_reshape,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('DC') \n",
    "        \n",
    "        title = 'Last DC'\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    return last_DC_val_before_shutdown_masked_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_overwinter_procedure(last_DC_val_before_shutdown_masked_reshape,overwinter_pcp,b_list):\n",
    "    '''Apply the overwinter procedure, see Wang et al. (2017) for more details \n",
    "    Parameters\n",
    "        last_DC_val_before_shutdown_masked_reshape (np_array): output of get_last_dc_before_shutdown, array containing the last\n",
    "        dc value before the station shut down interpolated across the study area \n",
    "        overwinter_pcp (np_array): overwinter precipitation amount in each cell \n",
    "        b_list (list, loaded from json): list containing information for the b parameter, can be formatted into an array \n",
    "    Returns \n",
    "        DCs (np_array): the new values for the start-up DC procedure, to be used in areas identified as needing the overwinter\n",
    "        procedure \n",
    "    '''\n",
    "    a = 1.0\n",
    "    b_array = np.array(b_list).reshape(overwinter_pcp.shape)\n",
    "    Qf = 800 * np.exp(-last_DC_val_before_shutdown_masked_reshape / 400)\n",
    "    Qs = a * Qf + b_array * (3.94 * overwinter_pcp)\n",
    "    DCs = 400 * np.log(800 / Qs) #this is natural logarithm \n",
    "\n",
    "    DCs[DCs < 15] = 15\n",
    "    return DCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib as plt \n",
    "import geopandas as gpd\n",
    "from descartes import PolygonPatch \n",
    "\n",
    "def DMC(input_date,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,dmc_yesterday,int index,bint show,str shapefile,\n",
    "        mask,endMask):\n",
    "    '''Calculate the DMC. See cffdrs R code\n",
    "    Parameters\n",
    "        input_date (str): input date of interest\n",
    "        rain_grid: interpolated surface for rainfall on the date of interest\n",
    "        rh_grid: interpolated surface for relative humidity on the date of interest\n",
    "        temp_grid: interpolated surface for temperature on the date of interest\n",
    "        wind_grid: interpolated surface for wind on the date of interest\n",
    "        maxmin: bounds of the study area \n",
    "        dmc_yesterday: array of DMC values for yesterday (from the dmc stack list/if this function\n",
    "        is being used inside dmc_stack it is calculated then) \n",
    "        index (int): index of the date since Mar 1\n",
    "        show (bool): whether you want to show the map \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        mask (np_array): mask for the start dates \n",
    "        endMask (np_array): mask for the end days \n",
    "    Returns \n",
    "        dmc (np_array): array of dmc values on the date on interest for the study area\n",
    "    '''\n",
    "    yesterday_index = index-1\n",
    "\n",
    "    if yesterday_index == -1:\n",
    "        rain_shape = rain_grid.shape\n",
    "        dmc_initialize = np.zeros(rain_shape)+6\n",
    "        dmc_yesterday1 = dmc_initialize*mask\n",
    "    else: \n",
    "        dmc_yesterday1 = dmc_yesterday\n",
    "        dmc_yesterday1[np.where(np.isnan(dmc_yesterday1) & ~np.isnan(mask))] = 6\n",
    "\n",
    "    #dmc_yesterday = dmc_yesterday1.flatten()\n",
    "    input_date = str(input_date)\n",
    "    month = int(input_date[6])\n",
    "    #Get day length factor\n",
    "\n",
    "    ell01 = [6.5, 7.5, 9, 12.8, 13.9, 13.9, 12.4, 10.9, 9.4, 8, 7, 6]\n",
    "\n",
    "    #Put constraint on low end of temp\n",
    "    temp_grid[temp_grid < -1.1] = -1.1\n",
    "\n",
    "    #Log drying rate\n",
    "    rk = 1.84*(temp_grid+1.1)*(100-rh_grid)*ell01[month]*1.0E-4\n",
    "\n",
    "    #Make empty dmc array\n",
    "    new_shape = dmc_yesterday1.shape\n",
    "    dmc = np.zeros(new_shape)\n",
    "\n",
    "    #starting rain\n",
    "\n",
    "    netRain = 0.92*rain_grid-1.27\n",
    "\n",
    "    #initial moisture content, modified same as cffdrs package\n",
    "    wmi = 20 + 280/np.exp(0.023*dmc_yesterday1)\n",
    "\n",
    "    #if else depending on yesterday dmc, eq.13\n",
    "    b = np.zeros(new_shape)\n",
    "\n",
    "\n",
    "    b[dmc_yesterday1 <= 33] = 100/(0.5+0.3*dmc_yesterday1[dmc_yesterday1 <= 33])\n",
    "    b[(dmc_yesterday1 > 33) & (dmc_yesterday1 < 65)] = 14-1.3*np.log(dmc_yesterday1[(dmc_yesterday1 > 33) & (dmc_yesterday1 < 65)]) # np.log is ln\n",
    "    b[dmc_yesterday1 >= 65] = 6.5*np.log(dmc_yesterday1[dmc_yesterday1 >= 65])-17.2\n",
    "        \n",
    "\n",
    "    #eq 14, modified in R package\n",
    "    wmr = wmi + 1000 * netRain/(48.77 + b * netRain)\n",
    "\n",
    "    #eq 15 modified to be same as cffdrs package\n",
    "    \n",
    "    pr0 = 43.43 * (5.6348 - np.log(wmr-20)) #natural logarithm\n",
    "\n",
    "    pr0[pr0 <0] = 0\n",
    "    \n",
    "\n",
    "    rk_pr0 =pr0 + rk\n",
    "    rk_ydmc = dmc_yesterday1 + rk #we want to add rk because that's the drying rate \n",
    "    dmc[netRain > 1.5] = rk_pr0[netRain > 1.5]\n",
    "    dmc[netRain <= 1.5] = rk_ydmc[netRain <= 1.5]\n",
    "\n",
    "\n",
    "\n",
    "    dmc[dmc < 0]=0\n",
    "\n",
    "    dmc = dmc * mask * endMask # mask out areas that haven't been activated\n",
    "\n",
    "\n",
    "\n",
    "    if show == True: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "        circ = PolygonPatch(na_map['geometry'][0],visible=False)\n",
    "        ax.add_patch(circ) \n",
    "        plt.imshow(dmc,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1),clip_path=circ, \n",
    "                   clip_on=True) \n",
    "        \n",
    "      \n",
    "        #plt.imshow(dmc,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('DMC') \n",
    "        \n",
    "        title = 'DMC for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return dmc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmc_stack(dates,file_path_daily,file_path_hourly,var_name,shapefile,day_interpolated_surface,\n",
    "end_interpolated_surface,file_path_elev,idx_list,date_dictionary,latlon_dict,latlon_dictionary,json,interpolation_method):\n",
    "    '''Calc dmc for each day in fire season. For notes see cffdrs R code.\n",
    "    Parameters\n",
    "        dates (list): list of all dates within the fire season, inactive stations will be masked out \n",
    "        so you can define it as Mar 1 - Dec 31\n",
    "        file_path_daily (str): file path to the daily feather files\n",
    "        file_path_hourly (str): file path to the hourly feather files\n",
    "        var_name (str): name of the variable you are interpolating\n",
    "        shapefile (str): path to the study area shapefile \n",
    "        day_interpolated_surface (np_array): array of start-up days (since Mar 1) for the study area\n",
    "        end_interpolated_surface (np_array): array of end days (since Oct 1) for the study area\n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        date_dictionary (dict, loaded from .json): lookup file that has what day/month pairs each \n",
    "        station contains data for \n",
    "        latlon_dict (dict, loaded from .json): lat lons of the hourly stations\n",
    "        latlon_dictionary (dict, loaded from .json): lat lons of the daily stations\n",
    "        json (bool): if True, convert the array to a flat list so it can be written as a .json file\n",
    "        interpolation_method (str): the interpolation method to use to get the continuous DMC surface, \n",
    "        there are seven options - IDW-1, IDW-2, IDEW-1, IDEW-2, TPS, TPSS, OK \n",
    "    Returns \n",
    "        dmc_list (list of np_array): a list of the interpolated surfaces for the duff moisture code for \n",
    "        each day in the fire season \n",
    "    '''\n",
    "    dmc_list = [] \n",
    "    count = 0 \n",
    "    for dat in dates:\n",
    "        index = dates.index(dat) #need to run BEFORE we convert to string \n",
    "        gc.collect() \n",
    "        year = str(dat)[0:4]\n",
    "        dat = str(dat)\n",
    "        day_index= get_date_index(year,dat,3)\n",
    "        eDay_index = get_date_index(year,dat,10)\n",
    "\n",
    "        mask = make_start_date_mask(day_index,day_interpolated_surface)\n",
    "        if eDay_index < 0:\n",
    "            endMask = np.ones(end_interpolated_surface.shape) #in the case that the index is before Oct 1\n",
    "        else: \n",
    "            endMask = make_end_date_mask(eDay_index,end_interpolated_surface)\n",
    "    \n",
    "        hourly = str(dat)[0:10]+' 13:00'\n",
    "        \n",
    "        rainfall = get_pcp(str(dat)[0:10],file_path_daily,date_dictionary)\n",
    "        wind = get_wind_speed(hourly,file_path_hourly) #Using the list, get the data for wind speed for those stations on the input date\n",
    "        temp = get_noon_temp(hourly,file_path_hourly) #Using the list, get the data for temperature for those stations on the input date\n",
    "        rh = get_relative_humidity(hourly,file_path_hourly) #Using the list, get the data for rh% for those stations on the input date\n",
    "        \n",
    "        #what type of interpolation are we using here?\n",
    "\n",
    "        if interpolation_method == 'IDW-1': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = IDW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,1)\n",
    "            temp_grid, maxmin = IDW(latlon_dict,temp,hourly,var_name,shapefile,False,1)\n",
    "            rh_grid, maxmin = IDW(latlon_dict,rh,hourly,var_name,shapefile,False,1)\n",
    "            wind_grid, maxmin = IDW(latlon_dict,wind,hourly,var_name,shapefile,False,1)\n",
    "\n",
    "        if interpolation_method == 'IDW-2': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = IDW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,2)\n",
    "            temp_grid, maxmin = IDW(latlon_dict,temp,hourly,var_name,shapefile,False,2)\n",
    "            rh_grid, maxmin = IDW(latlon_dict,rh,hourly,var_name,shapefile,False,2)\n",
    "            wind_grid, maxmin = IDW(latlon_dict,wind,hourly,var_name,shapefile,False,2)\n",
    "\n",
    "        if interpolation_method == 'IDEW-1':\n",
    "\n",
    "            rain_grid, maxmin, elev_array  = IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            temp_grid, maxmin, elev_array  = IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            rh_grid, maxmin, elev_array  = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            wind_grid, maxmin, elev_array  = IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            \n",
    "        if interpolation_method == 'IDEW-2':\n",
    "\n",
    "            rain_grid, maxmin, elev_array  = IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            temp_grid, maxmin, elev_array  = IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            rh_grid, maxmin, elev_array  = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            wind_grid, maxmin, elev_array  = IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "\n",
    "        if interpolation_method == 'TPS':\n",
    "\n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,0)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,0)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,0)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,0)\n",
    "\n",
    "        if interpolation_method == 'TPSS':\n",
    "\n",
    "            num_stations_R = len(rainfall.keys())\n",
    "            num_stations_t = len(temp.keys())\n",
    "            num_stations_rh = len(rh.keys())\n",
    "            num_stations_w = len(wind.keys())\n",
    "            \n",
    "            smoothing_parameterR = int(num_stations_R)-(math.sqrt(2*num_stations_R))\n",
    "            smoothing_parameterT = int(num_stations_t)-(math.sqrt(2*num_stations_t))\n",
    "            smoothing_parameterRH = int(num_stations_rh)-(math.sqrt(2*num_stations_rh))\n",
    "            smoothing_parameterW = int(num_stations_w)-(math.sqrt(2*num_stations_w))\n",
    "            \n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,smoothing_parameterR)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,smoothing_parameterT)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,smoothing_parameterRH)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,smoothing_parameterW)\n",
    "\n",
    "\n",
    "        if interpolation_method == 'OK':\n",
    "\n",
    "            rain_grid, maxmin = OKriging(latlon_dictionary,rainfall,dat,var_name,shapefile,False)\n",
    "            temp_grid, maxmin = OKriging(latlon_dict,temp,hourly,var_name,shapefile,False)\n",
    "            rh_grid, maxmin = OKriging(latlon_dict,rh,hourly,var_name,shapefile,False)\n",
    "            wind_grid, maxmin = OKriging(latlon_dict,wind,hourly,var_name,shapefile,False)\n",
    "\n",
    "        if (interpolation_method == 'OK' or interpolation_method == 'TPSS' or interpolation_method == 'TPS' or interpolation_method == 'IDEW-2'\\\n",
    "           or interpolation_method == 'IDEW-1' or interpolation_method == 'IDW-2' or interpolation_method == 'IDW-1') != True:\n",
    "\n",
    "            print('The entered interpolation method is not recognized')\n",
    "            sys.exit()\n",
    "            \n",
    "        if count > 0:  \n",
    "            dmc_array = dmc_list[count-1] #the last one added will be yesterday's val, but there's a lag bc none was added when count was0, so just use count-1\n",
    "            index = count-1\n",
    "            dmc = DMC(dat,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,dmc_array,index,False,shapefile,mask,endMask)\n",
    "            dmc_list.append(dmc)\n",
    "        else:\n",
    "            rain_shape = rain_grid.shape\n",
    "            dmc_initialize = np.zeros(rain_shape)+6\n",
    "            dmc_yesterday1 = dmc_initialize*mask\n",
    "            dmc_list.append(dmc_yesterday1) #placeholder \n",
    "        count += 1\n",
    "\n",
    "    if json:\n",
    "        dmc_list = [i.tolist() for i in dmc_list]\n",
    "\n",
    "    return dmc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib as plt \n",
    "import geopandas as gpd\n",
    "from descartes import PolygonPatch \n",
    "\n",
    "def FFMC(input_date,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,ffmc_yesterday,int index,bint show,str shapefile,\n",
    "         mask,endMask):\n",
    "    '''Calculate the FFMC. See cffdrs R code\n",
    "    Parameters\n",
    "        input_date (str): input date of interest\n",
    "        rain_grid: interpolated surface for rainfall on the date of interest\n",
    "        rh_grid: interpolated surface for relative humidity on the date of interest\n",
    "        temp_grid: interpolated surface for temperature on the date of interest\n",
    "        wind_grid: interpolated surface for wind on the date of interest\n",
    "        maxmin: bounds of the study area \n",
    "        ffmc_yesterday: array of FFMC values for yesterday (from the ffmc stack list/if this function\n",
    "        is being used inside ffmc_stack it is calculated then) \n",
    "        index (int): index of the date since Mar 1\n",
    "        show (bool): whether you want to show the map \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        mask (np_array): mask for the start dates \n",
    "        endMask (np_array): mask for the end days \n",
    "    Returns \n",
    "        ffmc1 (np_array): array of ffmc values on the date on interest for the study area\n",
    "    '''\n",
    "    cdef int yesterday_index = index-1\n",
    "\n",
    "    if yesterday_index == -1:\n",
    "        rain_shape = rain_grid.shape\n",
    "        ffmc_initialize = np.zeros(rain_shape)+85\n",
    "        ffmc_yesterday1 = ffmc_initialize*mask #mask out areas that haven't started\n",
    "    else: \n",
    "        ffmc_yesterday1 = ffmc_yesterday\n",
    "        ffmc_yesterday1[np.where(np.isnan(ffmc_yesterday1) & ~np.isnan(mask))] = 85 #set started pixels since yesterday to 85\n",
    "\n",
    "\n",
    "    wmo = 147.2*(101-ffmc_yesterday)/(59.5+ffmc_yesterday)\n",
    "\n",
    "    rain_grid[rain_grid > 0.5] = rain_grid[rain_grid > 0.5] - 0.5\n",
    "\n",
    "    wmo[wmo>=150]=wmo[wmo >= 150]+0.0015*(wmo[wmo >= 150]-150)*\\\n",
    "                     (wmo[wmo >= 150]- 150)*np.sqrt(rain_grid[wmo >= 150]) + 42.5\\\n",
    "                     *rain_grid[wmo >= 150]*np.exp(-100/(251-wmo[wmo >= 150]))*\\\n",
    "                     (1-np.exp(-6.93/rain_grid[wmo >= 150]))\n",
    "    \n",
    "    wmo[wmo<150]=wmo[wmo<150]+42.5*rain_grid[wmo<150]*np.exp(-100/(251-wmo[wmo<150]))\\\n",
    "                  *(1-np.exp(-6.93/rain_grid[wmo<150]))\n",
    "\n",
    "    wmo[rain_grid < 0.5] = 147.2*(101-ffmc_yesterday[rain_grid < 0.5])/(59.5+ffmc_yesterday[rain_grid < 0.5])\n",
    "\n",
    "    wmo[wmo>250] = 250\n",
    "\n",
    "    ed=0.942*np.power(rh_grid,0.679)+(11*np.exp((rh_grid-100)/10))+0.18*(21.1-temp_grid)\\\n",
    "        *(1-1/np.exp(rh_grid*0.115))\n",
    "    \n",
    "    ew=0.618*np.power(rh_grid,0.753)+(10*np.exp((rh_grid-100)/10))+0.18*(21.1-temp_grid)*\\\n",
    "        (1-1/np.exp(rh_grid*0.115))\n",
    "\n",
    "    shape = rain_grid.shape \n",
    "    z = np.zeros(shape)\n",
    "    z[np.where((wmo<ed) & (wmo<ew))]=0.424*(1-np.power((rh_grid[np.where((wmo<ed) & (wmo<ew))]/100),1.7))\\\n",
    "                          +0.0694*np.sqrt(wind_grid[np.where((wmo<ed) & (wmo<ew))])*\\\n",
    "                  (1-np.power((rh_grid[np.where((wmo<ed) & (wmo<ew))]/100),8))\n",
    "    \n",
    "\n",
    "    z[np.where((wmo>=ed) & (wmo>=ew))] = 0\n",
    "\n",
    "    x=z*0.581*np.exp(0.0365*temp_grid)\n",
    "\n",
    "    shape = rain_grid.shape \n",
    "    wm = np.zeros(shape)    \n",
    "\n",
    "    wm[np.where((wmo<ed) & (wmo<ew))]= ew[np.where((wmo<ed) & (wmo<ew))]-\\\n",
    "                           (ew[np.where((wmo<ed) & (wmo<ew))]-\\\n",
    "                            wmo[np.where((wmo<ed) & (wmo<ew))])/(np.power(10,x[np.where((wmo<ed) & (wmo<ew))]))\n",
    "\n",
    "    wm[np.where((wmo>=ed) & (wmo>=ew))] = wmo[np.where((wmo>=ed) & (wmo>=ew))]\n",
    "\n",
    "    z[wmo>ed] = 0.424*(1-np.power((rh_grid[wmo>ed]/100),1.7))+0.0694\\\n",
    "                       *np.sqrt(wind_grid[wmo>ed])*(1-np.power((rh_grid[wmo>ed]/100),8))\n",
    "\n",
    "    x=z*0.581*np.exp(0.0365 * temp_grid)\n",
    "    wm[wmo>ed] = ed[wmo>ed] + (wmo[wmo>ed] - ed[wmo>ed])/(np.power(10,x[wmo>ed]))\n",
    "\n",
    "    ffmc1 = (59.5*(250-wm))/(147.2+wm)\n",
    "    \n",
    "    ffmc1[ffmc1 > 101] = 101\n",
    "\n",
    "    ffmc1[ffmc1 < 0] = 0\n",
    "\n",
    "    ffmc1 = ffmc1*mask* endMask\n",
    "\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "        circ = PolygonPatch(na_map['geometry'][0],visible=False)\n",
    "        ax.add_patch(circ) \n",
    "        plt.imshow(ffmc1,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1),clip_path=circ, \n",
    "                   clip_on=True) \n",
    "        \n",
    "        #plt.imshow(ffmc1,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('FFMC') \n",
    "        \n",
    "        title = 'FFMC for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return ffmc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffmc_stack(dates,file_path_daily,file_path_hourly,var_name,shapefile,day_interpolated_surface,\n",
    "               end_interpolated_surface,file_path_elev,idx_list,date_dictionary,latlon_dict,latlon_dictionary,\n",
    "               json,interpolation_method):\n",
    "    '''Calc ffmc for each day in season. For notes see cffdrs R code.\n",
    "    Parameters\n",
    "        dates (list): list of all dates within the fire season, inactive stations will be masked out \n",
    "        so you can define it as Mar 1 - Dec 31\n",
    "        file_path_daily (str): file path to the daily feather files\n",
    "        file_path_hourly (str): file path to the hourly feather files\n",
    "        var_name (str): name of the variable you are interpolating\n",
    "        shapefile (str): path to the study area shapefile \n",
    "        day_interpolated_surface (np_array): array of start-up days (since Mar 1) for the study area\n",
    "        end_interpolated_surface (np_array): array of end days (since Oct 1) for the study area\n",
    "        file_path_elev (str): file path to the elevation lookup file \n",
    "        idx_list (list): the index of the elevation data column in the lookup file \n",
    "        date_dictionary (dict, loaded from .json): lookup file that has what day/month pairs each \n",
    "        station contains data for \n",
    "        latlon_dict (dict, loaded from .json): lat lons of the hourly stations\n",
    "        latlon_dictionary (dict, loaded from .json): lat lons of the daily stations\n",
    "        json (bool): if True, convert the array to a flat list so it can be written as a .json file\n",
    "        interpolation_method (str): the interpolation method to use to get the continuous DMC surface, \n",
    "        there are seven options - IDW-1, IDW-2, IDEW-1, IDEW-2, TPS, TPSS, OK \n",
    "    Returns \n",
    "        ffmc_list (list of np_array): a list of the interpolated surfaces for the fine fuel moisture code for \n",
    "        each day in the fire season \n",
    "    '''\n",
    "    ffmc_list = [] \n",
    "    count = 0 \n",
    "    for dat in dates:\n",
    "        index = dates.index(dat) #need to run BEFORE we convert to string \n",
    "        gc.collect() \n",
    "        year = str(dat)[0:4]\n",
    "        dat = str(dat) #convert to str so that cython doesn't get confused \n",
    "        day_index= get_date_index(year,dat,3)\n",
    "        eDay_index = get_date_index(year,dat,10)\n",
    "\n",
    "        mask = make_start_date_mask(day_index,day_interpolated_surface)\n",
    "        if eDay_index < 0:\n",
    "            endMask = np.ones(end_interpolated_surface.shape) #in the case that the index is before Oct 1\n",
    "        else: \n",
    "            endMask = make_end_date_mask(eDay_index,end_interpolated_surface)\n",
    "\n",
    "        hourly = str(dat)[0:10]+' 13:00'\n",
    "        rainfall = get_pcp(str(dat)[0:10],file_path_daily,date_dictionary)\n",
    "        wind = get_wind_speed(hourly,file_path_hourly) #Using the list, get the data for wind speed for those stations on the input date\n",
    "        temp = get_noon_temp(hourly,file_path_hourly) #Using the list, get the data for temperature for those stations on the input date\n",
    "        rh = get_relative_humidity(hourly,file_path_hourly) #Using the list, get the data for rh% for those stations on the input date\n",
    "        \n",
    "        #what type of interpolation are we using here?\n",
    "\n",
    "        if interpolation_method == 'IDW-1': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = IDW(latlon_dictionary,rainfall,str(dat),var_name,shapefile,False,1)\n",
    "            temp_grid, maxmin = IDW(latlon_dict,temp,hourly,var_name,shapefile,False,1)\n",
    "            rh_grid, maxmin = IDW(latlon_dict,rh,hourly,var_name,shapefile,False,1)\n",
    "            wind_grid, maxmin = IDW(latlon_dict,wind,hourly,var_name,shapefile,False,1)\n",
    "\n",
    "        if interpolation_method == 'IDW-2': \n",
    "\n",
    "        \n",
    "            rain_grid, maxmin = IDW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,2)\n",
    "            temp_grid, maxmin = IDW(latlon_dict,temp,hourly,var_name,shapefile,False,2)\n",
    "            rh_grid, maxmin = IDW(latlon_dict,rh,hourly,var_name,shapefile,False,2)\n",
    "            wind_grid, maxmin = IDW(latlon_dict,wind,hourly,var_name,shapefile,False,2)\n",
    "\n",
    "        if interpolation_method == 'IDEW-1':\n",
    "\n",
    "            rain_grid, maxmin, elev_array = IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            temp_grid, maxmin, elev_array = IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            rh_grid, maxmin, elev_array = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            wind_grid, maxmin, elev_array = IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,1)\n",
    "            \n",
    "        if interpolation_method == 'IDEW-2':\n",
    "\n",
    "            rain_grid, maxmin, elev_array = IDEW(latlon_dictionary,rainfall,dat,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            temp_grid, maxmin, elev_array = IDEW(latlon_dict,temp,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            rh_grid, maxmin, elev_array = IDEW(latlon_dict,rh,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "            wind_grid, maxmin, elev_array = IDEW(latlon_dict,wind,hourly,var_name,shapefile,False,file_path_elev,idx_list,2)\n",
    "\n",
    "        if interpolation_method == 'TPS':\n",
    "\n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,0)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,0)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,0)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,0)\n",
    "\n",
    "        if interpolation_method == 'TPSS':\n",
    "\n",
    "            num_stations_R = len(rainfall.keys())\n",
    "            num_stations_t = len(temp.keys())\n",
    "            num_stations_rh = len(rh.keys())\n",
    "            num_stations_w = len(wind.keys())\n",
    "            \n",
    "            smoothing_parameterR = int(num_stations_R)-(math.sqrt(2*num_stations_R))\n",
    "            smoothing_parameterT = int(num_stations_t)-(math.sqrt(2*num_stations_t))\n",
    "            smoothing_parameterRH = int(num_stations_rh)-(math.sqrt(2*num_stations_rh))\n",
    "            smoothing_parameterW = int(num_stations_w)-(math.sqrt(2*num_stations_w))\n",
    "            \n",
    "            rain_grid, maxmin = TPS(latlon_dictionary,rainfall,dat,var_name,shapefile,False,smoothing_parameterR)\n",
    "            temp_grid, maxmin = TPS(latlon_dict,temp,hourly,var_name,shapefile,False,smoothing_parameterT)\n",
    "            rh_grid, maxmin = TPS(latlon_dict,rh,hourly,var_name,shapefile,False,smoothing_parameterRH)\n",
    "            wind_grid, maxmin = TPS(latlon_dict,wind,hourly,var_name,shapefile,False,smoothing_parameterW)\n",
    "\n",
    "        if interpolation_method == 'OK':\n",
    "\n",
    "            rain_grid, maxmin = OKriging(latlon_dictionary,rainfall,dat,var_name,shapefile,False)\n",
    "            temp_grid, maxmin = OKriging(latlon_dict,temp,hourly,var_name,shapefile,False)\n",
    "            rh_grid, maxmin = OKriging(latlon_dict,rh,hourly,var_name,shapefile,False)\n",
    "            wind_grid, maxmin = OKriging(latlon_dict,wind,hourly,var_name,shapefile,False)\n",
    "\n",
    "        if (interpolation_method == 'OK' or interpolation_method == 'TPSS' or interpolation_method == 'TPS' or interpolation_method == 'IDEW-2'\\\n",
    "           or interpolation_method == 'IDEW-1' or interpolation_method == 'IDW-2' or interpolation_method == 'IDW-1') != True:\n",
    "\n",
    "            print('The entered interpolation method is not recognized')\n",
    "            sys.exit()\n",
    "            \n",
    "        if count > 0:  \n",
    "            ffmc_array = ffmc_list[count-1] #the last one added will be yesterday's val, but there's a lag bc none was added when count was0, so just use count-1\n",
    "            index = count-1\n",
    "            ffmc = FFMC(dat,rain_grid,rh_grid,temp_grid,wind_grid,maxmin,ffmc_array,index,False,shapefile,mask,endMask)\n",
    "            ffmc_list.append(ffmc)\n",
    "        else:\n",
    "            rain_shape = rain_grid.shape\n",
    "            ffmc_initialize = np.zeros(rain_shape)+85\n",
    "            ffmc_yesterday1 = ffmc_initialize*mask\n",
    "            ffmc_list.append(ffmc_yesterday1) #placeholder\n",
    "                \n",
    "                \n",
    "        count += 1\n",
    "\n",
    "    if json:\n",
    "        ffmc_list = [i.tolist() for i in ffmc_list]\n",
    "\n",
    "    return ffmc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BUI(dmc,dc,maxmin,show,shapefile,mask,endMask): #BUI can be calculated on the fly\n",
    "    ''' Calculate BUI\n",
    "    Parameters\n",
    "        dmc (np_array): the dmc array for the date of interest\n",
    "        dc (np_array): the dc array for the date of interest\n",
    "        maxmin (list): bounds of the study area \n",
    "        show (bool): whether or not to display the map \n",
    "        shapefile (str): path to the study area shapefile \n",
    "        mask (np_array): mask for the start up date \n",
    "        endMask (np_array): mask for the shut down date \n",
    "    Returns \n",
    "        bui1 (np_array): array containing BUI values for the study area \n",
    "    '''\n",
    "    shape = dmc.shape\n",
    "    bui1 = np.zeros(shape)\n",
    "    \n",
    "    bui1[np.where((dmc == 0) & (dc == 0))] = 0\n",
    "    bui1[np.where((dmc > 0) & (dc > 0))] = 0.8 * dc[np.where((dmc > 0) & (dc > 0))]*\\\n",
    "                                          dmc[np.where((dmc > 0) & (dc > 0))]/(dmc[np.where((dmc > 0) & (dc > 0))]\\\n",
    "                                          + 0.4 * dc[np.where((dmc > 0) & (dc > 0))])\n",
    "    p = np.zeros(shape)\n",
    "    p[dmc == 0] = 0\n",
    "    p[dmc > 0] = (dmc[dmc > 0] - bui1[dmc > 0])/dmc[dmc > 0]\n",
    "\n",
    "    cc = 0.92 + (np.power((0.0114 * dmc),1.7))\n",
    "    \n",
    "    bui0 = dmc - cc * p\n",
    "\n",
    "    bui0[bui0 < 0] = 0\n",
    "\n",
    "    bui1[bui1 < dmc] = bui0[bui1 < dmc]\n",
    "\n",
    "    bui1 = bui1*mask* endMask\n",
    "\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(bui1,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('BUI') \n",
    "        \n",
    "        title = 'BUI for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return bui1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISI(ffmc,wind_grid,maxmin,show,shapefile,mask,endMask):\n",
    "    ''' Calculate ISI\n",
    "    Parameters\n",
    "        ffmc (np_array): ffmc array for the date of interest\n",
    "        wind_grid (np_array): wind speed interpolated array for the date of interest\n",
    "        maxmin (list): bounds of the shapefile\n",
    "        show (bool): whether or not to display the map \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        mask (np_array): mask for the start up date\n",
    "        endMask (np_array): mask for the shutdown date\n",
    "    Returns \n",
    "        isi (np_array): the calculated array for ISI for the study area \n",
    "    '''\n",
    "\n",
    "    fm = 147.2 * (101 - ffmc)/(59.5 + ffmc)\n",
    "\n",
    "    \n",
    "    fW = np.exp(0.05039 * wind_grid)\n",
    "    \n",
    "\n",
    "    fF = 91.9 * np.exp((-0.1386) * fm) * (1 + (fm**5.31) / 49300000)\n",
    "\n",
    "\n",
    "    isi = 0.208 * fW * fF\n",
    "    \n",
    "    isi = isi*mask* endMask\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(isi,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('ISI') \n",
    "        \n",
    "        title = 'ISI for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    return isi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FWI(isi,bui,maxmin,show,shapefile,mask,endMask):\n",
    "    ''' Calculate FWI\n",
    "    Parameters\n",
    "        isi (np_array): calculated isi surface for the date of interest \n",
    "        bui (np_array): calculated bui surface for the date of interest \n",
    "        maxmin (list): bounds of the study area\n",
    "        show (bool): whether or not to show the map \n",
    "        shapefile (str): path to the shapefile \n",
    "        mask (np_array): start up mask \n",
    "        endMask (np_array): shut down mask \n",
    "    Returns \n",
    "        fwi (np_array): calculated FWI surface for the study area \n",
    "    '''\n",
    "\n",
    "    shape = isi.shape\n",
    "    bb = np.zeros(shape)\n",
    "\n",
    "    bb[bui > 80] = 0.1 * isi[bui > 80] * (1000/(25 + 108.64/np.exp(0.023 * bui[bui > 80])))\n",
    "    bb[bui <= 80] =  0.1 * isi[bui <= 80] * (0.626 * np.power(bui[bui <= 80],0.809) + 2)\n",
    "\n",
    "    fwi = np.zeros(shape)\n",
    "    fwi[bb <= 1] = bb[bb <= 1]\n",
    "    fwi[bb > 1] = np.exp(2.72 * ((0.434 * np.log(bb[bb > 1]))**0.647)) #natural logarithm \n",
    "\n",
    "    fwi = fwi * mask * endMask\n",
    "\n",
    "    if show: \n",
    "        min_yProj_extent = maxmin[0]\n",
    "        max_yProj_extent = maxmin[1]\n",
    "        max_xProj_extent = maxmin[2]\n",
    "        min_xProj_extent = maxmin[3]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (15,15))\n",
    "        crs = {'init': 'esri:102001'}\n",
    "\n",
    "        na_map = gpd.read_file(shapefile)\n",
    "        \n",
    "      \n",
    "        plt.imshow(fwi,extent=(min_xProj_extent-1,max_xProj_extent+1,max_yProj_extent-1,min_yProj_extent+1)) \n",
    "        na_map.plot(ax = ax,color='white',edgecolor='k',linewidth=2,zorder=10,alpha=0.1)\n",
    "            \n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('FWI') \n",
    "        \n",
    "        title = 'FWI for %s'%(input_date) \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        \n",
    "        plt.show()\n",
    " \n",
    "    return fwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task to complete is to find the highest FWI metrics inside of an individual fire during the first four days since the report date. See: \n",
    "\n",
    "Amiro, B. D., Logan, K. A., Wotton, B. M., Flannigan, M. D., Todd, J. B., Stocks, B. J., & Martell, D. L. (2004). Fire weather index system components for large fires in the Canadian boreal forest. International Journal of Wildland Fire, 13(4), 391–400. https://doi.org/10.1071/WF03066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interpolated_val_in_fire(fire_shapefile,shapefile,latlon_dict,interpolated_surface):\n",
    "    '''This is a function to get the FWI metric value inside the fire.\n",
    "    We will use to calculate the max FWI metrics for a fire.\n",
    "    Parameters\n",
    "        fire_shapefile (str): path to the fire shapefile \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        latlon_dict (dict, loaded from json): dictionary of lat lon for each station \n",
    "        interpolated_surface (np_array): an array of values in the study area \n",
    "    Returns \n",
    "        ival, max_ival (either, float): maximum value in fire, either the closest point to the convex hull of the fire or \n",
    "        a sum of the points inside the fire\n",
    "    '''\n",
    "    lat = [] #Initialize empty lists to store data \n",
    "    lon = []\n",
    "\n",
    "    for station_name in latlon_dict.keys(): #Loop through the list of stations \n",
    "        loc = latlon_dict[station_name]\n",
    "        latitude = loc[0]\n",
    "        longitude = loc[1]\n",
    "        lat.append(float(latitude))\n",
    "        lon.append(float(longitude))\n",
    "\n",
    "    y = np.array(lat) #Convert to a numpy array for faster processing speed \n",
    "    x = np.array(lon)\n",
    "\n",
    "\n",
    "    na_map = gpd.read_file(shapefile)\n",
    "    bounds = na_map.bounds #Get the bounding box of the shapefile \n",
    "    xmax = bounds['maxx']\n",
    "    xmin= bounds['minx']\n",
    "    ymax = bounds['maxy']\n",
    "    ymin = bounds['miny']\n",
    "    pixelHeight = 10000 #We want a 10 by 10 pixel, or as close as we can get \n",
    "    pixelWidth = 10000\n",
    "            \n",
    "    num_col = int((xmax - xmin) / pixelHeight) #Calculate the number of rows cols to fill the bounding box at that resolution \n",
    "    num_row = int((ymax - ymin) / pixelWidth)\n",
    "\n",
    "    source_proj = pyproj.Proj(proj='latlong', datum = 'NAD83') #We dont know but assume NAD83\n",
    "    xProj, yProj = pyproj.Proj('esri:102001')(x,y) #Convert to Canada Albers Equal Area \n",
    "\n",
    "    yProj_extent=np.append(yProj,[bounds['maxy'],bounds['miny']]) #Add the bounding box coords to the dataset so we can extrapolate the interpolation to cover whole area\n",
    "    xProj_extent=np.append(xProj,[bounds['maxx'],bounds['minx']])\n",
    "    \n",
    "\n",
    "    Yi = np.linspace(np.min(yProj_extent),np.max(yProj_extent),num_row) #Get the value for lat lon in each cell we just made \n",
    "    Xi = np.linspace(np.min(xProj_extent),np.max(xProj_extent),num_col)\n",
    "\n",
    "    Xi,Yi = np.meshgrid(Xi,Yi)\n",
    "    concat = np.array((Xi.flatten(), Yi.flatten())).T #Because we are not using the lookup file, send in X,Y order \n",
    "    send_to_list = concat.tolist()\n",
    "\n",
    "    fire_map = gpd.read_file(fire_shapefile)\n",
    "    DF = fire_map.geometry.unary_union\n",
    "\n",
    "    meshPoints = [Point(item) for item in send_to_list]\n",
    "    df = pd.DataFrame(meshPoints)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=meshPoints)\n",
    "    \n",
    "    within_fire = gdf[gdf.geometry.within(DF)]\n",
    "\n",
    "\n",
    "    if len(within_fire) == 0:\n",
    "        #Get concave hull of the multipolygon\n",
    "        try: \n",
    "            bounding_box = DF.convex_hull\n",
    "            approx_centre_point = bounding_box.centroid.coords\n",
    "        except: #theres only one polygon\n",
    "            approx_centre_point = DF.centroid.coords\n",
    "\n",
    "        reshaped_coord= np.array(approx_centre_point)\n",
    "        hypot = cdist(concat,reshaped_coord)\n",
    "        where_distance_small = np.argmin(hypot) #argmin returns the index of where the smallest item is \n",
    "\n",
    "        centre = concat[where_distance_small]\n",
    "\n",
    "        #Get interpolated value here\n",
    "        intF = interpolated_surface.flatten() \n",
    "        ival = intF[where_distance_small]\n",
    " \n",
    "\n",
    "        return ival \n",
    "    \n",
    "    else: \n",
    "\n",
    "        intF = interpolated_surface.flatten()\n",
    "        listP = within_fire[0].tolist()\n",
    "        tupArray =[x.coords for x in listP]\n",
    "        xyFire = [(x[0][0],x[0][1],) for x in tupArray]\n",
    "\n",
    "        tup_sendtolist =[tuple(x) for x in send_to_list]\n",
    "        \n",
    "        index = [] \n",
    "        for pair in xyFire:\n",
    "\n",
    "        \n",
    "            indices = [i for i, x in enumerate(tup_sendtolist) if x == pair]\n",
    "\n",
    "            index.append(indices)\n",
    "\n",
    "        ivals = []\n",
    "        for i in index: \n",
    "\n",
    "            ivals.append(intF[i])\n",
    "\n",
    "\n",
    "        max_ival = max(ivals) #get the max val inside the fire\n",
    "\n",
    "        \n",
    "        return float(max_ival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_value_first_four_days(fire_shapefile,shapefile,latlon_dict,interpolated_surface_d1,interpolated_surface_d2,\n",
    "                                  interpolated_surface_d3,interpolated_surface_d4):\n",
    "    '''Function to return the highest FWI values in a fire for four input arrays  \n",
    "    Parameters\n",
    "        fire_shapefile (str): path to the fire shapefile \n",
    "        shapefile (str): path to the study area shapefile\n",
    "        latlon_dict (dict, loaded from json): dictionary of lat lon for each station \n",
    "        interpolated_surface_d1,d2,d3,d4 (np_array): an array of values in the study area, in the first four days of the fire\n",
    "    Returns \n",
    "        max_val (float): maximum value for first four days since report date \n",
    "    '''\n",
    "    v1 = get_interpolated_val_in_fire(fire_shapefile,shapefile,latlon_dict,interpolated_surface_d1)\n",
    "    v2 = get_interpolated_val_in_fire(fire_shapefile,shapefile,latlon_dict,interpolated_surface_d2)\n",
    "    v3 = get_interpolated_val_in_fire(fire_shapefile,shapefile,latlon_dict,interpolated_surface_d3)\n",
    "    v4 = get_interpolated_val_in_fire(fire_shapefile,shapefile,latlon_dict,interpolated_surface_d4)\n",
    "    list_vals = [v1,v2,v3,v4]\n",
    "    max_val = max(list_vals)\n",
    "    return max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_date_plus_three(fire_shapefile):\n",
    "    '''Function to return the first four days of the fire\n",
    "    Parameters\n",
    "        fire_shapefile (str): path to the fire shapefile \n",
    "    Returns \n",
    "        fire_dates (list): list of the report date and three days after\n",
    "    '''\n",
    "    fire_map = gpd.read_file(fire_shapefile)\n",
    "    rep_date = pd.to_datetime(fire_map['REP_DATE'].to_list()[0])\n",
    "    fire_dates = [str(rep_date)[0:10]] \n",
    "    for i in range(1,4):\n",
    "        next_date = rep_date+pd.DateOffset(i)\n",
    "        fire_dates.append(str(next_date)[0:10])\n",
    "\n",
    "    return fire_dates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the relationship of the FWI metrics with area burned using ridge regression. See: \n",
    "https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import math\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(path_to_excel_spreadsheet,var1,var2,var3,var4,var5,var6,var7,var8,var9,var10,all_variables,plot_distributions,\n",
    "                     plot_residual_histogram,transform): \n",
    "    '''Make a ridge regression model and print out the resulting coefficients and (if True) the histogram of residuals \n",
    "    Parameters\n",
    "        path_to_excel_spreadsheet (str): path to the spreadsheet containing the FWI values for each fire and the other covariates,\n",
    "        Notes: No trailing 0s in speadsheet, no space after parameter name in header \n",
    "        var1-10 (str): variable names, corresponding to the header titles \n",
    "        all_variables (bool): if True, will use all the variables, not just the FWI metrics \n",
    "        plot_distributions (bool): if True, will plot the correlation diagram for all the variables \n",
    "        plot_residual_histogram (bool): if True, will plot a histogram showing the residuals to check if normally distributed\n",
    "        transform (bool): if True, it will transform the input data (i.e. the fire surface area), \n",
    "        to make it normally distributed\n",
    "    Returns \n",
    "        Prints out regression coefficients, MAE, and R2 of the model \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(path_to_excel_spreadsheet)\n",
    "    mod = RidgeCV() \n",
    "    y = np.array(df['CALC_HA']).reshape(-1, 1)\n",
    "    \n",
    "    if all_variables: \n",
    "        X = np.array(df[[var1,var2,var3,var4,var5,var6,var7,var8,var9,var10]])\n",
    "    else: \n",
    "        X = np.array(df[[var5,var6,var7,var8,var9,var10]]) #just fwi\n",
    "    \n",
    "    mod.fit(X,y)\n",
    "    y_pred = mod.predict(X)\n",
    "    \n",
    "    if plot_distributions: \n",
    "        \n",
    "        if all_variables: \n",
    "\n",
    "            dataset = df[['CALC_HA',var1,var2,var3,var4,var5,var6,var7,var8,var9,var10]]\n",
    "        else: \n",
    "            dataset = df[['CALC_HA',var1,var2,var3,var4,var5,var6,var7,var8,var9,var10]]\n",
    "\n",
    "        _ = sns.pairplot(dataset, kind='reg', diag_kind='kde') #check for correlation\n",
    "\n",
    "        _.fig.set_size_inches(15,15)\n",
    "    \n",
    "    print('Coefficients: %s'%mod.coef_)\n",
    "    print('Mean squared error: %s'% mean_absolute_error(df['CALC_HA'], y_pred))\n",
    "    print('Coefficient of determination: %s'% r2_score(df['CALC_HA'], y_pred))\n",
    "\n",
    "    f,  ax0 = plt.subplots(1, 1)\n",
    "    ax0.scatter(df['CALC_HA'], y_pred)\n",
    "    ax0.plot([0, 150000], [0, 150000], '--k')\n",
    "    ax0.set_ylabel('Target predicted')\n",
    "    ax0.set_xlabel('True Target')\n",
    "    ax0.set_title('Ridge Regression \\n without target transformation')\n",
    "    ax0.set_xlim([0, 150000])\n",
    "    ax0.set_ylim([0, 150000])\n",
    "    plt.show()\n",
    "\n",
    "    if plot_residual_histogram:\n",
    "        fig, ax = plt.subplots()\n",
    "        residuals = y - mod.predict(X)\n",
    "        \n",
    "        mu = sum(residuals)/len(residuals)\n",
    "        var  = sum(pow(x-mu,2) for x in residuals) / len(residuals)\n",
    "        sigma  = math.sqrt(var)\n",
    "        n, bins, patches = ax.hist(residuals,50,density=1,align='left')\n",
    "        line = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "        ax.plot(bins, line, '--')\n",
    "        ax.set_xlabel('Residuals')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "    if transform:\n",
    "        mod2 = RidgeCV() \n",
    "\n",
    "        bc =QuantileTransformer(output_distribution='normal')\n",
    "        y_trans_bc = bc.fit(y).transform(y)\n",
    "        mod2.fit(X,y_trans_bc)\n",
    "\n",
    "        y_pred2 = mod2.predict(X) \n",
    "        \n",
    "        #Make new histogram\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        residuals = y - mod2.predict(X)\n",
    "        \n",
    "        mu = sum(residuals)/len(residuals)\n",
    "        var  = sum(pow(x-mu,2) for x in residuals) / len(residuals)\n",
    "        sigma  = math.sqrt(var)\n",
    "        n, bins, patches = ax.hist(residuals,50,density=1,align='left')\n",
    "        ax.set_xlabel('Residuals')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Coefficients (T): %s'%mod.coef_)\n",
    "        print('Mean squared error (T): %s'% mean_absolute_error(df['CALC_HA'], y_pred2))\n",
    "        print('Coefficient of determination (T): %s'% r2_score(df['CALC_HA'], y_pred2))\n",
    "\n",
    "        f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n",
    "        ax0.scatter(df['CALC_HA'], y_pred)\n",
    "        ax0.plot([0, 150000], [0, 150000], '--k')\n",
    "        ax0.set_ylabel('Target predicted')\n",
    "        ax0.set_xlabel('True Target')\n",
    "        ax0.set_title('Ridge Regression \\n without transformation')\n",
    "        ax0.set_xlim([0, 150000])\n",
    "        ax0.set_ylim([0, 150000])\n",
    "\n",
    "        ax1.scatter(df['CALC_HA'], y_pred2)\n",
    "        ax1.plot([0, 150000], [0, 150000], '--k')\n",
    "        ax1.set_ylabel('Target predicted')\n",
    "        ax1.set_xlabel('True Target')\n",
    "        ax1.set_title('Ridge Regression \\n with transformation')\n",
    "        ax1.set_xlim([0, 150000])\n",
    "        ax1.set_ylim([0, 150000])\n",
    "\n",
    "        f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
